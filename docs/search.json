[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Hang Zhao, I am a second year Master of Environmental Studies candidate, also pursuing a Certificate in GIS and Spatial Analysis. This website demonstrates all of my projects done during my Master’s study at Penn."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "MUSA 550 Final Project Template",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "",
    "text": "Philadelphia, renowned for its rich history and pulsating present, is home to a continually evolving real estate environment. However, a study by the Economic League reveals a nuanced picture: the overall proportion of Philadelphia households grappling with housing cost burden experienced a decrease from 29.8% to 26.7% between 2016 and 2021(Economic League, 2023). Nevertheless, this alteration in cost burden manifested divergently across various income brackets. Considering housing is a fundamental human necessity, ensuring affordability is crucial for maintaining well-being and quality of life. Consequently, comprehending the factors that influence housing values is essential for exerting better control over the housing market and making more strategic, informed decisions.\nThis report aims to explore the relationship between median house values and various neighborhood characteristics within the city of Philadelphia. It is widely understood that property values are influenced by both the conditions of the housing and the economic status of the property owners. By analyzing data at the Census block group level, we aim to comprehend the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#data-cleaning",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#data-cleaning",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data focuses on a variety of demographic variables in the census data, starting with 1,816 census data by census block level. To further refine the dataset, a systematic data cleansing process is employed that ultimately cleans the dataset into 1,720 observations. Firstly, block groups with a population of less than 40, those without any housing units, and those with median house values lower than $10,000 are identified and flagged for further action. Additionally, an outlier block group in North Philadelphia, characterized by an unusually high median house value (over $800,000) and very low median household income (less than $8,000), is isolated. These identified anomalies are either removed from the dataset or corrected as needed, ensuring that the final dataset consists of 1720 clean and validated observations. Comprehensive documentation and quality checks are performed throughout the process to maintain data integrity and transparency."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-analysis",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nThe first step involves importing a dataset from “RegressionData.csv” into R, examining the distribution of the dependent variable (MEDHVAL) and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES) using histograms, and calculating their mean and standard deviation. Additionally, logarithmic transformations are applied to these variables, with a special transformation (log(1 + [VAR])) used for variables with zero values. The histograms of both the original and transformed variables are created to assess normality. Finally, a summary statistics table is constructed to present the mean and standard deviation of each variable.\nUnderstanding the characteristics of the data set and the distribution of the variables facilitates the assessment of linearity and normality. The method involves plotting scatter plots as well as using histograms to examine the distribution of the data. This process helps to determine the applicability of different regression models, as various models have different assumptions, such as the normality of residuals. This comprehensive approach enables a thorough exploration of the dataset’s characteristics, facilitates data normalization where necessary, and prepares the data for subsequent regression analysis. While it is possible for a non-normally distributed variable to have normally distributed values, it is more likely that if the variable itself is not normally distributed, its residuals will not be normally distributed either. This effort is consistent with the goal of creating interpretable regression models, as normally distributed variables and residuals are easier to interpret and comply with regression assumptions, ultimately improving the reliability and utility of the model for understanding relationships in the data.\nThe next step is to assess the linearity of the relationships between the dependent variable (MEDHVAL) and each of the predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES). The methodology involves creating four scatter plots, one for each predictor, to visually examine the patterns and associations between these variables. This process enables a qualitative assessment of whether the relationships appear to be linear or exhibit other types of trends, which is crucial for determining the suitability of a linear regression model for subsequent analysis. The scatter plots provide a visual representation of the data, aiding in the decision-making process regarding the choice of regression techniques and the understanding of how predictors may influence the dependent variable.\n\\[y = \\beta_{0} + \\beta_{1} * x + ε\\]\nThe third step is assessing the relationships between predictor variables by calculating Pearson correlations, with a focus on identifying multicollinearity among them. The methodology involves using the cor function in R to compute these correlations, producing a correlation matrix. The process entails examining the values in the correlation matrix to determine if any predictors exhibit strong pairwise correlations, which could indicate multicollinearity. Pearson’s correlation coefficient is from -1 to 1, where -1 indicates a strong negative linear relationship, 1 indicates a strong positive linear relationship, and 0 implies no linear relationship. Multicollinearity, where predictor variables are highly correlated with each other, can lead to unstable and unreliable regression results. The aim is to decide whether it’s appropriate to include all four variables as predictors in the regression model based on the observed correlations, ensuring a robust and interpretable model for subsequent analysis.\n\\[\nr = \\frac{\\sum((X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y}))}{\\sqrt{\\sum(X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum(Y_i - \\bar{Y})^2}}\n\\]\nFinally, visualizing spatial patterns and relationships within geographic data by creating choropleth maps for five variables. The methodology involves utilizing the R programming language and the sf package for importing and handling shapefile data, and the ggplot2 package for creating the choropleth maps. The process begins with importing the shapefile and then plotting each variable individually with color scales chosen for clarity and consistency. The final step combines all five maps into a single figure for presentation, facilitating a visual exploration of spatial distributions and correlations among these variables, and enhancing the understanding of geographic patterns in the dataset."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#multiple-regression-analysis",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#multiple-regression-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Multiple Regression Analysis",
    "text": "Multiple Regression Analysis\nOrdinary Least Squares (OLS) regression is a statistical technique to determine the relationships between a variable of interest, known as the dependent variable, and one or more independent explanatory variables, often referred to as predictors. It is often used to assess the strength and direction of the correlations between variables, indicating whether it’s positive, negative, or no correlation. It also evaluates how well the model fits the data, providing goodness of fit information. Each beta coefficient of the predictors demonstrates to what extent the dependent variable will change when one unit changes in one of the predictors, holding all other predictors constant. However, while significant predictor variables indicate a certain relationship, they do not establish causation between variables.\nWe use regression analysis to determine the correlation between the dependent variable, which is the natural log of median house value, represented as LNMEDHVAL, and the predictors which are a proportion of housing units that are vacant PCTVACANT, percent of housing units that are detached single-family houses PCTSINGLES, proportion of residents in Block Group with at least a bachelor’s degree PCTBACHMOR, and the natural log of number of households that income below 100% poverty level LNNBELPOV100. Our equation is shown as follows:\n\\[\nLNMEDHVAL = \\beta_0 + \\beta_1PCTVACANT\\ + \\beta_2PCTSINGLES + \\beta_3PCTBACHMOR + \\beta_4LNNBELPOV100 + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, interpreting the value of the dependent variable when the predictors are 0; \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\) are the slope coefficients of the predictors.\nFor a linear regression model, for any fixed value of independent variable x, there are parameters \\(\\beta_0\\), \\(\\beta_i\\), and 𝜎, where i = 1 in a simple regression and i&gt;1 in a multiple regression, such that y = \\(\\beta_0\\) + \\(\\beta_i\\)xi +\\(\\epsilon\\), where \\(\\epsilon\\) ~ N(0, \\(\\sigma^2\\)). The term \\(\\epsilon\\) is known as an error term or residual, and for each observation i, is defined as a vertical deviation (distance) between the observed value of y and the predicted value of y, denoted by ŷ. In addition, \\(\\epsilon\\)~N(0, \\(\\sigma^2\\)) means that the error terms have a normal distribution with a mean of 0 and variance \\(\\sigma^2\\). This holds for any given value of x, the average error term will be 0, and a typical deviation from the regression line will be \\(\\sigma^2\\) units.\nThere are several assumptions we have to make prior to the linear regression. 1. Check the linearity of each predictor and the dependent variable by creating scatter plots. If no linearity can be observed from the plots, variable transformation or polynomial regression might be better. 2. Examine the normality of residuals by plotting out the histogram. If the histogram is not normally distributed, log transformation may be used to normalize both the dependent variable and predictor. However, sometimes log transformation is not appropriate, especially when there are high zero inflations. 3. Confirm homoscedasticity, which means that the variance of residuals should be constant throughout the different values of x. 4. Predictors should not be strongly correlated with each other, which is also called to prevent multicollinearity. 5. No fewer than 10 observations per predictor.\nGiven n observations on y, and k predictors x1 … xk, the estimates \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), …. \\(\\beta_k\\) are chosen simultaneously to minimize the expression for the Error Sum of Squares (SSE), given by:\n\\[\nSSE=\\sum_{i = 1}^{n}{\\epsilon^2}=\\sum_{i = 1}^{n}{(y - \\hat{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_1i-\\hat{\\beta_2}x_2i-...-\\hat{\\beta_k}x_ki)^2}\n\\]\nwhere ŷ is the predicted y of the model, which equals \\(\\beta_0\\)+\\(\\beta_1\\)\\(x_1\\)+\\(\\beta_2\\)\\(x_2\\)+…..+\\(\\beta_k\\)\\(x_k\\), with the minus sign before it would be demonstrated as the equation in the bracket. SSE represents the sum of squared error, or the sum of squared residuals \\(\\epsilon\\), which is the amount of variability in y that is not explained when accounting for x in the model. There is another term SST, which means the total sum of squares, is demonstrated as the following equation:\n\\[\nSST=\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\frac{\\sum_{i = 1}^{n}{y_i}}{n})^2}\n\\]\nwhere \\(\\bar{y}\\) here represents the overall mean of y values, therefore SST is interpreted as the squared deviation of that observation from the overall mean of y, and then summing those squared deviations across all observations i, without any regard to the value of x.\n\\[\n\\hat{\\rho}=r=Corr(x,y)=\\frac{\\sum_{i = 1}^{n}{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i = 1}^{n}{(x_i-\\bar{x})^2}}{\\sqrt{\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}}}}\n\\]\nSample correlation coefficient R is a point estimator of the population correlation \\(\\rho\\).\nIf we use the formula 1 – SSE/SST, we can get the coefficient of determination \\(R^2\\), which is the proportion of observed variation in the dependent variable y that was explained by the model. Also, it always ranges between 0 and 1.\n\\[\nR^2 = 1-\\frac{SSE}{SST}\n\\]\nTo assess our model, we examine the F-statistic and its corresponding p-value. The F-test, often referred to as the omnibus test, evaluates whether any of the independent variables in the model significantly predict the dependent variable. It tests the null hypothesis that none of the independent variables are significant predictors against the alternative hypothesis that at least one of them is. A model that fails to reject the null hypothesis is typically considered less effective. We then focus on the p-value associated with each independent variable. If the p-value for a specific independent variable is below 0.05, we can reject the null hypothesis, indicating that this particular predictor significantly influences the dependent variable. In this case, our null hypothesis, or H0 is that the coefficient \\(\\beta\\) equals zero, and the alternative hypothesis, or Ha states that the coefficient \\(\\beta\\) does not equal zero, demonstrating as H0: \\(\\beta\\)=0 and Ha: \\(\\beta\\)$$0;"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-analyses",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-analyses",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Analyses",
    "text": "Additional Analyses\nTo further test the relationship between median house values and studied neighborhood characteristics, we also run the stepwise regression. Stepwise regression is a statistical method that allows us to understand the statistical relationship between independent and dependent variables. The process of stepwise regression screens candidate variables and automatically identifies influential variables. In this scenario, stepwise regression is used to examine the statistical relationship between the dependent variable (MEDHVAL), and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, and PCTSINGLES) based on the Akaike Information Criterion (a mathematical method for evaluating how well a model fits the data it was generated from). Specifically, the algorithm adds or removes predictors to see if there is a significant change in the model fit determined by the AIC value and retains all predictors resulting in significant changes. The model with a smaller ACI is usually regarded as a better one. However, there are limitations as well. Firstly, stepwise regression often leads to overfitting. To be more specific, sometimes the dataset does not contain enough data samples to accurately represent all possible input data values, leading to poor generalization to new datasets. Furthermore, rather than relying on professional knowledge, the model relies on an automatic process of selecting predictive variables. Therefore, it may overlook a more comprehensive model.\nTo test the problem of overfitting, we implement the K-fold cross-validation, a method used for evaluating the model performance. To further explain this, in this scenario(k=5), the sample dataset is randomly divided into five folds for training and validation. During each run, one-fold is selected for validation, and the rest are used for training and further iterations. This process is repeated five times, each with a different fold serving as the validation set and the other four as the training set. After this process, we will get five different performance values for each fold, the average of which serves as a holistic performance metric to determine how generalizable our model is. In this scenario, we will use the root mean squared error (RMSE) as the referencing performance value to evaluate the model’s performance. The RMSE measures the average magnitude of errors between the predicted and observed values in a dataset. In other words, it tells us the standard deviation of the residuals (prediction errors).\nTurning to the discussion of the formula of the RMSE calculation, firstly, we need to get the SSE. In the formula below, Xi stands for the observed values, Xn stands for the corresponding predicted values. The SSE is calculated as:\n\\[\nSSE = \\sum_{i = 1}^{n}{(x_i- x_n )^2}\n\\] We can then get the mean squared error (MSE) by dividing SSE by the number of observations n: \\[\nMSE = \\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}\n\\]\nAfter taking the square root of the MSE, we get the value for RMSE: \\[\nRMSE =\\sqrt{\\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}}\n\\] In this study, we initially conduct cross-validation on the regression model incorporating all four predictors. Subsequently, for comparative purposes, we also perform cross-validation on a model using only PCTBANT(housing vacancy) and MEDHHINC(median household income) as predictors."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#software",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#software",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Software",
    "text": "Software\nThe software we used is R, a programming language with powerful statistical analysis capabilities."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-results",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Results",
    "text": "Exploratory Results\n\nSummary Statistics\nFirst of all, we import the csv and shp data files. To see the fundamental statistical information of the original data, we perform summary() function to see the mean values, and the sd() function to see the standard deviation of each variable. The results are shown below in the table.\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\n\n\n\n\nDependent Variable\n\n\n\n\nMedian House Value\n66287.73\n60006.08\n\n\nPredictors\n\n\n\n\nN of Households Living in Poverty.\n189.7709\n164.3185\n\n\n% of Individuals with Bechelor’s Degrees or Higher\n16.08137\n17.76956\n\n\n% of Vacant Houses\n11.28853\n9.628472\n\n\n% of Single House Units\n9.226473\n13.24925\n\n\n\n\n# look at mean values in summary table\nsummary(data)\n\n    POLY_ID          AREAKEY                MEDHVAL          PCTBACHMOR    \n Min.   :   1.0   Min.   :421010000000   Min.   :  10000   Min.   : 0.000  \n 1st Qu.: 430.8   1st Qu.:421010000000   1st Qu.:  35075   1st Qu.: 4.847  \n Median : 860.5   Median :421010000000   Median :  53250   Median :10.000  \n Mean   : 860.5   Mean   :421010000000   Mean   :  66288   Mean   :16.081  \n 3rd Qu.:1290.2   3rd Qu.:421010000000   3rd Qu.:  78625   3rd Qu.:20.074  \n Max.   :1720.0   Max.   :421010000000   Max.   :1000001   Max.   :92.987  \n    MEDHHINC        PCTVACANT        PCTSINGLES        NBELPOV100    \n Min.   :  2499   Min.   : 0.000   Min.   :  0.000   Min.   :   0.0  \n 1st Qu.: 21060   1st Qu.: 4.372   1st Qu.:  2.110   1st Qu.:  72.0  \n Median : 29719   Median : 9.091   Median :  5.714   Median : 147.0  \n Mean   : 31542   Mean   :11.289   Mean   :  9.226   Mean   : 189.8  \n 3rd Qu.: 38750   3rd Qu.:16.282   3rd Qu.: 11.056   3rd Qu.: 257.0  \n Max.   :200001   Max.   :77.119   Max.   :100.000   Max.   :1267.0  \n\n# print out all the standard deviations\nsd(data$MEDHVAL)\n\n[1] 60006.08\n\nsd(data$NBELPOV100)\n\n[1] 164.3185\n\nsd(data$PCTBACHMOR)\n\n[1] 17.76956\n\nsd(data$PCTVACANT)\n\n[1] 9.628472\n\nsd(data$PCTSINGLES)\n\n[1] 13.24925\n\n\n\n\nHistograms and Log Transformation\nThe histograms below illustrate the distribution of the dependent variable and the four predictors, where all of these histograms are positively skewed. Consequently, we have applied Log transformation to the original variables to normalize their distributions. We have added 1 to the log-transformed data to avoid Log(0) which is undefined. Following that, we present new histograms depicting the distribution of the log-transformed variables.\n\n\n\n\n\n\n\n\nFrom the new histograms, we can see that LNPCTBACHMOR, LNPCTVACANT, LNPCTSINGLES all have zero inflation, which means that there are very high frequency of zero values in the histograms after log transformation. Keeping that in mind, we will only use the log Median House Value presented as LNMEDHVAL (dependent variable), and log Number of Household living in Poverty LNNBELPV100 (one predictor) for the following regression analysis, while keeping the other three variables original.\nOther assumptions for linear regression including checking the linear relationship between dependent variable y and each of the predictors x, homoscedasticity of the variance of residuals, independence of observations, and multicollinearity will also be examined in the following section 3.3\n\n\n\n\n\n\n\n\n\n\nChoropleth maps\nChoropleth maps of each variable, LNMEDHVAL, LNNBELPOV, PCTVACANT, PCTSINGLES, and PCTBACHMOR are presented below. We used 5 quantile breaks as the map representation method. From the maps, we can see that there are some clear overlaps between LNMEDHVAL map(Figure 1) and PCTBACHMOR map(Figure 5), showing a strong correlation of the predictor % of Bachelor’s Degree to the dependent variable Median House Value. Whereas the LNNBELPOV and PCTVACANT maps have completely different patterns from the MEDHVAL map, illustrating very weak correlations. PCTSINGLES however, is presenting a partially similar pattern to the MEDHVAL, which may have some extent of correlation to the dependent variable.\nAmong the predictors, there are no obvious similarities between the maps, while LNNBELPOV does show a little overlap with the PCTVACANT, we do not expect there to be severe multicollinearity between the predictors.\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nThe correlation matrix in the graph and table is shown below. It is used to test multicollinearity between the predictors. The table shows that the greatest correlation coefficients between predictors are -0.31, 0.24, and -0.29, which are not strongly correlated. Therefore, no severe multicollinearity between the predictors has been observed, which aligns with the expectations from previous map observations.\n\n\n\n\n\nLook at multicollinearity (exclude the dependent variable in the correlation matrix table)\n\n\n            LNNBELPOV PCTBACHMOR  PCTVACANT PCTSINGLES\nLNNBELPOV   1.0000000 -0.3197668  0.2495470 -0.2905159\nPCTBACHMOR -0.3197668  1.0000000 -0.2983580  0.1975461\nPCTVACANT   0.2495470 -0.2983580  1.0000000 -0.1513734\nPCTSINGLES -0.2905159  0.1975461 -0.1513734  1.0000000"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-results",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Results",
    "text": "Regression Results\nIn our analysis of median house values in Philadelphia, we used a linear regression model to investigate the relationships between several predictor variables and the natural logarithm of median house values(LNMEDHVAL).\nThe final equation is as follows:\n\\[ln(y) = LNMEDHVAL = \\beta_{0} + \\beta_{1}PCTVACANT + \\beta_{2}PCTSINGLES  + \\beta_{3}PCTBACHMOR + \\beta_{4}LNBELPOV + \\epsilon\\]\nFrom the statistical summary table, there are several key findings. Firstly, the F-statistic is high and the P-value is much smaller than 0.05. Therefore, we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Secondly, the coefficients for each predictor variable also provide some insights.\nNotably, a higher percentage of vacant housing units(PCTVACANT) is associated with a significant decrease in median house values, indicating the negative impact of housing vacancy on property values. That is to say, a 1% additional proportion of vacant housing units is associated with a $19 decrease in median house values. In addition, a higher number of households with incomes below 100% of the poverty level(LNNBELPOV) is associated with a significant decrease in median house values as well. As the number of households in poverty changes by 1%, the expected value of median house values changes by \\((1.01^{\\beta_1} - 1)*100 = (1.01^{-.079} - 1) * 100 = -0.0786 \\%\\) Conversely, the percentage of housing units that are detached single-family houses(PCTSINGLES) has a strong positive relationship with house values. 1% additional percentage of housing units that are detached single-family houses is associated with a $3 increase in median house values. Also, a higher proportion of residents with at least a bachelor’s degree(PCTBACHMOR) exhibits a strong positive relationship with house values, showing that areas with a well-educated population tend to have higher property values. Specifically, the expected change in median house values associated with 1 additional percentage of residents who has at least a bachelor’s degree is \\((e^{\\beta_1} - 1)*100\\% = (e^{.021} - 1) * 100 \\% = 2.12 \\%\\) Finally, the multiple R-square(0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The adjusted R-squared further takes into account the number of predictors in the model, which is 66.15% in this case.\n\n# run the 'lm' function\nlm1 &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, \n          data=data_cor)\n\n\n# print out the statistical summary table\nsummary(lm1)\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_cor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# check the anova result\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: LNMEDHVAL\n             Df  Sum Sq Mean Sq  F value                Pr(&gt;F)    \nPCTVACANT     1 180.383 180.383 1343.093 &lt; 0.00000000000000022 ***\nPCTSINGLES    1  24.543  24.543  182.741 &lt; 0.00000000000000022 ***\nPCTBACHMOR    1 235.111 235.111 1750.586 &lt; 0.00000000000000022 ***\nLNNBELPOV     1  11.692  11.692   87.054 &lt; 0.00000000000000022 ***\nResiduals  1715 230.332   0.134                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-assumption-checks",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-assumption-checks",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Assumption Checks",
    "text": "Regression Assumption Checks\nIn this section, we’ll be talking about testing model assumptions. Through the exploratory analysis above, we already have a general understanding of the distribution of the variables through histograms. In this part, we will further test whether the assumption of the linear relationship between the dependent variable (LNMEDHVAL) and each predictor is valid.\n\nModel Assumptions: Linearity\nAs we can see from the scatter plots, none of the relationships between the dependent variable (LNMEDHVAL) and each of the predictors appear to be strictly linear. Except for the relationship between median home value (LNMEDHVAL) and the percentage of residents with at least a bachelor’s degree (PCTBACHMOR) which seems to be the most linear. The other scatter plots show data points either concentrated in the center or the lower left corner. Thus, with the exception of the relationship between LNMEDHVAL and PCTBACHMOR, the relationships between the dependent variable and most of the predictors deviate significantly from the assumption of strict linearity.\n\n\n\n\n\n\n\nNormality of residuals\nThe normality of residuals is important for point estimation, confidence intervals, and hypothesis tests only for small samples due to the central limit theorem. In our model, the number of observations reaches more than 1,400. Meanwhile, it’s easy to notice from the histogram that most of the residuals are clustered around 0 and the trend seems to be normal.\n\n\n\n\n\n\n\nAdditional Checks: Homoscedasticity\nStandardized residuals are residuals divided by their standard error. \\[\ne_i^* \\approx \\frac{\\epsilon_i}{s} \\approx \\frac{\\epsilon_i}{\\sqrt{\\frac{SSE}{n-2}}}\n\\] They are used to compare residuals for different observations to each other. If a particular standardized residual is 2, then the residual itself is 2 (estimated) standard deviations larger than what would be expected from fitting the “correct” model.\nBy examining the ‘Standardized Residual by Predicted Value’ scatter plot, the goal is to discern the presence of heteroscedasticity — a scenario where the variance of residuals differs for various fitted values. A clear pattern or funnel shape in this scatter plot would indicate heteroscedasticity, suggesting systematic under-predictions or over-predictions by the model for certain ranges of fitted values. Upon analysis, the scatter plot demonstrates a relatively consistent spread of residuals across the range of fitted values, pointing towards homoscedasticity.\nAdditionally, some points lie further from the dense cluster, potentially indicating outliers. These extremely standardized residuals can influence model estimates and might warrant further investigation.\n\n\n\n\n\n\n\nSpatial Autocorrelation\nObserving the maps of the dependent variable and the predictors, there’s a discernible spatial autocorrelation between the median house values and the percentage of residents holding at least a bachelor’s degree. Prominent clusters of higher values can be identified in the northwest, northeast, center city, and university city regions of Philadelphia. This suggests that block groups nearby tend to exhibit similar values, challenging the notion that these observations are spatially independent.\n\n\n\n\n\n\n\nStandardized regression residuals map\nFrom the map of the standardized regression residuals, there appear to be clusters of similar color. This suggests that there might be certain areas where the residuals are consistently high or low, which means the model might have systematically overestimated or underestimated the house values. In the map, given the clustering of similar colors in certain regions, there appears to be some degree of positive spatial autocorrelation in the residuals."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-models",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-models",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Models",
    "text": "Additional Models\n\nusing stepwise regression and determine the best model\nAs is depicted in the result of the stepwise model, all 4 predictors in the original model are retained in the final model. To be more specific, compared with other models with some of the variables dropped, the original has the smallest AIC, -3448.16, indicating that the original model does the best prediction.\n\nbest_model &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, data=data_cor)\nstep &lt;- stepAIC(best_model, direction=\"both\")\n\nStart:  AIC=-3448.16\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                      230.33 -3448.2\n- PCTSINGLES    1     2.407 232.74 -3432.3\n- LNNBELPOV100  1    11.692 242.02 -3365.0\n- PCTVACANT     1    51.543 281.87 -3102.8\n- PCTBACHMOR    1   199.014 429.35 -2379.0\n\n# stepwise regression - Analysis of Variance (ANOVA)\nstep$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\nFinal Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n\n  Step Df Deviance Resid. Df Resid. Dev       AIC\n1                       1715   230.3317 -3448.162\n\n\n\n\nK-fold model\nAfter performing cross-validation on the models, we obtained the following results: the RMSE for the original regression model stands at 0.366, while the secondary model has an RMSE of 0.443.\n\nrmse1\n\n[1] 0.3664306\n\n\n\nrmse2\n\n[1] 0.442712"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#conclusion",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#conclusion",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, we examined the relationship between median house values and several neighborhood characteristics using Philadelphia data at the Census block group level. More specifically, linear regression was done between the dependent variable MEDHVAL (Median House Values) and the predictors PCTBACHMOR (Percentage of Bachelor’s Degree or Higher), NBELPOV100 (Number of Households living in Poverty), PCTVACANT (Percentage of Vacant Houses), and PCTSINGLES (Percentage of Single House Units). Because all variables are positively skewed, we applied log transformation to each variable, and decided to use log MEDHVAL, and log NBELPOV100 in our model, because although all variables are normalized after log transformation, the other three variables all have zero inflations, while NBELPOV100 and MEDHVAL do not have/have negligible frequency of zero values.\nAfter that, regression assumptions were checked: multicollinearity, linear relationship between dependent variables and predictors, homoscedasticity of variance of residuals, and normality of residuals, where all the assumption requirements are successfully met in this model. The result of linear regression presented that all four predictors are significant with p values far less than 0.05, which means that the null hypothesis of the beta coefficient equal to zero can be rejected, and all four predictors are significantly correlated with the dependent variable LNMEDHVAL. Within those, predictor PCTBACHMOR demonstrates the most significant association with the LNMEDHVAL.\nFinally, we applied stepwise regression and k-fold cross-validation to further validate our result. Overall, all four predictors are kept in the stepwise regression, and the rmse value (root mean squared error) in our model is significantly lower than the rmse value in the model that only has PCTVACANT and MEDHHINC (Median household income) as predictors. Therefore, it validates that our model performs better."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#model-quality",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#model-quality",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Model Quality",
    "text": "Model Quality\nThe conducted analysis indicates that the regression model is a good one overall. Firstly, based on the regression results, the high F-statistic means that we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Also, the multiple R-square (0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The result of the stepwise regression further supports the strength of the model. Specifically, all predictors in the original model are retained in the final model, which means that the original model is the best. All predictors have a statistically strong relationship with the dependent variable. Moreover, a cross-validation comparison reveals the original model’s superiority. When considering all four variables, the model achieves a lower RMSE of 0.366, while a model based solely on ‘housing vacancy’ and ‘median household income’ has a higher RMSE of 0.443. This lower RMSE implies that the comprehensive model offers better predictive accuracy and alignment with actual values.\nThis analysis confirms a robust relationship between median household value and factors including residents’ educational level, housing vacancy, the proportion of detached single-family houses, and number of households living in poverty. Specifically, housing vacancy rates and the percentage of detached single-family homes can be seen as reflections of housing quality and price trends. Meanwhile, poverty and education levels provide insight into the economic standing and purchasing power of potential homeowners.\nWhile the current model is insightful, there is potential to enhance the model’s comprehensiveness by introducing additional variables. For instance, the age of the housing stock exerts a profound influence on housing prices, and the number of bedrooms within a property can also significantly impact its market value. These considerations underscore the opportunity for enriching the model with a more comprehensive set of predictors."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#limitations",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#limitations",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Limitations",
    "text": "Limitations\nIn terms of the model’s limitations, it’s important to note that the relationships between predictors and the dependent variable are not strictly linear. Only the relationship between PCTBACHMOR and MEDHVAL appears to be relatively linear. This violation of linearity assumptions could potentially introduce bias into parameter estimates and result in inaccurate outcomes. While attempts were made to address this by transforming some predictors using logarithmic transformations, certain predictors still contained significant zero values, so they were retained in their original form. Additionally, some predictors are interrelated with each other; for instance, the percentage of residents with at least a bachelor’s degree is negatively correlated with the poverty status. Furthermore, an examination of the residuals map reveals the presence of spatial autocorrelation, which could lead to inefficient parameter estimates. As such, addressing nonlinear relationships, correlated observations, and spatial autocorrelation is crucial when modeling this data.\nAlso, for the NBELPOV100 variable, we use raw numbers instead of percentages, which might make it difficult to compare across different geographical regions or time periods, as it lacks contexts or normalization. This may further lead to misleading interpretations of the predictor’s effect. On the other hand, it’s also challenging to explain the practical implications of changes in the number of households in poverty without considering the total population or percentage of poverty."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#ridge-and-lasso-regression",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#ridge-and-lasso-regression",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Ridge and LASSO Regression",
    "text": "Ridge and LASSO Regression\nRidge and LASSO regression are alternative regression techniques similar to each other that allow for multicollinearity, allow for a larger number of predictors than observations, and deal with overfitting by shrinking the coefficient of variables to 0. However, both Ridge and LASSO regression will result in biased predicted values while the variance becomes lower, and they will increase the complexity of the model and the way of interpretation. The problem of Ridge regression is that all k predictors will be included, that is, it cannot perform variable selection. LASSO regression on the other hand can do variable selection but still has other limitations. Normally, ridge/LASSO regression will be applied when there is severe multicollinearity, few observations relative to the number of predictors, or we would like a better fit for unseen data than with OLS regression. In this case, as our model does not have the problem of multicollinearity and our number of observations is larger than the number of predictors, we assume that it is unnecessary to perform those two regression methods."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "",
    "text": "Philadelphia, renowned for its rich history and pulsating present, is home to a continually evolving real estate environment. However, a study by the Economic League reveals a nuanced picture: the overall proportion of Philadelphia households grappling with housing cost burden experienced a decrease from 29.8% to 26.7% between 2016 and 2021(Economic League, 2023). Nevertheless, this alteration in cost burden manifested divergently across various income brackets. Considering housing is a fundamental human necessity, ensuring affordability is crucial for maintaining well-being and quality of life. Consequently, comprehending the factors that influence housing values is essential for exerting better control over the housing market and making more strategic, informed decisions.\nThis report aims to explore the relationship between median house values and various neighborhood characteristics within the city of Philadelphia. It is widely understood that property values are influenced by both the conditions of the housing and the economic status of the property owners. By analyzing data at the Census block group level, we aim to comprehend the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#data-cleaning",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#data-cleaning",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data focuses on a variety of demographic variables in the census data, starting with 1,816 census data by census block level. To further refine the dataset, a systematic data cleansing process is employed that ultimately cleans the dataset into 1,720 observations. Firstly, block groups with a population of less than 40, those without any housing units, and those with median house values lower than $10,000 are identified and flagged for further action. Additionally, an outlier block group in North Philadelphia, characterized by an unusually high median house value (over $800,000) and very low median household income (less than $8,000), is isolated. These identified anomalies are either removed from the dataset or corrected as needed, ensuring that the final dataset consists of 1720 clean and validated observations. Comprehensive documentation and quality checks are performed throughout the process to maintain data integrity and transparency."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-analysis",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nThe first step involves importing a dataset from “RegressionData.csv” into R, examining the distribution of the dependent variable (MEDHVAL) and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES) using histograms, and calculating their mean and standard deviation. Additionally, logarithmic transformations are applied to these variables, with a special transformation (log(1 + [VAR])) used for variables with zero values. The histograms of both the original and transformed variables are created to assess normality. Finally, a summary statistics table is constructed to present the mean and standard deviation of each variable.\nUnderstanding the characteristics of the data set and the distribution of the variables facilitates the assessment of linearity and normality. The method involves plotting scatter plots as well as using histograms to examine the distribution of the data. This process helps to determine the applicability of different regression models, as various models have different assumptions, such as the normality of residuals. This comprehensive approach enables a thorough exploration of the dataset’s characteristics, facilitates data normalization where necessary, and prepares the data for subsequent regression analysis. While it is possible for a non-normally distributed variable to have normally distributed values, it is more likely that if the variable itself is not normally distributed, its residuals will not be normally distributed either. This effort is consistent with the goal of creating interpretable regression models, as normally distributed variables and residuals are easier to interpret and comply with regression assumptions, ultimately improving the reliability and utility of the model for understanding relationships in the data.\nThe next step is to assess the linearity of the relationships between the dependent variable (MEDHVAL) and each of the predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES). The methodology involves creating four scatter plots, one for each predictor, to visually examine the patterns and associations between these variables. This process enables a qualitative assessment of whether the relationships appear to be linear or exhibit other types of trends, which is crucial for determining the suitability of a linear regression model for subsequent analysis. The scatter plots provide a visual representation of the data, aiding in the decision-making process regarding the choice of regression techniques and the understanding of how predictors may influence the dependent variable.\n\\[y = \\beta_{0} + \\beta_{1} * x + ε\\]\nThe third step is assessing the relationships between predictor variables by calculating Pearson correlations, with a focus on identifying multicollinearity among them. The methodology involves using the cor function in R to compute these correlations, producing a correlation matrix. The process entails examining the values in the correlation matrix to determine if any predictors exhibit strong pairwise correlations, which could indicate multicollinearity. Pearson’s correlation coefficient is from -1 to 1, where -1 indicates a strong negative linear relationship, 1 indicates a strong positive linear relationship, and 0 implies no linear relationship. Multicollinearity, where predictor variables are highly correlated with each other, can lead to unstable and unreliable regression results. The aim is to decide whether it’s appropriate to include all four variables as predictors in the regression model based on the observed correlations, ensuring a robust and interpretable model for subsequent analysis.\n\\[\nr = \\frac{\\sum((X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y}))}{\\sqrt{\\sum(X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum(Y_i - \\bar{Y})^2}}\n\\]\nFinally, visualizing spatial patterns and relationships within geographic data by creating choropleth maps for five variables. The methodology involves utilizing the R programming language and the sf package for importing and handling shapefile data, and the ggplot2 package for creating the choropleth maps. The process begins with importing the shapefile and then plotting each variable individually with color scales chosen for clarity and consistency. The final step combines all five maps into a single figure for presentation, facilitating a visual exploration of spatial distributions and correlations among these variables, and enhancing the understanding of geographic patterns in the dataset."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#multiple-regression-analysis",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#multiple-regression-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Multiple Regression Analysis",
    "text": "Multiple Regression Analysis\nOrdinary Least Squares (OLS) regression is a statistical technique to determine the relationships between a variable of interest, known as the dependent variable, and one or more independent explanatory variables, often referred to as predictors. It is often used to assess the strength and direction of the correlations between variables, indicating whether it’s positive, negative, or no correlation. It also evaluates how well the model fits the data, providing goodness of fit information. Each beta coefficient of the predictors demonstrates to what extent the dependent variable will change when one unit changes in one of the predictors, holding all other predictors constant. However, while significant predictor variables indicate a certain relationship, they do not establish causation between variables.\nWe use regression analysis to determine the correlation between the dependent variable, which is the natural log of median house value, represented as LNMEDHVAL, and the predictors which are a proportion of housing units that are vacant PCTVACANT, percent of housing units that are detached single-family houses PCTSINGLES, proportion of residents in Block Group with at least a bachelor’s degree PCTBACHMOR, and the natural log of number of households that income below 100% poverty level LNNBELPOV100. Our equation is shown as follows:\n\\[\nLNMEDHVAL = \\beta_0 + \\beta_1PCTVACANT\\ + \\beta_2PCTSINGLES + \\beta_3PCTBACHMOR + \\beta_4LNNBELPOV100 + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, interpreting the value of the dependent variable when the predictors are 0; \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\) are the slope coefficients of the predictors.\nFor a linear regression model, for any fixed value of independent variable x, there are parameters \\(\\beta_0\\), \\(\\beta_i\\), and 𝜎, where i = 1 in a simple regression and i&gt;1 in a multiple regression, such that y = \\(\\beta_0\\) + \\(\\beta_i\\)xi +\\(\\epsilon\\), where \\(\\epsilon\\) ~ N(0, \\(\\sigma^2\\)). The term \\(\\epsilon\\) is known as an error term or residual, and for each observation i, is defined as a vertical deviation (distance) between the observed value of y and the predicted value of y, denoted by ŷ. In addition, \\(\\epsilon\\)~N(0, \\(\\sigma^2\\)) means that the error terms have a normal distribution with a mean of 0 and variance \\(\\sigma^2\\). This holds for any given value of x, the average error term will be 0, and a typical deviation from the regression line will be \\(\\sigma^2\\) units.\nThere are several assumptions we have to make prior to the linear regression. 1. Check the linearity of each predictor and the dependent variable by creating scatter plots. If no linearity can be observed from the plots, variable transformation or polynomial regression might be better. 2. Examine the normality of residuals by plotting out the histogram. If the histogram is not normally distributed, log transformation may be used to normalize both the dependent variable and predictor. However, sometimes log transformation is not appropriate, especially when there are high zero inflations. 3. Confirm homoscedasticity, which means that the variance of residuals should be constant throughout the different values of x. 4. Predictors should not be strongly correlated with each other, which is also called to prevent multicollinearity. 5. No fewer than 10 observations per predictor.\nGiven n observations on y, and k predictors x1 … xk, the estimates \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), …. \\(\\beta_k\\) are chosen simultaneously to minimize the expression for the Error Sum of Squares (SSE), given by:\n\\[\nSSE=\\sum_{i = 1}^{n}{\\epsilon^2}=\\sum_{i = 1}^{n}{(y - \\hat{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_1i-\\hat{\\beta_2}x_2i-...-\\hat{\\beta_k}x_ki)^2}\n\\]\nwhere ŷ is the predicted y of the model, which equals \\(\\beta_0\\)+\\(\\beta_1\\)\\(x_1\\)+\\(\\beta_2\\)\\(x_2\\)+…..+\\(\\beta_k\\)\\(x_k\\), with the minus sign before it would be demonstrated as the equation in the bracket. SSE represents the sum of squared error, or the sum of squared residuals \\(\\epsilon\\), which is the amount of variability in y that is not explained when accounting for x in the model. There is another term SST, which means the total sum of squares, is demonstrated as the following equation:\n\\[\nSST=\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\frac{\\sum_{i = 1}^{n}{y_i}}{n})^2}\n\\]\nwhere \\(\\bar{y}\\) here represents the overall mean of y values, therefore SST is interpreted as the squared deviation of that observation from the overall mean of y, and then summing those squared deviations across all observations i, without any regard to the value of x.\n\\[\n\\hat{\\rho}=r=Corr(x,y)=\\frac{\\sum_{i = 1}^{n}{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i = 1}^{n}{(x_i-\\bar{x})^2}}{\\sqrt{\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}}}}\n\\]\nSample correlation coefficient R is a point estimator of the population correlation \\(\\rho\\).\nIf we use the formula 1 – SSE/SST, we can get the coefficient of determination \\(R^2\\), which is the proportion of observed variation in the dependent variable y that was explained by the model. Also, it always ranges between 0 and 1.\n\\[\nR^2 = 1-\\frac{SSE}{SST}\n\\]\nTo assess our model, we examine the F-statistic and its corresponding p-value. The F-test, often referred to as the omnibus test, evaluates whether any of the independent variables in the model significantly predict the dependent variable. It tests the null hypothesis that none of the independent variables are significant predictors against the alternative hypothesis that at least one of them is. A model that fails to reject the null hypothesis is typically considered less effective. We then focus on the p-value associated with each independent variable. If the p-value for a specific independent variable is below 0.05, we can reject the null hypothesis, indicating that this particular predictor significantly influences the dependent variable. In this case, our null hypothesis, or H0 is that the coefficient \\(\\beta\\) equals zero, and the alternative hypothesis, or Ha states that the coefficient \\(\\beta\\) does not equal zero, demonstrating as H0: \\(\\beta\\)=0 and Ha: \\(\\beta\\)$$0;"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#additional-analyses",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#additional-analyses",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Analyses",
    "text": "Additional Analyses\nTo further test the relationship between median house values and studied neighborhood characteristics, we also run the stepwise regression. Stepwise regression is a statistical method that allows us to understand the statistical relationship between independent and dependent variables. The process of stepwise regression screens candidate variables and automatically identifies influential variables. In this scenario, stepwise regression is used to examine the statistical relationship between the dependent variable (MEDHVAL), and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, and PCTSINGLES) based on the Akaike Information Criterion (a mathematical method for evaluating how well a model fits the data it was generated from). Specifically, the algorithm adds or removes predictors to see if there is a significant change in the model fit determined by the AIC value and retains all predictors resulting in significant changes. The model with a smaller ACI is usually regarded as a better one. However, there are limitations as well. Firstly, stepwise regression often leads to overfitting. To be more specific, sometimes the dataset does not contain enough data samples to accurately represent all possible input data values, leading to poor generalization to new datasets. Furthermore, rather than relying on professional knowledge, the model relies on an automatic process of selecting predictive variables. Therefore, it may overlook a more comprehensive model.\nTo test the problem of overfitting, we implement the K-fold cross-validation, a method used for evaluating the model performance. To further explain this, in this scenario(k=5), the sample dataset is randomly divided into five folds for training and validation. During each run, one-fold is selected for validation, and the rest are used for training and further iterations. This process is repeated five times, each with a different fold serving as the validation set and the other four as the training set. After this process, we will get five different performance values for each fold, the average of which serves as a holistic performance metric to determine how generalizable our model is. In this scenario, we will use the root mean squared error (RMSE) as the referencing performance value to evaluate the model’s performance. The RMSE measures the average magnitude of errors between the predicted and observed values in a dataset. In other words, it tells us the standard deviation of the residuals (prediction errors).\nTurning to the discussion of the formula of the RMSE calculation, firstly, we need to get the SSE. In the formula below, Xi stands for the observed values, Xn stands for the corresponding predicted values. The SSE is calculated as:\n\\[\nSSE = \\sum_{i = 1}^{n}{(x_i- x_n )^2}\n\\] We can then get the mean squared error (MSE) by dividing SSE by the number of observations n: \\[\nMSE = \\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}\n\\]\nAfter taking the square root of the MSE, we get the value for RMSE: \\[\nRMSE =\\sqrt{\\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}}\n\\] In this study, we initially conduct cross-validation on the regression model incorporating all four predictors. Subsequently, for comparative purposes, we also perform cross-validation on a model using only PCTBANT(housing vacancy) and MEDHHINC(median household income) as predictors."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#software",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#software",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Software",
    "text": "Software\nThe software we used is R, a programming language with powerful statistical analysis capabilities."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-results",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Results",
    "text": "Exploratory Results\n\nSummary Statistics\nFirst of all, we import the csv and shp data files. To see the fundamental statistical information of the original data, we perform summary() function to see the mean values, and the sd() function to see the standard deviation of each variable. The results are shown below in the table.\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\n\n\n\n\nDependent Variable\n\n\n\n\nMedian House Value\n66287.73\n60006.08\n\n\nPredictors\n\n\n\n\nN of Households Living in Poverty.\n189.7709\n164.3185\n\n\n% of Individuals with Bechelor’s Degrees or Higher\n16.08137\n17.76956\n\n\n% of Vacant Houses\n11.28853\n9.628472\n\n\n% of Single House Units\n9.226473\n13.24925\n\n\n\n\n# look at mean values in summary table\nsummary(data)\n\n    POLY_ID          AREAKEY                MEDHVAL          PCTBACHMOR    \n Min.   :   1.0   Min.   :421010000000   Min.   :  10000   Min.   : 0.000  \n 1st Qu.: 430.8   1st Qu.:421010000000   1st Qu.:  35075   1st Qu.: 4.847  \n Median : 860.5   Median :421010000000   Median :  53250   Median :10.000  \n Mean   : 860.5   Mean   :421010000000   Mean   :  66288   Mean   :16.081  \n 3rd Qu.:1290.2   3rd Qu.:421010000000   3rd Qu.:  78625   3rd Qu.:20.074  \n Max.   :1720.0   Max.   :421010000000   Max.   :1000001   Max.   :92.987  \n    MEDHHINC        PCTVACANT        PCTSINGLES        NBELPOV100    \n Min.   :  2499   Min.   : 0.000   Min.   :  0.000   Min.   :   0.0  \n 1st Qu.: 21060   1st Qu.: 4.372   1st Qu.:  2.110   1st Qu.:  72.0  \n Median : 29719   Median : 9.091   Median :  5.714   Median : 147.0  \n Mean   : 31542   Mean   :11.289   Mean   :  9.226   Mean   : 189.8  \n 3rd Qu.: 38750   3rd Qu.:16.282   3rd Qu.: 11.056   3rd Qu.: 257.0  \n Max.   :200001   Max.   :77.119   Max.   :100.000   Max.   :1267.0  \n\n# print out all the standard deviations\nsd(data$MEDHVAL)\n\n[1] 60006.08\n\nsd(data$NBELPOV100)\n\n[1] 164.3185\n\nsd(data$PCTBACHMOR)\n\n[1] 17.76956\n\nsd(data$PCTVACANT)\n\n[1] 9.628472\n\nsd(data$PCTSINGLES)\n\n[1] 13.24925\n\n\n\n\nHistograms and Log Transformation\nThe histograms below illustrate the distribution of the dependent variable and the four predictors, where all of these histograms are positively skewed. Consequently, we have applied Log transformation to the original variables to normalize their distributions. We have added 1 to the log-transformed data to avoid Log(0) which is undefined. Following that, we present new histograms depicting the distribution of the log-transformed variables.\n\n\n\n\n\n\n\n\nFrom the new histograms, we can see that LNPCTBACHMOR, LNPCTVACANT, LNPCTSINGLES all have zero inflation, which means that there are very high frequency of zero values in the histograms after log transformation. Keeping that in mind, we will only use the log Median House Value presented as LNMEDHVAL (dependent variable), and log Number of Household living in Poverty LNNBELPV100 (one predictor) for the following regression analysis, while keeping the other three variables original.\nOther assumptions for linear regression including checking the linear relationship between dependent variable y and each of the predictors x, homoscedasticity of the variance of residuals, independence of observations, and multicollinearity will also be examined in the following section 3.3\n\n\n\n\n\n\n\n\n\n\nChoropleth maps\nChoropleth maps of each variable, LNMEDHVAL, LNNBELPOV, PCTVACANT, PCTSINGLES, and PCTBACHMOR are presented below. We used 5 quantile breaks as the map representation method. From the maps, we can see that there are some clear overlaps between LNMEDHVAL map(Figure 1) and PCTBACHMOR map(Figure 5), showing a strong correlation of the predictor % of Bachelor’s Degree to the dependent variable Median House Value. Whereas the LNNBELPOV and PCTVACANT maps have completely different patterns from the MEDHVAL map, illustrating very weak correlations. PCTSINGLES however, is presenting a partially similar pattern to the MEDHVAL, which may have some extent of correlation to the dependent variable.\nAmong the predictors, there are no obvious similarities between the maps, while LNNBELPOV does show a little overlap with the PCTVACANT, we do not expect there to be severe multicollinearity between the predictors.\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nThe correlation matrix in the graph and table is shown below. It is used to test multicollinearity between the predictors. The table shows that the greatest correlation coefficients between predictors are -0.31, 0.24, and -0.29, which are not strongly correlated. Therefore, no severe multicollinearity between the predictors has been observed, which aligns with the expectations from previous map observations.\n\n\n\n\n\nLook at multicollinearity (exclude the dependent variable in the correlation matrix table)\n\n\n            LNNBELPOV PCTBACHMOR  PCTVACANT PCTSINGLES\nLNNBELPOV   1.0000000 -0.3197668  0.2495470 -0.2905159\nPCTBACHMOR -0.3197668  1.0000000 -0.2983580  0.1975461\nPCTVACANT   0.2495470 -0.2983580  1.0000000 -0.1513734\nPCTSINGLES -0.2905159  0.1975461 -0.1513734  1.0000000"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#regression-results",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#regression-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Results",
    "text": "Regression Results\nIn our analysis of median house values in Philadelphia, we used a linear regression model to investigate the relationships between several predictor variables and the natural logarithm of median house values(LNMEDHVAL).\nThe final equation is as follows:\n\\[ln(y) = LNMEDHVAL = \\beta_{0} + \\beta_{1}PCTVACANT + \\beta_{2}PCTSINGLES  + \\beta_{3}PCTBACHMOR + \\beta_{4}LNBELPOV + \\epsilon\\]\nFrom the statistical summary table, there are several key findings. Firstly, the F-statistic is high and the P-value is much smaller than 0.05. Therefore, we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Secondly, the coefficients for each predictor variable also provide some insights.\nNotably, a higher percentage of vacant housing units(PCTVACANT) is associated with a significant decrease in median house values, indicating the negative impact of housing vacancy on property values. That is to say, a 1% additional proportion of vacant housing units is associated with a $19 decrease in median house values. In addition, a higher number of households with incomes below 100% of the poverty level(LNNBELPOV) is associated with a significant decrease in median house values as well. As the number of households in poverty changes by 1%, the expected value of median house values changes by \\((1.01^{\\beta_1} - 1)*100 = (1.01^{-.079} - 1) * 100 = -0.0786 \\%\\) Conversely, the percentage of housing units that are detached single-family houses(PCTSINGLES) has a strong positive relationship with house values. 1% additional percentage of housing units that are detached single-family houses is associated with a $3 increase in median house values. Also, a higher proportion of residents with at least a bachelor’s degree(PCTBACHMOR) exhibits a strong positive relationship with house values, showing that areas with a well-educated population tend to have higher property values. Specifically, the expected change in median house values associated with 1 additional percentage of residents who has at least a bachelor’s degree is \\((e^{\\beta_1} - 1)*100\\% = (e^{.021} - 1) * 100 \\% = 2.12 \\%\\) Finally, the multiple R-square(0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The adjusted R-squared further takes into account the number of predictors in the model, which is 66.15% in this case.\n\n# run the 'lm' function\nlm1 &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, \n          data=data_cor)\n\n\n# print out the statistical summary table\nsummary(lm1)\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_cor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# check the anova result\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: LNMEDHVAL\n             Df  Sum Sq Mean Sq  F value                Pr(&gt;F)    \nPCTVACANT     1 180.383 180.383 1343.093 &lt; 0.00000000000000022 ***\nPCTSINGLES    1  24.543  24.543  182.741 &lt; 0.00000000000000022 ***\nPCTBACHMOR    1 235.111 235.111 1750.586 &lt; 0.00000000000000022 ***\nLNNBELPOV     1  11.692  11.692   87.054 &lt; 0.00000000000000022 ***\nResiduals  1715 230.332   0.134                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#regression-assumption-checks",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#regression-assumption-checks",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Assumption Checks",
    "text": "Regression Assumption Checks\nIn this section, we’ll be talking about testing model assumptions. Through the exploratory analysis above, we already have a general understanding of the distribution of the variables through histograms. In this part, we will further test whether the assumption of the linear relationship between the dependent variable (LNMEDHVAL) and each predictor is valid.\n\nModel Assumptions: Linearity\nAs we can see from the scatter plots, none of the relationships between the dependent variable (LNMEDHVAL) and each of the predictors appear to be strictly linear. Except for the relationship between median home value (LNMEDHVAL) and the percentage of residents with at least a bachelor’s degree (PCTBACHMOR) which seems to be the most linear. The other scatter plots show data points either concentrated in the center or the lower left corner. Thus, with the exception of the relationship between LNMEDHVAL and PCTBACHMOR, the relationships between the dependent variable and most of the predictors deviate significantly from the assumption of strict linearity.\n\n\n\n\n\n\n\nNormality of residuals\nThe normality of residuals is important for point estimation, confidence intervals, and hypothesis tests only for small samples due to the central limit theorem. In our model, the number of observations reaches more than 1,400. Meanwhile, it’s easy to notice from the histogram that most of the residuals are clustered around 0 and the trend seems to be normal.\n\n\n\n\n\n\n\nAdditional Checks: Homoscedasticity\nStandardized residuals are residuals divided by their standard error. \\[\ne_i^* \\approx \\frac{\\epsilon_i}{s} \\approx \\frac{\\epsilon_i}{\\sqrt{\\frac{SSE}{n-2}}}\n\\] They are used to compare residuals for different observations to each other. If a particular standardized residual is 2, then the residual itself is 2 (estimated) standard deviations larger than what would be expected from fitting the “correct” model.\nBy examining the ‘Standardized Residual by Predicted Value’ scatter plot, the goal is to discern the presence of heteroscedasticity — a scenario where the variance of residuals differs for various fitted values. A clear pattern or funnel shape in this scatter plot would indicate heteroscedasticity, suggesting systematic under-predictions or over-predictions by the model for certain ranges of fitted values. Upon analysis, the scatter plot demonstrates a relatively consistent spread of residuals across the range of fitted values, pointing towards homoscedasticity.\nAdditionally, some points lie further from the dense cluster, potentially indicating outliers. These extremely standardized residuals can influence model estimates and might warrant further investigation.\n\n\n\n\n\n\n\nSpatial Autocorrelation\nObserving the maps of the dependent variable and the predictors, there’s a discernible spatial autocorrelation between the median house values and the percentage of residents holding at least a bachelor’s degree. Prominent clusters of higher values can be identified in the northwest, northeast, center city, and university city regions of Philadelphia. This suggests that block groups nearby tend to exhibit similar values, challenging the notion that these observations are spatially independent.\n\n\n\n\n\n\n\nStandardized regression residuals map\nFrom the map of the standardized regression residuals, there appear to be clusters of similar color. This suggests that there might be certain areas where the residuals are consistently high or low, which means the model might have systematically overestimated or underestimated the house values. In the map, given the clustering of similar colors in certain regions, there appears to be some degree of positive spatial autocorrelation in the residuals."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#additional-models",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#additional-models",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Models",
    "text": "Additional Models\n\nusing stepwise regression and determine the best model\nAs is depicted in the result of the stepwise model, all 4 predictors in the original model are retained in the final model. To be more specific, compared with other models with some of the variables dropped, the original has the smallest AIC, -3448.16, indicating that the original model does the best prediction.\n\nbest_model &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, data=data_cor)\nstep &lt;- stepAIC(best_model, direction=\"both\")\n\nStart:  AIC=-3448.16\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                      230.33 -3448.2\n- PCTSINGLES    1     2.407 232.74 -3432.3\n- LNNBELPOV100  1    11.692 242.02 -3365.0\n- PCTVACANT     1    51.543 281.87 -3102.8\n- PCTBACHMOR    1   199.014 429.35 -2379.0\n\n# stepwise regression - Analysis of Variance (ANOVA)\nstep$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\nFinal Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n\n  Step Df Deviance Resid. Df Resid. Dev       AIC\n1                       1715   230.3317 -3448.162\n\n\n\n\nK-fold model\nAfter performing cross-validation on the models, we obtained the following results: the RMSE for the original regression model stands at 0.366, while the secondary model has an RMSE of 0.443.\n\nrmse1\n\n[1] 0.3664306\n\n\n\nrmse2\n\n[1] 0.442712"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#conclusion",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#conclusion",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, we examined the relationship between median house values and several neighborhood characteristics using Philadelphia data at the Census block group level. More specifically, linear regression was done between the dependent variable MEDHVAL (Median House Values) and the predictors PCTBACHMOR (Percentage of Bachelor’s Degree or Higher), NBELPOV100 (Number of Households living in Poverty), PCTVACANT (Percentage of Vacant Houses), and PCTSINGLES (Percentage of Single House Units). Because all variables are positively skewed, we applied log transformation to each variable, and decided to use log MEDHVAL, and log NBELPOV100 in our model, because although all variables are normalized after log transformation, the other three variables all have zero inflations, while NBELPOV100 and MEDHVAL do not have/have negligible frequency of zero values.\nAfter that, regression assumptions were checked: multicollinearity, linear relationship between dependent variables and predictors, homoscedasticity of variance of residuals, and normality of residuals, where all the assumption requirements are successfully met in this model. The result of linear regression presented that all four predictors are significant with p values far less than 0.05, which means that the null hypothesis of the beta coefficient equal to zero can be rejected, and all four predictors are significantly correlated with the dependent variable LNMEDHVAL. Within those, predictor PCTBACHMOR demonstrates the most significant association with the LNMEDHVAL.\nFinally, we applied stepwise regression and k-fold cross-validation to further validate our result. Overall, all four predictors are kept in the stepwise regression, and the rmse value (root mean squared error) in our model is significantly lower than the rmse value in the model that only has PCTVACANT and MEDHHINC (Median household income) as predictors. Therefore, it validates that our model performs better."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#model-quality",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#model-quality",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Model Quality",
    "text": "Model Quality\nThe conducted analysis indicates that the regression model is a good one overall. Firstly, based on the regression results, the high F-statistic means that we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Also, the multiple R-square (0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The result of the stepwise regression further supports the strength of the model. Specifically, all predictors in the original model are retained in the final model, which means that the original model is the best. All predictors have a statistically strong relationship with the dependent variable. Moreover, a cross-validation comparison reveals the original model’s superiority. When considering all four variables, the model achieves a lower RMSE of 0.366, while a model based solely on ‘housing vacancy’ and ‘median household income’ has a higher RMSE of 0.443. This lower RMSE implies that the comprehensive model offers better predictive accuracy and alignment with actual values.\nThis analysis confirms a robust relationship between median household value and factors including residents’ educational level, housing vacancy, the proportion of detached single-family houses, and number of households living in poverty. Specifically, housing vacancy rates and the percentage of detached single-family homes can be seen as reflections of housing quality and price trends. Meanwhile, poverty and education levels provide insight into the economic standing and purchasing power of potential homeowners.\nWhile the current model is insightful, there is potential to enhance the model’s comprehensiveness by introducing additional variables. For instance, the age of the housing stock exerts a profound influence on housing prices, and the number of bedrooms within a property can also significantly impact its market value. These considerations underscore the opportunity for enriching the model with a more comprehensive set of predictors."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#limitations",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#limitations",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Limitations",
    "text": "Limitations\nIn terms of the model’s limitations, it’s important to note that the relationships between predictors and the dependent variable are not strictly linear. Only the relationship between PCTBACHMOR and MEDHVAL appears to be relatively linear. This violation of linearity assumptions could potentially introduce bias into parameter estimates and result in inaccurate outcomes. While attempts were made to address this by transforming some predictors using logarithmic transformations, certain predictors still contained significant zero values, so they were retained in their original form. Additionally, some predictors are interrelated with each other; for instance, the percentage of residents with at least a bachelor’s degree is negatively correlated with the poverty status. Furthermore, an examination of the residuals map reveals the presence of spatial autocorrelation, which could lead to inefficient parameter estimates. As such, addressing nonlinear relationships, correlated observations, and spatial autocorrelation is crucial when modeling this data.\nAlso, for the NBELPOV100 variable, we use raw numbers instead of percentages, which might make it difficult to compare across different geographical regions or time periods, as it lacks contexts or normalization. This may further lead to misleading interpretations of the predictor’s effect. On the other hand, it’s also challenging to explain the practical implications of changes in the number of households in poverty without considering the total population or percentage of poverty."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#ridge-and-lasso-regression",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#ridge-and-lasso-regression",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Ridge and LASSO Regression",
    "text": "Ridge and LASSO Regression\nRidge and LASSO regression are alternative regression techniques similar to each other that allow for multicollinearity, allow for a larger number of predictors than observations, and deal with overfitting by shrinking the coefficient of variables to 0. However, both Ridge and LASSO regression will result in biased predicted values while the variance becomes lower, and they will increase the complexity of the model and the way of interpretation. The problem of Ridge regression is that all k predictors will be included, that is, it cannot perform variable selection. LASSO regression on the other hand can do variable selection but still has other limitations. Normally, ridge/LASSO regression will be applied when there is severe multicollinearity, few observations relative to the number of predictors, or we would like a better fit for unseen data than with OLS regression. In this case, as our model does not have the problem of multicollinearity and our number of observations is larger than the number of predictors, we assume that it is unnecessary to perform those two regression methods."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html",
    "href": "MUSA500_Stats/MUSA500_HW2.html",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "",
    "text": "Philadelphia, celebrated for its historical depth and vibrant present, boasts an ever-changing real estate landscape. Yet, a report from the Economic League paints a more intricate image: between 2016 and 2021, the percentage of Philadelphia households struggling with housing costs dipped from 29.8% to 26.7% (Economic League, 2023). Given the essential nature of housing as a basic human need, ensuring its affordability is paramount to sustaining quality of life. Thus, understanding the elements that shape housing prices is key to navigating the housing market more effectively and making wiser, informed choices.\nIn our previous exploration of Philadelphia’s housing landscape, we used Ordinary Least Squares (OLS) regression to examine the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty. While OLS provided valuable insights, it operates on the assumption of no spatial autocorrelation. However, the real world, especially in the domain of housing and neighborhood dynamics, often defies this assumption as there is the phenomena of spatial autocorrelation, which can lead to biased and inefficient estimates if not addressed in regression models. To confront this inherent spatial nature of our data, in this report, we will venture into spatial lag, spatial error, and geographically weighted regression methodologies to understand if these models can better account for the spatial dependencies lurking in our OLS residuals, offering a more holistic and accurate picture of Philadelphia’s housing valuation dynamics."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#a-description-of-the-concept-of-spatial-autocorrelation",
    "href": "MUSA500_Stats/MUSA500_HW2.html#a-description-of-the-concept-of-spatial-autocorrelation",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "A Description of the Concept of Spatial Autocorrelation",
    "text": "A Description of the Concept of Spatial Autocorrelation\nThere’s a saying: “everything is related to everything else, but near things are more related than distant things.” This adage, known as Tobler’s First Law of Geography, brings to the fore the concept of spatial autocorrelation – the phenomenon where geographically close observations influence each other. In other words, the attributes of places (or events) become more dissimilar as they are located further apart.\nTo evaluate spatial autocorrelation, we use Moran’s I, a correlation coefficient that measures the spatial relationships within a dataset. Essentially, it assesses the similarity of an object to its neighbors.\nTurning to the formula of Moran’s I, the Moran’s I can be calculated as: \\[\nI=\\frac{N\\sum_{i = 1}\\sum_{j = 1}^{n}wij{(x_i-\\bar{x})(x_j-\\bar{x})}}{\\sum_{i = 1}\\sum_{j = 1}^{n}wij{\\sum_{i = 1}^{n}{(x_i-\\bar{x})^2}}}\n\\] In this formula: N is the number of spatial units, Xi is the variable of interest, Xj is the variable value at another location j, X- is the mean of the variable X, wij is a matrix of spatial weights between spatial units i and j.\nIn this report, the weight matrix we use is queen contiguity. Based on this spatial matrix, a unit is considered adjacent (or a neighbor) to another if it shares either a border or a vertex (corner) with the other unit. This concept draws parallels to the movement of a queen in chess, which can traverse any number of squares in vertical, horizontal, or diagonal directions. For a dataset with n observations, this leads to an n x n matrix, commonly referred to as the weight or link matrix, which captures the pairwise spatial associations across the data. In this matrix, a ‘1’ denotes neighboring spatial units, while a ‘0’ signifies non-neighboring units. For this report, we will consistently use this weight matrix. However, it’s generally advisable to test multiple weight matrices to ensure that our findings aren’t solely influenced by the specific matrix chosen.\nTo determine if the spatial autocorrelation, as measured by Moran’s I, is significant, we perform random permutations, which tests the null hypothesis that there is no spatial autocorrelation against the alternative hypothesis that there is significant spatial autocorrelation. In this process, the observed value of Moran’s I is compared to a distribution of Moran’s I values generated from many random permutations of the spatial data. To elaborate, the observed house price values undergo 999 random shuffles, producing a corresponding 999 Moran’s I values from these permutations. Next, we arrange the 1000 Moran’s I values in decreasing order to determine the position of the Moran’s I value for the observed house price variable in relation to the values from the random permutations. If our observed value is in the extreme ends of this distribution (either very high or very low), we reject the null hypothesis, indicating that the observed spatial autocorrelation is significant.\nWhile Moran’s I provides a global measure of spatial autocorrelation, it doesn’t tell us where the local clusters and local spatial outliers are. Local spatial autocorrelation, like Local Indicators of Spatial Association (LISA), allow us to identify specific areas of significant clustering or dispersion. For each block, by looking at the deviations of its housing value from the mean housing values(zi) and that of its neighbors(zj), spatial weights between i and j, and the total number of observations(n), we can then determine if it’s part of a significant cluster of similar values (high-high or low-low) or if it’s an outlier in its neighborhood (high-low or low-high). To be more specific, a positive value indicates that housing value of block i is surrounded by blocks with similar housing values, either all high or all low. A negative value indicates that housing value of block i a positive value indicates that housing value of block I is surrounded by blocks with similar housing values, either all high or all low. Also, a value near zero indicates no significant local spatial autocorrelation.\nSignificance tests for local spatial autocorrelation are based on Monte Carlo permutation approach, which tests the null hypothesis that there is no local spatial autocorrelation at location I against the alternative hypothesis that the local spatial autocorrelation is significant. During the permutation, the housing values of each block will be randomly shuffled for 999 times, based on which we calculate the new Moran’s I value for every location for each permutation. The value of Moran’s I at location i for the original dataset is ranked relative to the list of the values produced by the reshufflings. When values of the Moran’s I at location i for the original dataset are very low or very high relative to the list of results produced by the shuffling procedure, they are significant. A pseudo significance level can be ascertained by observing the rank of the observed value in comparison to the permuted outcomes. For instance, if the value of Moran’s I at location i from the original configuration ranks as the 88th highest out of 999 permutations, it’s viewed as a 88 in 1000 event with a pseudosignificance of p ~ 0.088."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions",
    "href": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "A Review of OLS Regression and Assumptions",
    "text": "A Review of OLS Regression and Assumptions\nIn our OLS regression model, we found that the predictors—PCTVACANT, PCTSINGLES, PCTBACHMOR, and LNNBELPOV100, are significantly correlated with the dependent variable LNMEDHVAL, and we rejected the null hypothesis that all beta coefficients are zero. We thoroughly assessed the regression assumptions for OLS regression, including the normality of residuals, homoscedasticity, absence of multicollinearity, linearity between the dependent variable y and each predictor x, and the independence of observations. While some assumptions are met in our model, there are some that must be challenged. We examined the spatial autocorrelation simply by plotting choropleth graphs, revealing noticeable spatial autocorrelation between the dependent variable and one of the predictors. Furthermore, the standardized regression residuals map exhibits some degree of spatial autocorrelation, challenging the assumption that residuals are random, and the observations are independent.\nTherefore, we are using statistical method to test the spatial autocorrelation, which is called Moran’s I value, indicating whether spatial autocorrelation exists or not. Moran’s I value is between -1 to +1, and the more positive (approaching to +1) the number is, the stronger positive spatial autocorrelations there would be, and more negative (approaching to -1) the more negative spatial autocorrelations.\nTo further assess spatial autocorrelation within OLS residuals, we employed an additional method that involves regressing these residuals against those of nearby locations. For this analysis, two distinct approaches were used to define neighbors, the Rook Neighbor and the Queen Neighbor Matrix. While rook neighbor method only considers the immediate neighbors in the four cardinal directions with directly shared boundaries, queen neighbor accounts for neighbors with shared corners or intersections, thereby considering a broader range of spatial relationships. Generally, the Queen Neighbor Matrix is preferred due to its broader scope.\nIn this process, the resulting residuals are calculated as the average of the residuals from these neighboring locations. We then conducted a linear regression using OLS residuals against these averaged neighbor residuals. The focus of this analysis was on the significance of the relationship and the magnitude of the slope coefficient. A p-value less than 0.05 would lead us to reject the null hypothesis, thereby confirming the presence of spatial autocorrelation, if the slope coefficient is larger than 0.\nHeteroscedasticity is defined as the dispersion of residuals varies by level of predicted variable. To test the assumption of homoscedasticity, we have three tests that can be used in R: the Breusch-Pagan Test, Koenker-Bassett Test, and the White test. The null hypothesis here is that of homoscedasticity. If the p-value is less than 0.05, then we can reject the null hypothesis for the alternate hypothesis of heteroscedasticity.\nAnother assumption, normality of errors, meaning that the errors should be random noise, and they also should be normally distributed. The Jarque-Bera test in R examines the null hypothesis that the residuals are from a normal distribution, whereas the null hypothesis is that the errors are normal while the alternative hypothesis of non-normality."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression",
    "href": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Spatial Lag and Spatial Error Regression",
    "text": "Spatial Lag and Spatial Error Regression\nIn this assignment, we will use R for running spatial lag and spatial error regressions. Spatial lag regression assumes that the value of the dependent variable at one location is associated with the values of that variable in nearby locations.\n\\[\n\\begin{aligned}\nLNMEDHVAL = rho * W_y + \\beta_0 + \\beta_1 * PCTVACANT + \\beta_2 * PCTSINGLE + \\\\\n\\beta_3 * PCTBACHMOR + \\beta_4 * LNNBELPOV100 + \\epsilon\n\\end{aligned}\n\\]\n1)Here, ρ (rho) is the coefficient of the y-lag variable Wy, similar to how β1 is the coefficient of the variable X1. ( ρ is constrained between -1 and 1.) ρ (rho) is the spatial lag of the dependent variable entered as a predictor of the dependent variable. That is, it’s the value of the dependent variable in nearby areas. 2) B0 coefficient is the intercept or constant term which represents the expected mean value of y when all the predictor variables are equal to zero and also when spatially lagged dependent variable is equal to zero. B coefficients are the slope coefficients corresponding to each predictor variable(PCTVACANT, PCBACHOMORE, PCTSINGLES,NBELPOV100) 3) ε is the residuals, which represents the portion of y that cannot be explained by the model, capturing the effects of all other factors affecting y that are not included in the model. Spatial error regression assumes that the residual at one location is associated with residuals at nearby locations. Nearby is as defined by the weighs matrix W(rook, queen, within a certain distance of one another)\n\\[\n\\begin{aligned}\nLNMEDHVAL = \\beta_0 + \\beta_1 * PCTVACANT + \\beta_2 * PCTSINGLE + \\\\\n\\beta_3 * PCTBACHMOR + \\beta_4 * LNNBELPOV100 + \\lambda * W _\\epsilon + u\n\\end{aligned}\n\\]\nSimilarly with spatial lag model. What’s different is that the λ is the coefficient of spatially lagged residuals. Also, the u is simply random noise. We still assume that each of the predictors is linearly related with the dependent variable, that the residuals are normal, and that there should not be multicollinearity. The goal of spatial lag and spatial error regression is to take into consideration the fact there may be spatial dependencies in the residuals/the data. And through these two methods, the residuals will no longer be spatially autocorrelated and less heteroscedastic. Then, we’ll compare the results of spatial lag regression with OLS and the results of spatial error regression with OLS. After which we’ll further talk about how to pick the spatial models that perform the best. When comparing between spatial models, a number of measures are used for model comparability. These criteria include Akaike Information Criterion/Schwarz Criterion; Log Likelihood; Likelihood Ratio Test.\nFirstly, Akaike Information Criterion (AIC) and Schwartz Criterion (SC) are measures of the goodness of fit of an estimated statistical model. They are relative measures of the information that is lost when a given model is used to describe reality and can be said to describe the tradeoff between precision and complexity of the model. Typically, the lower AIC and SC, the better the fit. Secondly, the Log Likelihood is associated with the maximum likelihood method of fitting a statistical model to the data and estimating model parameters. Maximum likelihood picks the values of the model parameters that make data more likely than any other values of the parameters would make them. The higher the log likelihood, the better the model fit. However, this measure should only be used for comparing nested models. In this sense, OLS is a special case of spatial lag and spatial error models, where the coefficient of the weighted residuals term is zero. While spatial lag and spatial error are not a special case of each other. As such, we can’t use the log likelihood ratio to compare them. Thirdly, the Likelihood ratio test compares the OLS model with the spatial model. The null hypothesis is the spatial lag(error) model is not a better specification than the OLS model. If p &lt; 005, we reject the null hypothesis, and state that the spatial lag(error) model is doing a better job than the OLS model. Another way to compare is by examining the Moran’s I statistic applied to the regression residuals in the first place. Moran’s I measures whether the residuals from a regression model exhibit spatial autocorrelation, which implies that the model may not adequately account for the spatial relationships among observations. If the Moran’s I is significantly different from zero, it suggests that there is spatial autocorrelation, indicating that the OLS model may not fully address the spatial aspects.\nThere’s also a different approach to compare OLS results with spatial lag and spatial error results by looking at the Lagrange Multiplier Diagnostics in the regression output. This method provides a strategy for finding the maximum/minimum of a function subject to constraints. Specifically, we can compare the LM(error) vs LM(lag) statistics. If neither of them is statistically significant(p&gt;0.05 for both), it suggests that OLS is a better fit. However, if one of the LM statistics is more statistically significant, indicated by a lower p-value or a higher test statistic value, then that model may be a better choice."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression",
    "href": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Geographically Weighted Regression",
    "text": "Geographically Weighted Regression\nFor this assignment, we will conduct Geographically Weighted Regression (GWR) entirely in R.\nGeographically weighted regression (GWR) is a spatial analysis method that takes non-stationary variables, for example demographic factors in our case, into consideration and models the local regression between these predictors and an outcome of interest (Columbia University, 2023). Simpson’s paradox states that when the population is divided into smaller sub-groups, the relationship between two variables within a population changes, disappears, or even reverses (Sprenger & Weinberger, 2021).\nLocal regression refers to regression for each location, where you will need multiple observations to run a regression not just a single observation. GWR uses other observations in the dataset to run the regression, observations that are close to location I are given greater weights.\nThe equation for GWR model is written for each observation i=1…n:\n\\[\ny_i = \\beta_{i0} + \\beta_{i1} x_{i1} + \\beta_{i2} x_{i2} + \\cdots + \\beta_{im} x_{im} + \\varepsilon_i = \\beta_{i0} + \\sum_{k=1}^{m} \\beta_{ik} x_{ik} + \\varepsilon_i\n\\]\nSubscript i in the equation above indicates that the regression model describes the relationship between the dependent variable y and predictors xk, (k=1…m) around the location of observation i, and that the relationship is specific to that location.\nTo run a local regression, multiple observations (locations) are needed, not just a single observation (location) i. GWR uses other observations in the dataset to run the regression, observations that are close to location i are given greater weights. The weight of an observation varies with location i, observations closer to I have a stronger influence on the estimation of the parameters for location i.\nBandwidth is the distance h to express how farther the weighing kernel is covering. Fixed bandwidth means that although the number of observations will vary around each point I, the bandwidth distance h (and the area) will remain constant. Adaptive bandwidth means that the number of observations will remain fixed, but the area will not be the same. In this case we are going to use adaptive bandwidth, as the fixed bandwidth is more appropriate in a setting where the distribution of the observations is relatively stable across space, while here the polygons are heterogeneously shaped or sized, so adaptive bandwidth is selected.\nMost of the assumptions in OLS still hold in GWR, including the normality of residuals, homoscedasticity, no multicollinearity. Here for multicollinearity, we would look at the condition number in the attribute table, which indicates when the results are unstable due to local multicollinearity. The rule is, the results may not be reliable when the condition number is greater than 30, equal to null, or equal to -1.79769e+308. In addition to those, GWR also requires lots more observations, with at least 300.\nP-value, which is usual to test whether the parameter estimates are significantly different from zero, is not that important in GWR model. As there is one set of parameters associated with each regression point, as well as one set of standard errors, then there are potentially hundreds or thousands of tests that would be required to determine whether parameters are locally significant."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#spatial-autocorrelation",
    "href": "MUSA500_Stats/MUSA500_HW2.html#spatial-autocorrelation",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nThe very premise of spatial autocorrelation lies in the observation that near things are more similar to one another than to things farther away. This is Tobler’s First Law of Geography. Therefore, we begin by defining neighbors for each of the block groups in Philadelphia. Here, we will be using Queen Neighbors.\n\n\nNeighbour list object:\nNumber of regions: 1720 \nNumber of nonzero links: 10526 \nPercentage nonzero weights: 0.3558004 \nAverage number of links: 6.119767 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  27 \n  4  16  52 175 348 493 344 177  62  28  10   4   2   1   1   1   1   1 \n4 least connected regions:\n441 708 1391 1665 with 1 link\n1 most connected region:\n1636 with 27 links\n\n\n\n\n\n\n\n\n\n\n\nGlobal Moran’s I\nAfter conducting thorough computations in R, we obtained a global Moran’s I statistic of 0.79 for our dependent variable LNMEDHVAL. This figure is notably high, indicating a strong level of spatial autocorrelation. To further substantiate this finding, we employed a random permutation test. In this process, we randomly shuffled the values of LNMEDHVAL, recalculated Moran’s I for each shuffle, and repeated this procedure 999 times. This approach helps us to test our null hypothesis (H0) that suggests no spatial correlation, against the alternative hypotheses: Ha1, which posits positive spatial correlation, and Ha2, which suggests negative spatial autocorrelations. From the random permutations test results, we can also observe that with 1000 permutation, the p-value of is far less than 0.05. Moreover, from the histogram we can see that the original Moran’s I value is apparently separate from the other Moran’s I values from the 999 permutations, which means that it is much higher than the other Moran’s I values, indicating that there is significant spatial autocorrelation for our dependent variable LNMEDHVAL.\n\n\n[1] 0.793565\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  data_geom$LNMEDHVAL \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.79356, observed rank = 1000, p-value &lt;\n0.00000000000000022\nalternative hypothesis: two.sided\n\n\n\n\n\nIf the above were not sufficiently convincing, we can also create a plot to visualize the relationship between the LNMEDHVAL of the block groups and their neighbors. If there is no spatial autocorrelation, where there is no relationship between block group observations and those of their neighbors, a clear pattern should not be presented in the plot below. However, we observe that this is not the case.\n\n\n\n\n\n\n\nLocal Moran’s I\nMoving to local Moran’s I, which assesses the similarity of a location to nearby neighbors, we examined both the significance map and the cluster map. The presence of ‘high-high’ and ‘low-low’ clusters signifies strong spatial autocorrelation, indicating that both a location and its neighbors deviate from the global mean in a similar manner (either both above or both below the mean). Conversely, ‘high-low’ and ‘low-high’ clusters suggest dissimilarity between a location and its neighbors, implying an absence of spatial autocorrelation. Lastly, areas marked as ‘not significant’ indicate that the local Moran’s I value does not achieve statistical significance at the 0.05 level.\n\n\n          Ii            E.Ii       Var.Ii     Z.Ii  Pr(z != E(Ii))\n1 5.35196819 -0.003049833231 1.3051455983 4.687394 0.0000027670601\n2 4.41225942 -0.002216601273 0.7590492452 5.066922 0.0000004043007\n3 3.50068095 -0.003049833231 0.7444928827 4.060696 0.0000489266898\n4 2.44447746 -0.000843880799 0.2410048919 4.981074 0.0000006323230\n5 1.88349103 -0.001094174334 0.6259107138 2.382098 0.0172143256212\n6 0.09949306 -0.000001607927 0.0009208032 3.278811 0.0010424535943"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions-results",
    "href": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions-results",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "A Review of OLS Regression and Assumptions: Results",
    "text": "A Review of OLS Regression and Assumptions: Results\n\nOLS Regression: Review\nIn our analysis based on the OLS (Ordinary Least Squares) output from Assignment 1, we concluded that the null hypothesis - which assumes that the beta coefficients of the predictors are zero - can be confidently rejected. This implies that each predictor exhibits a significant correlation with the dependent variable MEDHVAL. Notably, PCTBACHMOR shows the strongest positive correlation, indicating that a 1% increase in the proportion of residents with at least a bachelor’s degree is associated with an approximate 2.09% increase in median house values. Furthermore, the OLS model’s multiple and adjusted R-squared values, approximately 0.66, suggest that our model accounts for about 66% of the variance in LNMEDHVAL.\n\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTSINGLES + PCTVACANT + PCTBACHMOR + \n    LNNBELPOV, data = data_geom)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\n'log Lik.' -711.4933 (df=6)\n\n\n\n\nOLS Regression: Heteroscedasticity\nDuring our diagnostic checks for the OLS model, we identified a bit of heteroscedasticity through the residual plot. To investigate this further, we applied three statistical tests for heteroscedasticity: the Breusch-Pagan test, the Koenker-Bassett test, and the White test. The null hypothesis is that of homoscedasticity, and if the p-value is less than 0.05, then we can reject the null hypothesis for the alternate hypothesis of heteroscedasticity. From the results, we can see that all three tests are giving p values far less than 0.05, indicating that there is a problem with heteroscedasticity. This finding aligns with our earlier observations from Homework 1, where we noticed heteroscedasticity in the plot of standardized residuals versus predicted values.\n\n\n\n    Breusch-Pagan test\n\ndata:  reg\nBP = 113.19, df = 4, p-value &lt; 0.00000000000000022\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  reg\nBP = 42.868, df = 4, p-value = 0.00000001102\n\n\nWhite's test results\n\nNull hypothesis: Homoskedasticity of the residuals\nAlternative hypothesis: Heteroskedasticity of the residuals\nTest Statistic: 43.94\nP-value: 0\n\n\n\n\n\n\n\n\n\nOLS Regression: Normality of errors\nIn addition to heteroscedasticity, we also check the normality of errors using a statistical test called Jarque-Bera test, which examines whether the errors are random noise or not, giving that the null hypothesis of normal distribution of residuals. From the test results, we can conclude that the errors are non-normal as p-value is less than 0.05, rejecting our null hypothesis. However, this statistical test result is differing from the conclusion we obtained in HW1, where we observed a normally distributed histogram of the standardized residuals. This discrepancy might be due to the highly conservative nature of the Jarque-Bera test, which is sensitive to even minor deviations from normality, often leading to their identification as statistically significant. Therefore, even though the histogram of residuals seems to be normal, there still might be slight non-normality existing, which is hard to observe by human eyes.\n\n\n\n    Jarque Bera Test\n\ndata:  reg$residuals\nX-squared = 778.96, df = 2, p-value &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\nOLS Regression: Residuals and Neighbor Residuals\nTo delve deeper into the issue of spatial autocorrelation, we conducted a regression analysis between OLS residuals and Spatial Lag residuals, using a Queen Weighted Matrix. The results revealed a significant beta coefficient with a relatively high value of 0.73. This suggests a strong statistical association between neighboring residuals, where a unit change in the lagged residual corresponds to a 0.73 unit change in the residual. Then we performed the Moran’s I test showing the Moran’s I value of the standardized residuals, which is significant and showing a value of 0.31.Within the scatter plot of spatially lagged residuals against standardized residuals, it also presents a linear correlated pattern, which is telling that there is a positive spatial autocorrelation in the OLS residuals which is problematic.\nWe will test the presence of spatial autocorrelation in two ways: 1) by regressing residuals on their queen neighbors, and 2) by looking at the Moran’s I of the residuals.\n\n\n\n\n\nFirst, let’s regress the OLS standardized residuals on the spatial lag of the OLS residuals (i.e., OLS residuals at the queen neighbors). We can see that the beta coefficient of the lagged residuals is significant and positive (0.732, p&lt;&lt;0.05), meaning that there’s a significant level of spatial autocorrelation in the residuals. This is consistent with Moran’s I of the residuals we see below.\n\n\n\nCall:\nlm(formula = standardised ~ resnb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3685 -0.4450  0.0585  0.4618  5.4435 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.01281    0.02121  -0.604               0.546    \nresnb        0.73235    0.03244  22.576 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8793 on 1718 degrees of freedom\nMultiple R-squared:  0.2288,    Adjusted R-squared:  0.2283 \nF-statistic: 509.7 on 1 and 1718 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\n\nAgain, we can use moran.mc to generate a Moran’s I statistic and a pseudo p-value.\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  standardised \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.3124, observed rank = 1000, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\n\n\n\n\nFrom the above, it is strongly apparent that spatial autocorrelation exists among the regression residuals of the OLS Model. The p-value is very small indicating that Moran’s I is significant. Because there’s clearly spatial autocorrelation in OLS residuals, the OLS Model is inappropriate and we need to consider another method. Here, we will attempt to run the Spatial Lag Model, the Spatial Error Model, and Geographically Weighted Regression."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression-results",
    "href": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression-results",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Spatial Lag and Spatial Error Regression Results",
    "text": "Spatial Lag and Spatial Error Regression Results\n\nRegression Analysis: Spatial Lag Regression\n\n\n\nCall:lagsarlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_geom, listw = queenlist)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1.655421 -0.117248  0.018654  0.133126  1.726436 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value              Pr(&gt;|z|)\n(Intercept)  3.89845489  0.20111357  19.3843 &lt; 0.00000000000000022\nPCTVACANT   -0.00852940  0.00074367 -11.4694 &lt; 0.00000000000000022\nPCTSINGLES   0.00203342  0.00051577   3.9425         0.00008063503\nPCTBACHMOR   0.00851381  0.00052193  16.3120 &lt; 0.00000000000000022\nLNNBELPOV   -0.03405466  0.00629287  -5.4116         0.00000006246\n\nRho: 0.6511, LR test value: 911.51, p-value: &lt; 0.000000000000000222\nAsymptotic standard error: 0.01805\n    z-value: 36.072, p-value: &lt; 0.000000000000000222\nWald statistic: 1301.2, p-value: &lt; 0.000000000000000222\n\nLog likelihood: -255.74 for lag model\nML residual variance (sigma squared): 0.071948, (sigma: 0.26823)\nNumber of observations: 1720 \nNumber of parameters estimated: 7 \nAIC: 525.48, (AIC for lm: 1435)\nLM test for residual autocorrelation\ntest value: 67.737, p-value: 0.00000000000000022204\n\n\n\n    Likelihood ratio for spatial linear models\n\ndata:  \nLikelihood ratio = 911.51, df = 1, p-value &lt; 0.00000000000000022\nsample estimates:\nLog likelihood of lagreg    Log likelihood of reg \n               -255.7400                -711.4933 \n\n\n\n    Breusch-Pagan test\n\ndata:  \nBP = 210.76, df = 4, p-value &lt; 0.00000000000000022\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  \nBP = 51.411, df = 4, p-value = 0.0000000001832\n\n\n\n    Jarque Bera Test\n\ndata:  lagreg$residuals\nX-squared = 2756.9, df = 2, p-value &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reslag \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.082412, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\n\nThe spatial lag model yields several key insights. Firstly, the W_LNMEDHAL is 0.6511, signifying a substantial and significant spatial relationship: median house values in one area are moderately strongly linked with those in adjacent areas. Moreover, the predictors, including LNNBELPOV, PCTBACHMOR, PCTSINGLES, and PCTVACNT, have p-values far below the 0.05 threshold, confirming their statistical significance. When comparing with the Ordinary Least Squares (OLS) model, there’s a slight uptick in the p-values for these predictors, hinting at a greater likelihood that the associations between median housing values and these predictors could be attributed to chance.\nAt the same time, as suggested by the result of the Breusch-Pagan test, the p value is way much smaller than 0.05, which suggests that we reject the null hypothesis and acknowledge the presence of heteroscedasticity.\nIn comparing the OLS and Spatial Lag regressions, we turn to the Akaike Information Criterion (AIC), Log Likelihood, and the Likelihood Ratio Test. The Spatial Lag regression yields an AIC of 525.48—markedly lower than the OLS regression’s AIC of 1435—and a Log Likelihood of -255.74, which surpasses the OLS regression’s -711.49. The Likelihood Ratio Test’s p-value is well below 0.05, leading us to discard the null hypothesis that the spatial lag model does not offer a better fit than the OLS model. Additionally, the Moran’s I value for the spatial lag model is a minimal -0.082412, indicating significantly reduced spatial autocorrelation in the residuals compared to the OLS regression. This evidence suggests that the Spatial Lag model provides a better fit.\n\n\nRegression Analysis: Spatial Error Regression\n\n\n\nCall:errorsarlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_geom, listw = queenlist)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1.926477 -0.115408  0.014889  0.133852  1.948664 \n\nType: error \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value              Pr(&gt;|z|)\n(Intercept) 10.90643427  0.05346777 203.9815 &lt; 0.00000000000000022\nPCTVACANT   -0.00578308  0.00088670  -6.5220      0.00000000006937\nPCTSINGLES   0.00267792  0.00062083   4.3134      0.00001607389089\nPCTBACHMOR   0.00981293  0.00072896  13.4615 &lt; 0.00000000000000022\nLNNBELPOV   -0.03453409  0.00708933  -4.8713      0.00000110881162\n\nLambda: 0.81492, LR test value: 677.61, p-value: &lt; 0.000000000000000222\nAsymptotic standard error: 0.016373\n    z-value: 49.772, p-value: &lt; 0.000000000000000222\nWald statistic: 2477.2, p-value: &lt; 0.000000000000000222\n\nLog likelihood: -372.6904 for error model\nML residual variance (sigma squared): 0.076551, (sigma: 0.27668)\nNumber of observations: 1720 \nNumber of parameters estimated: 7 \nAIC: NA (not available for weighted model), (AIC for lm: 1435)\n\n\n\n    Likelihood ratio for spatial linear models\n\ndata:  \nLikelihood ratio = 677.61, df = 1, p-value &lt; 0.00000000000000022\nsample estimates:\nLog likelihood of errreg    Log likelihood of reg \n               -372.6904                -711.4933 \n\n\n\n    Breusch-Pagan test\n\ndata:  \nBP = 23.213, df = 4, p-value = 0.0001148\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  \nBP = 5.1627, df = 4, p-value = 0.271\n\n\n\n    Jarque Bera Test\n\ndata:  errreg$residuals\nX-squared = 3507, df = 2, p-value &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reserr \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.094532, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\n\nNow we will also look at the results of Spatial Error regression. Here, we see that lambda has the value of 0.81492 and is significant, indicating that the median house value in an area has a strong relationship with median house value in surrounding areas. Also, the corresponding p-value of all the predictors are smaller than 0.05, indicating that all of them are significant. All predictors maintain p-values under 0.05, underscoring their significance. Yet, relative to the OLS regression, the predictors’ p-values have risen, signaling a diminution in the strength of their statistical significance.\nAt the same time, as suggested by the result of the Breusch-Pagan test, the p value is way much smaller than 0.05, pointing to heteroscedasticity within the Spatial Lag regression residuals.\nWhen we consider the AIC and Log Likelihood for the Spatial Error regression, we find an AIC of 754.985 and a Log Likelihood of -372.6904—both figures are more favorable than those from the OLS model. The Moran’s I value for the Spatial Error regression is just -0.094532, indicating even weaker spatial autocorrelation compared with the OLS model. Given that the Spatial Lag model has the lowest AIC, it stands as the best-performing model.\nLastly, in comparing the Spatial Lag and Spatial Error models, we note that the AIC for the Spatial Lag is lower than that of the Spatial Error Model, suggesting the former as the more predictive model. Since the Spatial Lag and Spatial Error models are not nested, direct comparison using the log-likelihood ratio is not applicable, thus we rely on the AIC for this assessment."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression-results",
    "href": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression-results",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Geographically Weighted Regression Results",
    "text": "Geographically Weighted Regression Results\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\n\n\nBandwidth: 0.381966 AIC: 1278.637 \nBandwidth: 0.618034 AIC: 1333.984 \nBandwidth: 0.236068 AIC: 1209.441 \nBandwidth: 0.145898 AIC: 1115.788 \nBandwidth: 0.09016994 AIC: 1007.261 \nBandwidth: 0.05572809 AIC: 910.3448 \nBandwidth: 0.03444185 AIC: 821.2049 \nBandwidth: 0.02128624 AIC: 737.5153 \nBandwidth: 0.01315562 AIC: 681.5228 \nBandwidth: 0.008130619 AIC: 660.7924 \nBandwidth: 0.005024999 AIC: 714.1722 \nBandwidth: 0.009856235 AIC: 666.9998 \nBandwidth: 0.006944377 AIC: 667.5033 \nBandwidth: 0.008427513 AIC: 661.6706 \nBandwidth: 0.007677515 AIC: 663.5923 \nBandwidth: 0.008171309 AIC: 660.8446 \nBandwidth: 0.008052658 AIC: 661.0577 \nBandwidth: 0.008130619 AIC: 660.7924 \n\n\n\n\nBandwidth: 47374.26 AIC: 1380.089 \nBandwidth: 76576.63 AIC: 1412.319 \nBandwidth: 29326.2 AIC: 1314.423 \nBandwidth: 18171.89 AIC: 1205.382 \nBandwidth: 11278.14 AIC: 1056.784 \nBandwidth: 7017.572 AIC: 904.0994 \nBandwidth: 4384.396 AIC: 773.8094 \nBandwidth: 2757.003 AIC: 701.2702 \nBandwidth: 1751.22 AIC: 920.906 \nBandwidth: 3378.612 AIC: 714.9353 \nBandwidth: 2578.424 AIC: 707.7338 \nBandwidth: 2916.626 AIC: 700.5588 \nBandwidth: 2860.559 AIC: 700.3531 \nBandwidth: 2865.211 AIC: 700.3527 \nBandwidth: 2863.515 AIC: 700.3524 \nBandwidth: 2863.494 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.493 AIC: 700.3524 \nBandwidth: 2862.372 AIC: 700.3525 \nBandwidth: 2863.064 AIC: 700.3524 \nBandwidth: 2863.329 AIC: 700.3524 \nBandwidth: 2863.43 AIC: 700.3524 \nBandwidth: 2863.468 AIC: 700.3524 \nBandwidth: 2863.483 AIC: 700.3524 \nBandwidth: 2863.489 AIC: 700.3524 \nBandwidth: 2863.491 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \n\n\n\n\n[1] 0.008130619\n\n\n[1] 2863.492\n\n\n\n\nCall:\ngwr(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = shps, gweight = gwr.Gauss, adapt = bw, \n    hatmatrix = TRUE, se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.008130619 (about 13 of 1720 data points)\nSummary of GWR coefficient estimates at data points:\n                   Min.    1st Qu.     Median    3rd Qu.       Max.  Global\nX.Intercept.  9.6727618 10.7143173 10.9542384 11.1742009 12.0831381 11.1138\nPCTVACANT    -0.0317407 -0.0142383 -0.0089599 -0.0035770  0.0167916 -0.0192\nPCTSINGLES   -0.0249706 -0.0075550 -0.0016626  0.0042280  0.0143340  0.0030\nPCTBACHMOR    0.0010974  0.0101380  0.0149279  0.0202187  0.0347258  0.0209\nLNNBELPOV    -0.2365244 -0.0733572 -0.0401186 -0.0126657  0.0948768 -0.0789\nNumber of data points: 1720 \nEffective number of parameters (residual: 2traceS - traceS'S): 360.5225 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 1359.477 \nSigma (residual: 2traceS - traceS'S): 0.2762201 \nEffective number of parameters (model: traceS): 257.9061 \nEffective degrees of freedom (model: traceS): 1462.094 \nSigma (model: traceS): 0.2663506 \nSigma (ML): 0.245571 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 660.7924 \nAIC (GWR p. 96, eq. 4.22): 308.7123 \nResidual sum of squares: 103.7248 \nQuasi-global R2: 0.8479244 \n\n\n\n\nCall:\ngwr(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = shps, bandwidth = bw_fixed, gweight = gwr.Gauss, \n    hatmatrix = TRUE, se.fit = TRUE)\nKernel function: gwr.Gauss \nFixed bandwidth: 2863.492 \nSummary of GWR coefficient estimates at data points:\n                   Min.    1st Qu.     Median    3rd Qu.       Max.  Global\nX.Intercept.  9.9111183 10.7329171 10.9397426 11.1639961 14.1200775 11.1138\nPCTVACANT    -0.0469926 -0.0137374 -0.0088796 -0.0038447  0.0778856 -0.0192\nPCTSINGLES   -0.0238330 -0.0073895 -0.0025702  0.0040499  0.0189995  0.0030\nPCTBACHMOR   -0.0860913  0.0118750  0.0168149  0.0213553  0.0306653  0.0209\nLNNBELPOV    -0.4449896 -0.0737744 -0.0433084 -0.0171174  0.1491700 -0.0789\nNumber of data points: 1720 \nEffective number of parameters (residual: 2traceS - traceS'S): 346.718 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 1373.282 \nSigma (residual: 2traceS - traceS'S): 0.2785229 \nEffective number of parameters (model: traceS): 255.6033 \nEffective degrees of freedom (model: traceS): 1464.397 \nSigma (model: traceS): 0.2697189 \nSigma (ML): 0.2488723 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 700.3524 \nAIC (GWR p. 96, eq. 4.22): 352.3471 \nResidual sum of squares: 106.5324 \nQuasi-global R2: 0.843808 \n\n\n\n\nObject of class SpatialPolygonsDataFrame\nCoordinates:\n        min       max\nx 2660604.8 2750171.3\ny  207610.6  304858.8\nIs projected: NA \nproj4string : [NA]\nData attributes:\n     sum.w        X.Intercept.      PCTVACANT           PCTSINGLES       \n Min.   :16.03   Min.   : 9.673   Min.   :-0.031741   Min.   :-0.024971  \n 1st Qu.:24.47   1st Qu.:10.714   1st Qu.:-0.014238   1st Qu.:-0.007555  \n Median :26.64   Median :10.954   Median :-0.008960   Median :-0.001663  \n Mean   :27.48   Mean   :10.937   Mean   :-0.009192   Mean   :-0.002074  \n 3rd Qu.:29.45   3rd Qu.:11.174   3rd Qu.:-0.003577   3rd Qu.: 0.004228  \n Max.   :86.70   Max.   :12.083   Max.   : 0.016792   Max.   : 0.014334  \n   PCTBACHMOR         LNNBELPOV        X.Intercept._se    PCTVACANT_se     \n Min.   :0.001097   Min.   :-0.23652   Min.   :0.09911   Min.   :0.001821  \n 1st Qu.:0.010138   1st Qu.:-0.07336   1st Qu.:0.19114   1st Qu.:0.004201  \n Median :0.014928   Median :-0.04012   Median :0.23474   Median :0.005458  \n Mean   :0.015267   Mean   :-0.04485   Mean   :0.25013   Mean   :0.006536  \n 3rd Qu.:0.020219   3rd Qu.:-0.01267   3rd Qu.:0.29127   3rd Qu.:0.007381  \n Max.   :0.034726   Max.   : 0.09488   Max.   :0.54791   Max.   :0.030192  \n PCTSINGLES_se      PCTBACHMOR_se        LNNBELPOV_se         gwr.e         \n Min.   :0.001177   Min.   :0.0007667   Min.   :0.01707   Min.   :-1.50370  \n 1st Qu.:0.003560   1st Qu.:0.0025261   1st Qu.:0.03521   1st Qu.:-0.09867  \n Median :0.005214   Median :0.0048373   Median :0.04198   Median : 0.01654  \n Mean   :0.005118   Mean   :0.0049127   Mean   :0.04413   Mean   : 0.01099  \n 3rd Qu.:0.006596   3rd Qu.:0.0066118   3rd Qu.:0.05035   3rd Qu.: 0.12800  \n Max.   :0.010560   Max.   :0.0151900   Max.   :0.09856   Max.   : 1.67766  \n      pred           pred.se           localR2       X.Intercept._se_EDF\n Min.   : 9.578   Min.   :0.02931   Min.   :0.1337   Min.   :0.1028     \n 1st Qu.:10.476   1st Qu.:0.05601   1st Qu.:0.5231   1st Qu.:0.1982     \n Median :10.831   Median :0.06773   Median :0.6342   Median :0.2434     \n Mean   :10.871   Mean   :0.07462   Mean   :0.6186   Mean   :0.2594     \n 3rd Qu.:11.232   3rd Qu.:0.08449   3rd Qu.:0.7312   3rd Qu.:0.3021     \n Max.   :13.307   Max.   :0.23204   Max.   :0.8863   Max.   :0.5682     \n PCTVACANT_se_EDF   PCTSINGLES_se_EDF  PCTBACHMOR_se_EDF   LNNBELPOV_se_EDF \n Min.   :0.001889   Min.   :0.001221   Min.   :0.0007951   Min.   :0.01770  \n 1st Qu.:0.004357   1st Qu.:0.003692   1st Qu.:0.0026197   1st Qu.:0.03651  \n Median :0.005661   Median :0.005407   Median :0.0050166   Median :0.04354  \n Mean   :0.006778   Mean   :0.005307   Mean   :0.0050947   Mean   :0.04576  \n 3rd Qu.:0.007654   3rd Qu.:0.006841   3rd Qu.:0.0068568   3rd Qu.:0.05221  \n Max.   :0.031311   Max.   :0.010951   Max.   :0.0157529   Max.   :0.10222  \n   pred.se.1      \n Min.   :0.03040  \n 1st Qu.:0.05808  \n Median :0.07024  \n Mean   :0.07739  \n 3rd Qu.:0.08762  \n Max.   :0.24063  \n\n\n\nGlobal GWR Results\n\nThe overall R-squared: GWR vs OLS regression By comparing the R2 from OLS (0.662) with the Quasi-global R2 from GWR (0.848), we can see that GWR yields a better fit as over 84.8% of variance in the dependent variable can be explained by the predictors, while the OLS can only explain over 66.2%.\nCompare the AIC: GWR vs OLS, Spatial Lag, Special Error We can also compare the Akaike Information Criteria(AIC) among GWR, OLS, Spatial Lag and Spatial Error models. The GWR model yields a lowest AIC with the value of 308.7, which is substantially smaller than the AIC for OLS, Spatial Error and Spatial Lag, which are 1443.4, 754.985 and 525.48 respectively. Therefore, the GWR model seems to be a better fit.\nMoran’s I scatterplot of GWR residuals\n\nFrom the MC simulation of Moran’s I for GWR model, we can see that there is a significant, though potentially weak, spatial autocorrelation present in the model. Specifically, the significance(p-value = 0.006) implies that there is a statistically significant spatial autocorrelation. Although the Moran’s I value is close to zero(0.03), it’s still suggested that there is a tendency for similar values to be clustered together than would be expected by random chance.\nWhen comparing the scatter plots of OLS, spatial lag and spatial error, we can see that the spatial autocorrelation of the OLS model is the most significant, and the distribution of its points is clearly skewed towards the first and third quadrants, suggesting that the variables may exhibit a clear spatial clustering pattern. The other three models have weaker spatial autocorrelation, and the distribution of points is more random and close to the centerline, indicating that the distribution of these variables in space may be more random.\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resgwr \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.033425, observed rank = 992, p-value = 0.016\nalternative hypothesis: two.sided\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  standardised \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.3124, observed rank = 1000, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reslag \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.082412, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reserr \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.094532, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\nLocal Regression Results\nThe map below shows the standardized coefficients which show the relationship between the predictor and the dependent variable to help us understand regional variation in the model explanatory variables. As we can see, areas with dark blue or red show either negative or positive relationship with the median house value that’s possibly significant. Specifically, the % of housing units with detached single family houses is positively significant with the median household value in Northwest and Northeast of the city, while the relationship become gradually negative when it comes to south Philadelphia near Schuylkill River. For the % of bachelor degree, there is a positive relationship with the dependent variable that’s possibly significant in Northwest of the city, center city, as well as the university city. As for the % of vacant housing and % of households living in poverty, there are negative relationships with the Median Household Value throughout the city as well.\n\n\n\n\n\nWe can also look at the spatial distribution of the local R-squares. We can see that the four predictors (PCBACHMORE,PCTSINGLES,LNNBELPOV,PCTVACANT) do a good job explaining the variance in our dependent variable in most parts of Philadelphia, except the central city & its north, as well as some northeast, west parts of Philadelphia.\n\ntm_shape(shps)+\n  tm_fill(col='localR2',  breaks=c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), n=5, palette = 'Blues')+\n  tm_layout(frame=FALSE)"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#model-comparison",
    "href": "MUSA500_Stats/MUSA500_HW2.html#model-comparison",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Model Comparison",
    "text": "Model Comparison\nGWR vs Spatial Error vs Spatial Lag vs OLS: By looking at the Moran’s I of the residuals & the AIC values from each model in the previous section, we can find that the GWR yields a lowest value in terms of Moran’s I and AIC value. As such, GWR is selected as the best model.\n\nSpatial Lag Model vs OLS: Again, Spatial Lag model yields a lower AIC value and a higher Log Likelihood than the OLS model. In the meantime, the likelihood ratio test is significant, indicating that the Spatial Lag model is better than OLS model.\nSpatial Error Model to OLS: Similarly, Spatial Error model has a lower AIC value and Higher Log Likelihood than the OLS model. And the likelihood ratio test is also significant. Therefore, it stands as the better-performing model.\nGWR to OLS: By comparing the R2 from OLS (0.662) with the Quasi-global R2 from GWR (0.848), we can also come up with the conclusion that the GWR is the better model as well. Also, there are hot spots across space in the standardized coefficients, suggesting that spatial variability is present. It also means that the OLS might not capture the varying spatial relationships between the dependent variable and the four predictors as it violates the assumption of stationarity. In summary, the GWR model is more appropriate than OLS as it can capture the local variations in relationships that a single global model cannot."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#limitations",
    "href": "MUSA500_Stats/MUSA500_HW2.html#limitations",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Limitations",
    "text": "Limitations\n\nSpatial Lag & Regression Models Based on the result of the spatial regressions, there are still limitations of heteroscedasticity. To be more specific, the variance of the residuals is not constant across the range of the independent variables. This suggests that the standard errors of the regression coefficients may be biased, meaning that p-values can be unreliable. To improve the model, there is potential to reference alternative modeling approaches such as weighted least squares or transforming the dependent variable to stabilize the variance of the errors.\nGWR Model GWR model also holds some similar assumptions like the normality of residuals, homoscedasticity and no multicollinearity. Based on our last assignment, the trends of residuals are close to normal. However, there is a concentration of data points towards the lower end of predicted values when checking homoscedasticity which might be an issue.\n\n\n\n\n\n\nAlso,there are also signs of multicollinearity in certain part of the area, for example the Northwest of the city. Appearently, there’re 2+ variables that have similar patterns of high-value clusters in that region. Also, the significance(p-value = 0.006) of the GWR model implies that there is a statistically significant spatial autocorrelation. This can lead to biased standard error estimates and inefficient estimators."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#clarifications",
    "href": "MUSA500_Stats/MUSA500_HW2.html#clarifications",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Clarifications",
    "text": "Clarifications\nWeighted residuals are the ones from a regression model that have been adjusted for spatial autocorrelation in the error terms, among which the spatial weight matrices are used in the estimation of spatial regression. Spatial Lag model residuals are the differences between the observed values of the dependent variable and the values predicted by the spatial lag model. They are expected to have no autocorrelation if the model can explain the spatial dependence.\nAlso, in the current and earlier versions of ArcGIS Pro, some local R-squares are negative, which obviously makes no sense. Therefore, we mainly use R to do this assignment."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#why-not-ols-regression-for-dv-is-binary",
    "href": "MUSA500_Stats/MUSA500_HW3.html#why-not-ols-regression-for-dv-is-binary",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Why not OLS Regression for DV is binary?",
    "text": "Why not OLS Regression for DV is binary?\nPreviously, we have performed Ordinary Least Squares (OLS) regression for continuous dependent variables, where the beta coefficients represent the amount of changes in the dependent variable Y corresponding to a one-unit increase in predictor X. However, in the current context, the dependent variable is binary, taking on values of either 0 or 1. Consequently, a one-unit increase in X does not translate to a proportional change in Y, since Y can only switch between the binary states of 0 and 1. Therefore, the concept of a beta coefficient increase in Y as used in OLS is inapplicable here."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-overview",
    "href": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-overview",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Logistic Regression: Overview",
    "text": "Logistic Regression: Overview\n\nThe Logit Function\nInstead, predicting P(Y=1|X=x), the probability that Y=1 could be an alternative, with a translator function such that the closer the predicted y value from linear regression model is to negative infinite, the closer our predicted probability is to 0, and the closer the y predicted value is to positive infinite, the closer predicted probability is to 1, with no predicted probabilities are less than 0 or greater than 1. This translator function here we are going to use is the Logit Function, which has an equation looks like this for one predictor:\n\\[\n\\ln\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\varepsilon\n\\] Where p = P(Y=1), and the quantity ‘p/(1-p)’ is called the odds, and the quantity ln(p/(1-p)) is called the log odds, or logit. In another words, the odds may be calculated as the desirable outcomes divided by the undesirable outcomes, showing as follows: \\[\nOdds = \\frac{\\#\\ \\text{desirable outcomes}}{\\#\\ \\text{undesirable outcomes}}\n\\]\nLogistic function, or the inverse-logit function, is another form of logit function, which can be expressed as: \\[\np = \\frac{e^{\\beta_{0} + \\beta_{1} x_{1}}}{1 + e^{\\beta_{0} + \\beta_{1} x_{1}}} = \\frac{1}{1 + e^{-\\left(\\beta_{0} + \\beta_{1} x_{1}\\right)}}\n\\] Express the logistic function in the form of line graph, it looks like a ‘S’ shape, which is plotted below:\n\n\n\n\n\nFrom the graph we can see that when \\(\\beta_0\\) + \\(\\beta_1\\)X1 equals to zero, the probability of Y=1 is 50%; As \\(\\beta_0\\) + \\(\\beta_1\\)X1 becomes larger, p approaches to 1; As \\(\\beta_0\\) + \\(\\beta_1\\)X1 becomes smaller, p approaches to 0.\n\n\nRegression Equation for Logit Model\nOur car crash data set has 43,364 observations. The relevant variables are described below:\n\nDependent Variable: DRINKING_D, Drinking driver indicator (1 = Yes, 0 = No)\nPredictor: FATAL_OR_M, Crash resulted in fatality or major injury (1 = Yes, 0 = No)\nPredictor: OVERTURNED, Crash involved an overturned vehicle (1 = Yes, 0 = No)\nPredictor: CELL_PHONE, Driver was using cell phone (1= Yes, 0 = No)\nPredictor: SPEEDING, Crash involved speeding car (1 = Yes, 0 = No)\nPredictor: AGGRESSIVE, Crash involved aggressive driving (1 = Yes, 0 = No)\nPredictor: DRIVER1617, Crash involved at least one driver who was 16 or 17 years old (1 = Yes, 0 = No)\nPredictor: DRIVER65PLUS, Crash involved at least one driver who was at least 65 years old (1 = Yes, 0 = No)\nPredictor: PCTBACHMOR,% of individuals 25 years of age or older who have at least a bachelor’s degree in the Census Block Group where the crash took place\nPredictor: MEDHHINC, Median household income in the Census Block Group where the crash took place\n\nThe logit function of the regression model, which incorporates multiple predictors (our model includes nine predictors, of which seven are binary and two are continuous), can be articulated as follows:\n\\[\n\\begin{aligned}\nODDS(DRINKING\\_D=1) = \\beta_0 + \\beta_1FATAL\\_OR\\_M + \\beta_2OVERTURNED + \\beta_3CELL\\_PHONE \\\\\n+ \\beta_4SPEEDING + \\beta_5AGGRESSIVE + \\beta_6DRIVER1617\n+ \\beta_7DRIVER65PLUS + \\beta_8PCTBACHMOR \\\\\n+ \\beta_9 MEDHHINC + \\varepsilon\n\\end{aligned}\n\\]\nThe equation represents the log odds of the event where Y equals 1, corresponding to the dependent variable DRINKING_D being 1. This indicates the scenario where a drinking driver is involved. It’s calculated as the ratio of the probability of DRINKING_D equals to 1 (presence of a drinking driver) to the probability of DRINKING_D does not equals to 1 (absence of a drinking driver).\nWhen Beta coefficient is positive, it indicates that as the predictor variable increases, the log odds of the outcome occurring increases, meaning the outcome becomes more likely, when beta coefficient is negative, the log odds of the outcome occurring decreases when predictor variable increases, and the outcome becomes less likely.\nSolving the equation of logit function to make p=P(Y=1), we can get a function generally known as the inverse logit, or the logistic function, in which it has the equation like this:\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x_1}}{1 + e^{\\beta_0 + \\beta_1 x_1}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}}\n\\]\nPlugging in the variables that we are interested in, in this case are the car crash model variables:\n\\[\n\\begin{aligned}\np &= P(DRINKING\\_D=1) \\\\\n  &= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1FATAL\\_OR\\_M + \\beta_2 OVERTURNED + \\ldots + \\beta_9 MEDHHINC)}} \\\\\n  &= \\frac{1}{1 + e^{-\\left(\\beta_0 + \\sum_{i=1}^{9} \\beta_{i} \\times X_{i}\\right)}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#hypothesis-testing",
    "href": "MUSA500_Stats/MUSA500_HW3.html#hypothesis-testing",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nIn executing the logistic regression, our initial step involves testing the null hypothesis (H0), which considers that \\(\\beta_i\\) = 0. This implies that the ith independent variable has no impact on the log-odds of the outcome. We contrast this with the alternative hypothesis (Ha), which asserts that \\(\\beta_i\\) \\(\\neq\\) 0, suggesting that the independent variable in question does indeed affect the log-odds of the outcome. This framework is crucial for determining the significance of each predictor in the model. Then, we will look at the quantity, which is sometimes called the Wald statistic within the context of logistic regression, to examine the hypothesis test. The Wald statistic follows a standard normal distribution under the null hypothesis. Thus, the quantity of the Wald statistics is equivalent to a z-score in a standard normal distribution. Then, we can find the p-value associated with the Wald statistics referencing the standard normal (z) distribution tables. If the Wald statistic is far from zero, which corresponds to a small p-value in the standard normal (z) distribution tables, we can reject the null hypothesis and get the conclusion that the independent variable has a significant effect on the outcome.\nAt the same time, in the context of logistic regression, while the estimated \\(\\beta\\) coefficients provide valuable information about the direction and magnitude of the effect a predictor variable has on the log-odds of the outcome, many statisticians and researchers also interpret the results based on odds ratios. The reason is that odds ratios offer a clear and interpretable measure of the strength and direction of the association between the predictor variables and the binary outcome. An odds ratio (OR) is the exponentiated form of the logistic regression coefficient. This transformation is particularly useful because it translates the coefficients into a multiplicative effect on the odds of the outcome occurring for a one-unit increase in the predictor variable. When the odds ratio is: Greater than 1, it indicates that as the predictor increases by one unit, the odds of the outcome occurring increase. Less than 1, it indicates that as the predictor increases by one unit, the odds of the outcome occurring decrease. Exactly 1, it indicates that as the predictor has no effect on the odds of the outcome occurring."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#quality-of-model-fit",
    "href": "MUSA500_Stats/MUSA500_HW3.html#quality-of-model-fit",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Quality of Model Fit",
    "text": "Quality of Model Fit\nUnlike OLS regression, R-Squared can’t be interpreted in logistic regression as the % of variance explained by the model. As there is no continuous outcome variable to explain variance for. Instead, it models the probability of an event occurring based on other independent variables.\nAkaike Information Criterion (AIC) is a measure of the goodness of fit of an estimated statistical model. It’s a relative measure of the information that is lost when a given model is used to describe reality and can be said to describe the tradeoff between precision and complexity of the model. Typically, the lower AIC, the better the fit.\nSensitivity (True Positive Rate) measures the proportion of actual positives which are correctly identified as such and is complementary to the False Negative Rate. Specificity (Ture Negative Rate) measures the proportion of negatives which are correctly identified as such and is complementary to the False Positive Rate. Misclassification rate refers to both False Negative Rate and False Positive Rate. Technically, we’re looking for more correct predictions, for example, being able to predict a high probability of Y=1 if Y is actually 1, and a low probability of Y=1 if Y is actually 0. That’s being said, higher values of sensitivity and specificity are better. The fitted values 𝑦̂, which refer to probabilities that Y=1.\n\\[\nP(Y = 1) = \\hat{y}_i = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\ldots + \\hat{\\beta}_3 x_{3i}}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\ldots + \\hat{\\beta}_3 x_{3i}}}\n\\]\nAnd similarly, \\[\n\\epsilon = y_i - \\hat{y}_i\n\\]\nThe choice of cut-off value demonstrates the trade-offs between different specificity and sensitivity and really depends on different scenarios. Using a cut-off of 0.5 may not be suitable for imbalanced datasets. Ideally, we should choose a cut-off value that will somehow balance and optimize the sensitivity and specificity.\nROC curve is a way to plot sensitivity (True Positive Rate) against Specificity (False Positive Rate), it can be used to examine predictive quality of the model and determine a best cut-off value by optimizing sensitivity and specificity. When interpreting the ROC Curve plot, a cut-off for which the ROC curve has the minimum distance from the upper left corner of the graph maximizes sensitivity and specificity. We can also look at the Youden index, a cut-off for which sensitivity and specificity is maximized. For area under ROC Curve(AUC), it is a measure of prediction accuracy of the model. Usually, higher AUCs mean that we can find a cut-off value for which both sensitivity and specificity of the model are relatively high. The possible values range between 0.5 and 1, a rough guide for classifying the accuracy is: * .90-1 = excellent; * .80-.90 = good; * .70-.80 = fair; * .60-.70 = poor; * .50-.60 = fail; AUC may be interpreted as the probability that the model correctly ranks two randomly selected observations where one has Y=1 and the other one has Y=0."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#assumptions-of-logistic-regression",
    "href": "MUSA500_Stats/MUSA500_HW3.html#assumptions-of-logistic-regression",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Assumptions of Logistic Regression",
    "text": "Assumptions of Logistic Regression\nWith OLS regression, we need to make sure the linear relationship between dependent variable and each predictor, the normality of residuals, and homoscedasticity. But in Logistic regression, there’re no such assumptions. However, the dependent variable in logistic regression must be binary. Also, there should be independence of observations and no severe multicollinearity. Besides that, larger samples are also needed than for OLS because Maximum Likelihood Estimation is used to estimate regression coefficients. For example, at least 50 observations per predictor are needed, compared to about 10 pre predictor in OLS regression."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis-before-logistic-regression",
    "href": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis-before-logistic-regression",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Exploratory Analysis Before Logistic Regression",
    "text": "Exploratory Analysis Before Logistic Regression\nPrior to executing logistic regression, we will conduct exploratory analyses to gain a thorough understanding of the dataset and the interrelations among variables. Initially, we will go through the process of cross-tabulations. This technique examines the connections between the dependent binary variable and other binary predictors. Utilizing R, we’ll construct contingency tables to display the variables’ frequency distributions. Additionally, we will apply the Chi-Square (\\(\\chi^2\\)) test in R to determine if the distribution of one categorical variable depends on another. For example, a cross-tabulation of the DRINKING_D and FATAL_OR_M variables will test the Null Hypothesis (H0): There is no discrepancy in fatality rates between crashes with alcohol-impaired drivers and those without. Conversely, the Alternative Hypothesis (Ha) states that such a discrepancy exists. A significant \\(\\chi^2\\) statistic, combined with a p-value under 0.05, would suggest enough evidence to reject the null hypothesis. In other words, we can conclude there is a significant association between alcohol impairment in drivers and the likelihood of crash fatalities.\nFurther, we will conduct an independent samples t-test to compare the means of a continuous variable across two distinct groups. This test is designed to discern whether any observed mean differences are random or indicative of a genuine disparity in the population. For instance, using PCTBACHMOR as an example, the test will find out if there is a statistically significant mean difference in PCTBACHMOR between crashes with and without alcohol-impaired drivers. Here, the null hypothesis (H0) states that there is no mean difference in PCTBACHMOR values between the two groups, while the alternative hypothesis (Ha) suggests a disparity. Should the t-statistic be significantly high, and the p-value falls beneath 0.05, we would reject H0, concluding a notable difference in average PCTBACHMOR values between crashes involving alcohol-impaired drivers and those that do not."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis",
    "href": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\n\n        CRN DRINKING_D COLLISION_ FATAL_OR_M OVERTURNED CELL_PHONE SPEEDING\n1 200806719          0          7          0          0          0        0\n2 200807695          0          7          1          0          0        0\n3 200808809          0          8          0          0          0        0\n4 200809857          0          5          0          0          0        0\n5 200812736          0          1          0          0          0        0\n6 200907381          1          7          1          0          0        0\n  AGGRESSIVE DRIVER1617 DRIVER65PLUS      AREAKEY PCTBACHMOR MEDHHINC\n1          0          0            0 421010001001    64.4737    49107\n2          1          0            0 421010001001    64.4737    49107\n3          0          0            0 421010001001    64.4737    49107\n4          1          0            0 421010001001    64.4737    49107\n5          0          0            0 421010001001    64.4737    49107\n6          0          0            1 421010001001    64.4737    49107\n\n\n\nTabulation of the Dependent Variable & Predictors\nLet’s look at the tabulation of our binary dependent variable, ‘DRINKING_D’.\n\n\n      CRN              DRINKING_D        COLLISION_      FATAL_OR_M     \n Min.   :200800863   Min.   :0.00000   Min.   :0.000   Min.   :0.00000  \n 1st Qu.:200905452   1st Qu.:0.00000   1st Qu.:2.000   1st Qu.:0.00000  \n Median :201008468   Median :0.00000   Median :4.000   Median :0.00000  \n Mean   :201021771   Mean   :0.05731   Mean   :4.433   Mean   :0.03157  \n 3rd Qu.:201111811   3rd Qu.:0.00000   3rd Qu.:7.000   3rd Qu.:0.00000  \n Max.   :201303171   Max.   :1.00000   Max.   :9.000   Max.   :1.00000  \n   OVERTURNED        CELL_PHONE         SPEEDING         AGGRESSIVE    \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.0000  \n Mean   :0.01665   Mean   :0.01047   Mean   :0.03508   Mean   :0.4483  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  \n   DRIVER1617       DRIVER65PLUS       AREAKEY               PCTBACHMOR    \n Min.   :0.00000   Min.   :0.0000   Min.   :421010001001   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:421010104004   1st Qu.: 5.176  \n Median :0.00000   Median :0.0000   Median :421010194001   Median :10.015  \n Mean   :0.01582   Mean   :0.1005   Mean   :421010195889   Mean   :16.572  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:421010292004   3rd Qu.:20.758  \n Max.   :1.00000   Max.   :1.0000   Max.   :421010366001   Max.   :92.987  \n    MEDHHINC     \n Min.   :  2499  \n 1st Qu.: 21277  \n Median : 29464  \n Mean   : 31513  \n 3rd Qu.: 38068  \n Max.   :200001  \n\n\n\n    0     1 \n40879  2485 \n\n\n\n        0         1 \n0.9426944 0.0573056 \n\n\nWe see that there are 94.3% drivers who drink while driving, and only 5.7% who don’t drink. The probability of drivers who drink while driving can be calculated using the formula\n\\[Probability(DRINKING) = \\frac{Number \\; of \\; Drinking \\; Driver}{Total \\; Number \\; of \\; Drivers} = \\frac{40879}{43364} = .94. \\]\nSimilarly, the odds of drinking drivers can be calculated using the formula\n\\[Odds(DRINKING) = \\frac{Number \\; of \\; Drinking \\; Drivers}{Number \\; of \\; Non-drinking \\; Drivers} = \\frac{40879}{2485} = 16.45. \\]\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$FATAL_OR_M \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     39698 |      1181 |     40879 | \n                  |     0.945 |     0.863 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2297 |       188 |      2485 | \n                  |     0.055 |     0.137 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     41995 |      1369 |     43364 | \n                  |     0.968 |     0.032 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  167.5615     d.f. =  1     p =  0.00000000000000000000000000000000000002522202 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  166.0354     d.f. =  1     p =  0.00000000000000000000000000000000000005434077 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$OVERTURNED \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     40267 |       612 |     40879 | \n                  |     0.944 |     0.848 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2375 |       110 |      2485 | \n                  |     0.056 |     0.152 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     42642 |       722 |     43364 | \n                  |     0.983 |     0.017 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  122.788     d.f. =  1     p =  0.0000000000000000000000000001551762 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  121.0052     d.f. =  1     p =  0.000000000000000000000000000381124 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$CELL_PHONE \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     40453 |       426 |     40879 | \n                  |     0.943 |     0.938 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2457 |        28 |      2485 | \n                  |     0.057 |     0.062 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     42910 |       454 |     43364 | \n                  |     0.990 |     0.010 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  0.162071     d.f. =  1     p =  0.6872569 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  0.09065262     d.f. =  1     p =  0.7633491 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$SPEEDING \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     39618 |      1261 |     40879 | \n                  |     0.947 |     0.829 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2225 |       260 |      2485 | \n                  |     0.053 |     0.171 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     41843 |      1521 |     43364 | \n                  |     0.965 |     0.035 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  376.7808     d.f. =  1     p =  0.000000000000000000000000000000000000000000000000000000000000000000000000000000000006249562 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  374.6039     d.f. =  1     p =  0.00000000000000000000000000000000000000000000000000000000000000000000000000000000001861184 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$AGGRESSIVE \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     22357 |     18522 |     40879 | \n                  |     0.934 |     0.953 |           | \n------------------|-----------|-----------|-----------|\n                1 |      1569 |       916 |      2485 | \n                  |     0.066 |     0.047 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     23926 |     19438 |     43364 | \n                  |     0.552 |     0.448 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  67.60186     d.f. =  1     p =  0.000000000000000200079 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  67.2607     d.f. =  1     p =  0.0000000000000002378758 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$DRIVER1617 \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     40205 |       674 |     40879 | \n                  |     0.942 |     0.983 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2473 |        12 |      2485 | \n                  |     0.058 |     0.017 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     42678 |       686 |     43364 | \n                  |     0.984 |     0.016 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  20.45167     d.f. =  1     p =  0.000006115619 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  19.7097     d.f. =  1     p =  0.000009014275 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$DRIVER65PLUS \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     36642 |      4237 |     40879 | \n                  |     0.939 |     0.973 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2366 |       119 |      2485 | \n                  |     0.061 |     0.027 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     39008 |      4356 |     43364 | \n                  |     0.900 |     0.100 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  80.6047     d.f. =  1     p =  0.000000000000000000275703 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  79.9888     d.f. =  1     p =  0.0000000000000000003765375 \n\n \n\n\nPrior to the predictive modeling, we use the Chi-Square test to determine whether the distribution of the categorical variable varies with respect to the drunk driving. From the table below, we can see that the majority of the variables except cell phone have a P-value lower than 0.05. Therefore, we can reject the null hypothesis and confirm that there are associations between drunk driving and overturned vehicle, speeding car, aggressive driving,young driver, old drivers, crash fatalities.\n\n\n\n\n\n\n  \n    \n      Traffic Accident Statistics - Categorical Variables\n    \n    \n    \n      Category\n      Drinking_N\n      Drinking_Perc\n      Non_Drinking_N\n      Non_Drinking_Perc\n      Total_N\n      p_value\n    \n  \n  \n    FATAL_OR_M\n188\n0.43354\n1,181\n2.72346\n1,369\n0\n    OVERTURNED\n110\n0.25367\n612\n1.41131\n722\n0\n    CELL_PHONE\n28\n0.06457\n426\n0.98238\n454\n0.68726\n    SPEEDING\n260\n0.59958\n1,261\n2.90794\n1,521\n0\n    AGGRESSIVE\n916\n2.11235\n18,522\n42.71285\n19,438\n0\n    DRIVER1617\n12\n0.02767\n674\n1.55428\n686\n0.00001\n    DRIVER65PLUS\n119\n0.27442\n4,237\n9.77078\n4,356\n0\n  \n  \n  \n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  mydata$PCTBACHMOR by mydata$DRINKING_D\nt = -0.10842, df = 2777.5, p-value = 0.9137\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.7991398  0.7153982\nsample estimates:\nmean in group 0 mean in group 1 \n       16.56986        16.61173 \n\n\n\n    Welch Two Sample t-test\n\ndata:  mydata$MEDHHINC by mydata$DRINKING_D\nt = -1.4053, df = 2763.9, p-value = 0.16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1235.2508   203.8544\nsample estimates:\nmean in group 0 mean in group 1 \n       31483.05        31998.75 \n\n\nFor the continuous variables PCTBACHMOR,MEDHHINC, we further examine whether their means differ for drunk driving or non-drunk driving. By using the independent samples t-test, the P-value for both variables are not statistically significantly different for crashes that involve drunk drivers and crashes that don’t. Therefore, it can be concluded that the average values of the variables PCTBACHMOR and MEDHHINC are the same for accidents involving drunk drivers and those not involving them.\n\n\n\n\n\n\n  \n    \n      Traffic Accident Statistics - Continuous Variables\n    \n    \n    \n      Category\n      Drinking_mean\n      Drinking_sd\n      Non_Drinking_mean\n      Non_Drinking_sd\n      P_value\n    \n  \n  \n    PCTBACHMOR\n16.61173\n18.72091\n16.56986\n18.21426\n0.9137\n    MEDHHINC\n31,998.75\n17,810.5\n31,483.05\n16,930.1\n0.16\n  \n  \n  \n\n\n\n\nAmong the independent variables, drunk drivers tend to have higher fatality rate, faster car speed, and higher percentage of overturning cars from the primary exploratory analysis.\n\n\nAssociation"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-assumptions",
    "href": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-assumptions",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Logistic Regression Assumptions",
    "text": "Logistic Regression Assumptions\n\nPairwise Pearson Correlations Matrix\nThe correlation matrix in the graph and table is shown below. It is used to test multicollinearity between the predictors. The table shows that the greatest correlation coefficients between predictors are 0.50 and 0.47, which are not strongly correlated. Therefore, no severe multicollinearity between the predictors has been observed.\n\n\n\n\n\n\n\n               FATAL_OR_M    OVERTURNED    CELL_PHONE      SPEEDING  AGGRESSIVE\nFATAL_OR_M    1.000000000  0.0331959240  0.0021603225  0.0817126678 -0.01104729\nOVERTURNED    0.033195924  1.0000000000 -0.0009897786  0.0594402861  0.01643894\nCELL_PHONE    0.002160322 -0.0009897786  1.0000000000 -0.0036011640 -0.02574299\nSPEEDING      0.081712668  0.0594402861 -0.0036011640  1.0000000000  0.21152537\nAGGRESSIVE   -0.011047295  0.0164389397 -0.0257429929  0.2115253684  1.00000000\nDRIVER1617   -0.002808379  0.0037239674  0.0014851333  0.0160115997  0.02842895\nDRIVER65PLUS -0.012512349 -0.0195009743 -0.0027172590 -0.0328541108  0.01502693\nPCTBACHMOR   -0.014652265  0.0093321352 -0.0012458540 -0.0007390853  0.02712211\nMEDHHINC     -0.018212431  0.0279213029  0.0020998852  0.0117866805  0.04344045\n               DRIVER1617 DRIVER65PLUS    PCTBACHMOR     MEDHHINC\nFATAL_OR_M   -0.002808379 -0.012512349 -0.0146522648 -0.018212431\nOVERTURNED    0.003723967 -0.019500974  0.0093321352  0.027921303\nCELL_PHONE    0.001485133 -0.002717259 -0.0012458540  0.002099885\nSPEEDING      0.016011600 -0.032854111 -0.0007390853  0.011786681\nAGGRESSIVE    0.028428953  0.015026930  0.0271221096  0.043440451\nDRIVER1617    1.000000000 -0.020848417 -0.0026359662  0.022877425\nDRIVER65PLUS -0.020848417  1.000000000  0.0261903901  0.050337711\nPCTBACHMOR   -0.002635966  0.026190390  1.0000000000  0.477869537\nMEDHHINC      0.022877425  0.050337711  0.4778695368  1.000000000\n\n\n\n\nAssumptions Check\nLogistic regression has several assumptions, for example the dependent variable must be binary, independence of observations, no severe multicollinearity, and large sample size needed (at least 50 observations per predictor). Our dataset confirms the binary nature of our dependent variable, DRINKING_D, which is limited to values of 0 or 1. The absence of geographical data in our dataset leads us to assume independence of observations without spatial autocorrelations. We assessed multicollinearity among predictors using a Pearson correlation matrix. Here, we define the multicollinearity as the situation where two or more predictors are very strongly correlated with each other, with r&gt;0.9 or r&lt;-0.9. From the matrix table, it indicates generally low or no significant correlations between the predictors, with a notable, yet understandable, relationship between MEDHHINC (Median House Income) and PCTBACHMOR (Percentage of individuals having bachelor’s degree), reflecting the typical correlation between higher education and income.\nWhile a correlation matrix effectively displays pairwise correlations between variables, it’s important to note that the Pearson correlation assumes variables are continuous, not categorical. Given that most of our predictors are binary, with only a couple being continuous, neither Pearson nor Spearman correlation is ideal for assessing correlation and multicollinearity in our case. Instead, we can use the T-test to analyze correlations between a binary and a continuous variable, and the Chi-Squared test for correlations between two binary variables."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-results",
    "href": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-results",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Logistic Regression Results",
    "text": "Logistic Regression Results\n\nLogistic Regression With All Predictors\n\n\n\nCall:\nglm(formula = DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + \n    SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS + PCTBACHMOR + \n    MEDHHINC, family = \"binomial\", data = mydata)\n\nCoefficients:\n                 Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)  -2.732506616  0.045875659 -59.563 &lt; 0.0000000000000002 ***\nFATAL_OR_M    0.814013802  0.083806924   9.713 &lt; 0.0000000000000002 ***\nOVERTURNED    0.928921376  0.109166324   8.509 &lt; 0.0000000000000002 ***\nCELL_PHONE    0.029550085  0.197777821   0.149               0.8812    \nSPEEDING      1.538975665  0.080545894  19.107 &lt; 0.0000000000000002 ***\nAGGRESSIVE   -0.596915946  0.047779238 -12.493 &lt; 0.0000000000000002 ***\nDRIVER1617   -1.280295964  0.293147168  -4.367 0.000012572447127933 ***\nDRIVER65PLUS -0.774664640  0.095858315  -8.081 0.000000000000000641 ***\nPCTBACHMOR   -0.000370634  0.001296387  -0.286               0.7750    \nMEDHHINC      0.000002804  0.000001341   2.091               0.0365 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19036  on 43363  degrees of freedom\nResidual deviance: 18340  on 43354  degrees of freedom\nAIC: 18360\n\nNumber of Fisher Scoring iterations: 6\n\n\nBased on the regression results, among all studied predictors, except for CELL_PHONE and PCTBACHMOR with a p value greater than 0.05, the rest of them are all significant. The odd ratio provides some insights. The OR of FATAL_OR_M is 2.26 (95% CI, 1.9 – 2.65), suggesting that in crashes where there was a fatality or major injury (FATAL_OR_M = 1), the odds of the crash involving drunk driving are 2.26 times higher compared to crashes where there was no fatality or major injury (FATAL_OR_M = 0).In other words, crashes that result in fatalities or major injuries are more than twice as likely to involve drunk driving as those that do not result in such severe outcomes. The OR of OVERTURNED is 2.53(95% CI, 2.03 – 3.12), suggesting that in involved an overturned vehicle, the odds of the crash involving drunk driving are 2.53 times higher compared to crashes where no vehicle was overturned. The OR of CELL_PHONE is 1.03(95% CI, 0.68 – 1.49), suggesting that in crashes where driver was using cellphones, the odds of the crash involving drunk driving are 1.03 times higher compared to crashes where driver wasn’t using cellphones. The Odds Ratio (OR) associated with SPEEDING is 4.66(95% CI, 3.97 – 5.45). This indicates that in crashes involving speeding cars, the likelihood of drunk driving being a factor is 4.66 times greater than in crashes where speeding is not a factor. This substantial increase highlights the strong association between speeding and the likelihood of drunk driving in accidents. Also, the OR for AGGRESSIVE driving is 0.55(95% CI, 0.5 – 0.6). This suggests that in incidents involving aggressive driving, the probability of drunk driving being involved is only 55% of that in crashes where aggressive driving is not present. The OR of DRIVER1617 is 0.28(95% CI, 0.15 – 0.47), suggesting that in crashes involved at least one driver who was 16 or 17 years old, the odd of the crash involving drunk driving is 0.28 times as much as crashes where no driver was 16 or 17 years old. The OR of DRIVER65PLUS is 0.46(95% CI, 0.38 – 0.55), suggesting that in crashes involved at least one driver who was at least 65 years old, the odd of the crash involving drunk driving is 0.46 times as much as crashes where no driver was at least 65 years old.\n\n\n (Intercept)   FATAL_OR_M   OVERTURNED   CELL_PHONE     SPEEDING   AGGRESSIVE \n  0.06505601   2.25694878   2.53177687   1.02999102   4.65981462   0.55050681 \n  DRIVER1617 DRIVER65PLUS   PCTBACHMOR     MEDHHINC \n  0.27795502   0.46085831   0.99962944   1.00000280 \n\n\n\n\nWaiting for profiling to be done...\n\n\n                     OR      2.5 %     97.5 %\n(Intercept)  0.06505601 0.05947628 0.07119524\nFATAL_OR_M   2.25694878 1.90991409 2.65313350\nOVERTURNED   2.53177687 2.03462326 3.12242730\nCELL_PHONE   1.02999102 0.68354737 1.48846840\nSPEEDING     4.65981462 3.97413085 5.45020642\nAGGRESSIVE   0.55050681 0.50101688 0.60423487\nDRIVER1617   0.27795502 0.14774429 0.47109277\nDRIVER65PLUS 0.46085831 0.37998364 0.55347851\nPCTBACHMOR   0.99962944 0.99707035 1.00215087\nMEDHHINC     1.00000280 1.00000013 1.00000539\n\n\n\n\nWaiting for profiling to be done...\n\n\n                Estimate Std. Error     z value   Pr(&gt;|z|)         OR\n(Intercept)  -2.73250662 0.04587566 -59.5633209 0.00000000 0.06505601\nFATAL_OR_M    0.81401380 0.08380692   9.7129660 0.00000000 2.25694878\nOVERTURNED    0.92892138 0.10916632   8.5092302 0.00000000 2.53177687\nCELL_PHONE    0.02955008 0.19777782   0.1494105 0.88122972 1.02999102\nSPEEDING      1.53897567 0.08054589  19.1068171 0.00000000 4.65981462\nAGGRESSIVE   -0.59691595 0.04777924 -12.4932078 0.00000000 0.55050681\nDRIVER1617   -1.28029596 0.29314717  -4.3674171 0.00001257 0.27795502\nDRIVER65PLUS -0.77466464 0.09585832  -8.0813505 0.00000000 0.46085831\nPCTBACHMOR   -0.00037063 0.00129639  -0.2858974 0.77495667 0.99962944\nMEDHHINC      0.00000280 0.00000134   2.0913870 0.03649338 1.00000280\n                  2.5 %     97.5 %\n(Intercept)  0.05947628 0.07119524\nFATAL_OR_M   1.90991409 2.65313350\nOVERTURNED   2.03462326 3.12242730\nCELL_PHONE   0.68354737 1.48846840\nSPEEDING     3.97413085 5.45020642\nAGGRESSIVE   0.50101688 0.60423487\nDRIVER1617   0.14774429 0.47109277\nDRIVER65PLUS 0.37998364 0.55347851\nPCTBACHMOR   0.99707035 1.00215087\nMEDHHINC     1.00000013 1.00000539\n\n\n\n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n  fit.binary |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |      2374 |        41 |      2415 | \n             |     0.058 |     0.016 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |     38505 |      2444 |     40949 | \n             |     0.942 |     0.984 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary2 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |      2613 |        48 |      2661 | \n             |     0.064 |     0.019 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |     38266 |      2437 |     40703 | \n             |     0.936 |     0.981 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary5 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     19176 |       659 |     19835 | \n             |     0.469 |     0.265 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |     21703 |      1826 |     23529 | \n             |     0.531 |     0.735 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary7 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     37356 |      1935 |     39291 | \n             |     0.914 |     0.779 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      3523 |       550 |      4073 | \n             |     0.086 |     0.221 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary8 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     38370 |      2026 |     40396 | \n             |     0.939 |     0.815 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      2509 |       459 |      2968 | \n             |     0.061 |     0.185 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary9 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     38670 |      2067 |     40737 | \n             |     0.946 |     0.832 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      2209 |       418 |      2627 | \n             |     0.054 |     0.168 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary10 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     38762 |      2077 |     40839 | \n             |     0.948 |     0.836 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      2117 |       408 |      2525 | \n             |     0.052 |     0.164 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary15 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     39743 |      2226 |     41969 | \n             |     0.972 |     0.896 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      1136 |       259 |      1395 | \n             |     0.028 |     0.104 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary20 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     40690 |      2428 |     43118 | \n             |     0.995 |     0.977 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |       189 |        57 |       246 | \n             |     0.005 |     0.023 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary50 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     40875 |      2481 |     43356 | \n             |     1.000 |     0.998 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |         4 |         4 |         8 | \n             |     0.000 |     0.002 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nWe also computed the specificity, sensitivity, and misclassification rates for various probability cut-offs. The results, presented in the cutoff value table, reveal that a cutoff value of 0.02 corresponds to the highest misclassification rate at 0.89. In contrast, cutoff values of 0.2 and 0.5 demonstrate the lowest misclassification rates, both standing at 0.06. This implies that at these cutoff points, only 6% of cases were inaccurately classified, either as false positives or false negatives, indicating a higher accuracy in predictive performance.\n\n\n\n\n\n\n\n  \n    \n      Cut-off Value Table\n    \n    \n    \n      Cut_off_Value\n      Sensitivity\n      Specificity\n      Misclassification_Rate\n    \n  \n  \n    0.02\n0.98\n0.06\n0.89\n    0.03\n0.98\n0.06\n0.88\n    0.05\n0.27\n0.45\n0.52\n    0.07\n0.22\n0.91\n0.13\n    0.08\n0.18\n0.94\n0.1\n    0.09\n0.17\n0.95\n0.1\n    0.10\n0.16\n0.95\n0.1\n    0.15\n0.1\n0.97\n0.1\n    0.2\n0.02\n1\n0.06\n    0.5\n0\n1\n0.06\n  \n  \n  \n\n\n\n\n\n\nROC\nWe also look at the ROC curve to further assess our model. Based on the result, the optimal cut off rate in this scenario, which minimizes both sensitivity and specificity, is 0.06. At this cutoff, the sensitivity is 0.66, meaning the model correctly identifies 66% of alcohol-related crashes. The specificity is 0.55, meaning the model correctly identifies 55% of crashes that are not alcohol related. The result is different from the result shown in the previous section, where the cutoff value 0.2 with the lowest minimum mis-classification rates of 0.06 is considered as the optimal one. The reason is that the former approach focuses more on overall accuracy of the model rather than balancing sensitivity and specificity, while the ROC approach prioritizes a more balanced approach between capturing as many true alcohol-related crashes as possible (sensitivity) and correctly identifying non-alcohol-related crashes (specificity).\nAlso, the AUC (area under the ROC curve), which is usually interpreted as the probability that the model correctly ranks two randomly selected observations where one has 𝑦=1 and the other one has 𝑦=0, provides some insight. To elaborate, the AUC here is 0.64, meaning that there is a 64% chance that the model will be able to distinguish between a crash that was caused by alcohol and one that was not. This AUC is better than random guessing but shows that there is still considerable room for improvement in the model’s ability to discriminate between positive and negative instances.\n\n\n  labels predictions\n1      0  0.06794568\n2      0  0.08305194\n3      0  0.06794568\n4      0  0.03858293\n5      0  0.06794568\n6      1  0.07048039\n\n\n\n\n\n[[1]]\n[1] 0.6398695\n\n\n                  [,1]\nsensitivity 0.66076459\nspecificity 0.54524328\ncutoff      0.06365151\n\n\n\n\nLogistic Regression With Binary Predictors Only\nFor comparison, we also run another logistic regression with the binary predictors only (FATAL_OR_M, OVERTURNED, CELL_PHONE, SPEEDING, AGGRESSIVE, DRIVER1617, DRIVER65PLUS). To elaborate, the OR of FATAL_OR_M is 2.25 (95% CI, 1.9 – 2.64), suggesting that in crashes where there was a fatality or major injury (FATAL_OR_M = 1), the odds of the crash involving drunk driving are 2.25 times higher compared to crashes where there was no fatality or major injury (FATAL_OR_M = 0).In other words, crashes that result in fatalities or major injuries are more than twice as likely to involve drunk driving as those that do not result in such severe outcomes. The OR of OVERTURNED is 2.56(95% CI, 2.06 – 3.16), suggesting that in involved an overturned vehicle, the odds of the crash involving drunk driving are 2.56 times higher compared to crashes where no vehicle was overturned. The OR of CELL_PHONE is 1.03(95% CI, 0.68 – 1.49), suggesting that in crashes where driver was using cellphones, the odds of the crash involving drunk driving are 1.03 times higher compared to crashes where driver wasn’t using cellphones. The OR associated with SPEEDING is 4.67(95% CI, 3.98 – 5.46). This indicates that in crashes involving speeding cars, the likelihood of drunk driving being a factor is 4.67 times greater than in crashes where speeding is not a factor. This substantial increase highlights the strong association between speeding and the likelihood of drunk driving in accidents. Also, the OR for AGGRESSIVE driving is 0.55(95% CI, 0.5 – 0.6). This suggests that in incidents involving aggressive driving, the probability of drunk driving being involved is only 55% of that in crashes where aggressive driving is not present. The OR of DRIVER1617 is 0.28(95% CI, 0.15 – 0.48), suggesting that in crashes involved at least one driver who was 16 or 17 years old, the odd of the crash involving drunk driving is 0.28 times as much as crashes where no driver was 16 or 17 years old. The OR of DRIVER65PLUS is 0.46(95% CI, 0.38 – 0.56), suggesting that in crashes involved at least one driver who was at least 65 years old, the odd of the crash involving drunk driving is 0.46 times as much as crashes where no driver was at least 65 years old.\n\n\n\nCall:\nglm(formula = DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + \n    SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS, family = \"binomial\", \n    data = mydata)\n\nCoefficients:\n             Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)  -2.65190    0.02753 -96.324 &lt; 0.0000000000000002 ***\nFATAL_OR_M    0.80932    0.08376   9.662 &lt; 0.0000000000000002 ***\nOVERTURNED    0.93978    0.10903   8.619 &lt; 0.0000000000000002 ***\nCELL_PHONE    0.03107    0.19777   0.157                0.875    \nSPEEDING      1.54032    0.08053  19.128 &lt; 0.0000000000000002 ***\nAGGRESSIVE   -0.59365    0.04775 -12.433 &lt; 0.0000000000000002 ***\nDRIVER1617   -1.27158    0.29311  -4.338  0.00001436374143265 ***\nDRIVER65PLUS -0.76646    0.09576  -8.004  0.00000000000000121 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19036  on 43363  degrees of freedom\nResidual deviance: 18344  on 43356  degrees of freedom\nAIC: 18360\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n (Intercept)   FATAL_OR_M   OVERTURNED   CELL_PHONE     SPEEDING   AGGRESSIVE \n  0.07051713   2.24636998   2.55942903   1.03156149   4.66608472   0.55230941 \n  DRIVER1617 DRIVER65PLUS \n  0.28038936   0.46465631 \n\n\n\n\nWaiting for profiling to be done...\n\n\n                    OR2      2.5 %    97.5 %\n(Intercept)  0.07051713 0.06678642 0.0743978\nFATAL_OR_M   2.24636998 1.90112455 2.6404533\nOVERTURNED   2.55942903 2.05736015 3.1556897\nCELL_PHONE   1.03156149 0.68459779 1.4907150\nSPEEDING     4.66608472 3.97961862 5.4573472\nAGGRESSIVE   0.55230941 0.50268818 0.6061758\nDRIVER1617   0.28038936 0.14904734 0.4751771\nDRIVER65PLUS 0.46465631 0.38318289 0.5579332\n\n\n\n\nWaiting for profiling to be done...\n\n\n                Estimate Std. Error     z value   Pr(&gt;|z|)        OR2\n(Intercept)  -2.65189961 0.02753107 -96.3238683 0.00000000 0.07051713\nFATAL_OR_M    0.80931557 0.08376150   9.6621431 0.00000000 2.24636998\nOVERTURNED    0.93978420 0.10903433   8.6191585 0.00000000 2.55942903\nCELL_PHONE    0.03107367 0.19777088   0.1571195 0.87515064 1.03156149\nSPEEDING      1.54032033 0.08052787  19.1277908 0.00000000 4.66608472\nAGGRESSIVE   -0.59364687 0.04774781 -12.4329656 0.00000000 0.55230941\nDRIVER1617   -1.27157607 0.29310969  -4.3382260 0.00001436 0.28038936\nDRIVER65PLUS -0.76645727 0.09576440  -8.0035718 0.00000000 0.46465631\n                  2.5 %    97.5 %\n(Intercept)  0.06678642 0.0743978\nFATAL_OR_M   1.90112455 2.6404533\nOVERTURNED   2.05736015 3.1556897\nCELL_PHONE   0.68459779 1.4907150\nSPEEDING     3.97961862 5.4573472\nAGGRESSIVE   0.50268818 0.6061758\nDRIVER1617   0.14904734 0.4751771\nDRIVER65PLUS 0.38318289 0.5579332\n\n\nAs is shown in the result table, similar with the original model all the perdictors, except for the CELL_PHONE, are significant. Also, when looking at the AIC, the AIC for the original model is 18359.6, while the AIC for the new model is 18360.5, suggesting that the original model is a better model.\n\n\n         df      AIC\nmylogit  10 18359.63\nmylogit2  8 18360.47"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#summarizing-findings",
    "href": "MUSA500_Stats/MUSA500_HW3.html#summarizing-findings",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Summarizing findings",
    "text": "Summarizing findings\nIn our study, we analyzed a dataset of 43,364 car crash observations to identify factors associated with crashes involving drunk driving. The dependent variable was ‘DRINKING_D’ (Drinking driver indicator), and various predictors like crash severity, vehicle overturning, cell phone usage, speeding, aggressive driving, age of drivers, and socioeconomic factors including median house income and percentage of individuals getting bachelor’s degree or higher were examined.\nBased on the logistic regression result, factors such as crash severity (‘FATAL_OR_M’), vehicle overturning (‘OVERTURNED’), speeding (‘SPEEDING’), age groups of drivers (‘DRIVER1617’ and ‘DRIVER65PLUS’), and median house income (‘MEDHHINC’) significantly predicted drunk driving incidents, although the ‘MEDHHINC’ is not that significant which has a p-value of 0.036 and beta coefficient of 0.0000028. On the contrary, cell phone usage (‘CELL_PHONE’) and education level (‘PCTBACHMOR’) were not significantly associated with the drunk driver crashes. Overall, the findings align closely with our initial expectations. Most variables exhibited behavior that was anticipated, particularly the strong associations observed between speeding and vehicle overturning with instances of drunk driving. It is also reasonable that the cell phone usage, socioeconomic factor median house income and education level do not necessarily have a significant impact on the likelihood of drunk driving incidents. However, it is a bit surprising that aggressive driving, are associated with ORs less than 1, which is suggesting a lower likelihood of drunk driving but is not following our initial expectation.\n\n\nCounts:\n\n\n\n    0     1 \n40879  2485 \n\n\n\nPercentages:\n\n\n\n       0        1 \n94.26944  5.73056 \n\n\nOur dependent variable DRINKING_D has the appropriate count of each category, having 43364 sample size with 2485 events where DRINKING_D equals 1 occurring, which account for 5.73% of the total. From what Paul Allison proposed, it could be problematic when the sample size is small and there is rarity of the events, which does not really apply in our case. Consequently, the application of logistic regression in our study is appropriate, given the substantial sample size and the proportion of events within it."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#limitations",
    "href": "MUSA500_Stats/MUSA500_HW3.html#limitations",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Limitations",
    "text": "Limitations\nOne limitation of the model may be failing to include all possible confounding variables that could influence the likelihood of drunk driving accidents. For instance, factors like road conditions, weather, or specific traffic patterns, which might have a significant impact, are not considered. In addition, in reality the traffic crash data will have some extent of spatial autocorrelation as certain areas might have higher accident rates due to specific local conditions. However, in our case we did not take those into considerations.\nThe primary limitation of our analysis is reflected in the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, a crucial metric for assessing the model’s predictive accuracy. In our model, the AUC is approximately 0.64, which, according to standard guidelines, is categorized as poor (grade D). This rating indicates a relatively low effectiveness of the model in distinguishing between the binary outcomes, specifically in correctly predicting ‘1’ responses as ‘1s’ and ‘0’ responses as ‘0s’."
  }
]