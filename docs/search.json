[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Hang Zhao, I am a second year Master of Environmental Studies candidate, also pursuing a Certificate in GIS and Spatial Analysis. This website demonstrates my projects and coursework done during my Master’s study at Penn."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "MUSA 550 Final Project Template",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "",
    "text": "Philadelphia, renowned for its rich history and pulsating present, is home to a continually evolving real estate environment. However, a study by the Economic League reveals a nuanced picture: the overall proportion of Philadelphia households grappling with housing cost burden experienced a decrease from 29.8% to 26.7% between 2016 and 2021(Economic League, 2023). Nevertheless, this alteration in cost burden manifested divergently across various income brackets. Considering housing is a fundamental human necessity, ensuring affordability is crucial for maintaining well-being and quality of life. Consequently, comprehending the factors that influence housing values is essential for exerting better control over the housing market and making more strategic, informed decisions.\nThis report aims to explore the relationship between median house values and various neighborhood characteristics within the city of Philadelphia. It is widely understood that property values are influenced by both the conditions of the housing and the economic status of the property owners. By analyzing data at the Census block group level, we aim to comprehend the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#data-cleaning",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#data-cleaning",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data focuses on a variety of demographic variables in the census data, starting with 1,816 census data by census block level. To further refine the dataset, a systematic data cleansing process is employed that ultimately cleans the dataset into 1,720 observations. Firstly, block groups with a population of less than 40, those without any housing units, and those with median house values lower than $10,000 are identified and flagged for further action. Additionally, an outlier block group in North Philadelphia, characterized by an unusually high median house value (over $800,000) and very low median household income (less than $8,000), is isolated. These identified anomalies are either removed from the dataset or corrected as needed, ensuring that the final dataset consists of 1720 clean and validated observations. Comprehensive documentation and quality checks are performed throughout the process to maintain data integrity and transparency."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-analysis",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nThe first step involves importing a dataset from “RegressionData.csv” into R, examining the distribution of the dependent variable (MEDHVAL) and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES) using histograms, and calculating their mean and standard deviation. Additionally, logarithmic transformations are applied to these variables, with a special transformation (log(1 + [VAR])) used for variables with zero values. The histograms of both the original and transformed variables are created to assess normality. Finally, a summary statistics table is constructed to present the mean and standard deviation of each variable.\nUnderstanding the characteristics of the data set and the distribution of the variables facilitates the assessment of linearity and normality. The method involves plotting scatter plots as well as using histograms to examine the distribution of the data. This process helps to determine the applicability of different regression models, as various models have different assumptions, such as the normality of residuals. This comprehensive approach enables a thorough exploration of the dataset’s characteristics, facilitates data normalization where necessary, and prepares the data for subsequent regression analysis. While it is possible for a non-normally distributed variable to have normally distributed values, it is more likely that if the variable itself is not normally distributed, its residuals will not be normally distributed either. This effort is consistent with the goal of creating interpretable regression models, as normally distributed variables and residuals are easier to interpret and comply with regression assumptions, ultimately improving the reliability and utility of the model for understanding relationships in the data.\nThe next step is to assess the linearity of the relationships between the dependent variable (MEDHVAL) and each of the predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES). The methodology involves creating four scatter plots, one for each predictor, to visually examine the patterns and associations between these variables. This process enables a qualitative assessment of whether the relationships appear to be linear or exhibit other types of trends, which is crucial for determining the suitability of a linear regression model for subsequent analysis. The scatter plots provide a visual representation of the data, aiding in the decision-making process regarding the choice of regression techniques and the understanding of how predictors may influence the dependent variable.\n\\[y = \\beta_{0} + \\beta_{1} * x + ε\\]\nThe third step is assessing the relationships between predictor variables by calculating Pearson correlations, with a focus on identifying multicollinearity among them. The methodology involves using the cor function in R to compute these correlations, producing a correlation matrix. The process entails examining the values in the correlation matrix to determine if any predictors exhibit strong pairwise correlations, which could indicate multicollinearity. Pearson’s correlation coefficient is from -1 to 1, where -1 indicates a strong negative linear relationship, 1 indicates a strong positive linear relationship, and 0 implies no linear relationship. Multicollinearity, where predictor variables are highly correlated with each other, can lead to unstable and unreliable regression results. The aim is to decide whether it’s appropriate to include all four variables as predictors in the regression model based on the observed correlations, ensuring a robust and interpretable model for subsequent analysis.\n\\[\nr = \\frac{\\sum((X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y}))}{\\sqrt{\\sum(X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum(Y_i - \\bar{Y})^2}}\n\\]\nFinally, visualizing spatial patterns and relationships within geographic data by creating choropleth maps for five variables. The methodology involves utilizing the R programming language and the sf package for importing and handling shapefile data, and the ggplot2 package for creating the choropleth maps. The process begins with importing the shapefile and then plotting each variable individually with color scales chosen for clarity and consistency. The final step combines all five maps into a single figure for presentation, facilitating a visual exploration of spatial distributions and correlations among these variables, and enhancing the understanding of geographic patterns in the dataset."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#multiple-regression-analysis",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#multiple-regression-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Multiple Regression Analysis",
    "text": "Multiple Regression Analysis\nOrdinary Least Squares (OLS) regression is a statistical technique to determine the relationships between a variable of interest, known as the dependent variable, and one or more independent explanatory variables, often referred to as predictors. It is often used to assess the strength and direction of the correlations between variables, indicating whether it’s positive, negative, or no correlation. It also evaluates how well the model fits the data, providing goodness of fit information. Each beta coefficient of the predictors demonstrates to what extent the dependent variable will change when one unit changes in one of the predictors, holding all other predictors constant. However, while significant predictor variables indicate a certain relationship, they do not establish causation between variables.\nWe use regression analysis to determine the correlation between the dependent variable, which is the natural log of median house value, represented as LNMEDHVAL, and the predictors which are a proportion of housing units that are vacant PCTVACANT, percent of housing units that are detached single-family houses PCTSINGLES, proportion of residents in Block Group with at least a bachelor’s degree PCTBACHMOR, and the natural log of number of households that income below 100% poverty level LNNBELPOV100. Our equation is shown as follows:\n\\[\nLNMEDHVAL = \\beta_0 + \\beta_1PCTVACANT\\ + \\beta_2PCTSINGLES + \\beta_3PCTBACHMOR + \\beta_4LNNBELPOV100 + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, interpreting the value of the dependent variable when the predictors are 0; \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\) are the slope coefficients of the predictors.\nFor a linear regression model, for any fixed value of independent variable x, there are parameters \\(\\beta_0\\), \\(\\beta_i\\), and 𝜎, where i = 1 in a simple regression and i&gt;1 in a multiple regression, such that y = \\(\\beta_0\\) + \\(\\beta_i\\)xi +\\(\\epsilon\\), where \\(\\epsilon\\) ~ N(0, \\(\\sigma^2\\)). The term \\(\\epsilon\\) is known as an error term or residual, and for each observation i, is defined as a vertical deviation (distance) between the observed value of y and the predicted value of y, denoted by ŷ. In addition, \\(\\epsilon\\)~N(0, \\(\\sigma^2\\)) means that the error terms have a normal distribution with a mean of 0 and variance \\(\\sigma^2\\). This holds for any given value of x, the average error term will be 0, and a typical deviation from the regression line will be \\(\\sigma^2\\) units.\nThere are several assumptions we have to make prior to the linear regression. 1. Check the linearity of each predictor and the dependent variable by creating scatter plots. If no linearity can be observed from the plots, variable transformation or polynomial regression might be better. 2. Examine the normality of residuals by plotting out the histogram. If the histogram is not normally distributed, log transformation may be used to normalize both the dependent variable and predictor. However, sometimes log transformation is not appropriate, especially when there are high zero inflations. 3. Confirm homoscedasticity, which means that the variance of residuals should be constant throughout the different values of x. 4. Predictors should not be strongly correlated with each other, which is also called to prevent multicollinearity. 5. No fewer than 10 observations per predictor.\nGiven n observations on y, and k predictors x1 … xk, the estimates \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), …. \\(\\beta_k\\) are chosen simultaneously to minimize the expression for the Error Sum of Squares (SSE), given by:\n\\[\nSSE=\\sum_{i = 1}^{n}{\\epsilon^2}=\\sum_{i = 1}^{n}{(y - \\hat{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_1i-\\hat{\\beta_2}x_2i-...-\\hat{\\beta_k}x_ki)^2}\n\\]\nwhere ŷ is the predicted y of the model, which equals \\(\\beta_0\\)+\\(\\beta_1\\)\\(x_1\\)+\\(\\beta_2\\)\\(x_2\\)+…..+\\(\\beta_k\\)\\(x_k\\), with the minus sign before it would be demonstrated as the equation in the bracket. SSE represents the sum of squared error, or the sum of squared residuals \\(\\epsilon\\), which is the amount of variability in y that is not explained when accounting for x in the model. There is another term SST, which means the total sum of squares, is demonstrated as the following equation:\n\\[\nSST=\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\frac{\\sum_{i = 1}^{n}{y_i}}{n})^2}\n\\]\nwhere \\(\\bar{y}\\) here represents the overall mean of y values, therefore SST is interpreted as the squared deviation of that observation from the overall mean of y, and then summing those squared deviations across all observations i, without any regard to the value of x.\n\\[\n\\hat{\\rho}=r=Corr(x,y)=\\frac{\\sum_{i = 1}^{n}{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i = 1}^{n}{(x_i-\\bar{x})^2}}{\\sqrt{\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}}}}\n\\]\nSample correlation coefficient R is a point estimator of the population correlation \\(\\rho\\).\nIf we use the formula 1 – SSE/SST, we can get the coefficient of determination \\(R^2\\), which is the proportion of observed variation in the dependent variable y that was explained by the model. Also, it always ranges between 0 and 1.\n\\[\nR^2 = 1-\\frac{SSE}{SST}\n\\]\nTo assess our model, we examine the F-statistic and its corresponding p-value. The F-test, often referred to as the omnibus test, evaluates whether any of the independent variables in the model significantly predict the dependent variable. It tests the null hypothesis that none of the independent variables are significant predictors against the alternative hypothesis that at least one of them is. A model that fails to reject the null hypothesis is typically considered less effective. We then focus on the p-value associated with each independent variable. If the p-value for a specific independent variable is below 0.05, we can reject the null hypothesis, indicating that this particular predictor significantly influences the dependent variable. In this case, our null hypothesis, or H0 is that the coefficient \\(\\beta\\) equals zero, and the alternative hypothesis, or Ha states that the coefficient \\(\\beta\\) does not equal zero, demonstrating as H0: \\(\\beta\\)=0 and Ha: \\(\\beta \\neq 0\\);"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-analyses",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-analyses",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Analyses",
    "text": "Additional Analyses\nTo further test the relationship between median house values and studied neighborhood characteristics, we also run the stepwise regression. Stepwise regression is a statistical method that allows us to understand the statistical relationship between independent and dependent variables. The process of stepwise regression screens candidate variables and automatically identifies influential variables. In this scenario, stepwise regression is used to examine the statistical relationship between the dependent variable (MEDHVAL), and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, and PCTSINGLES) based on the Akaike Information Criterion (a mathematical method for evaluating how well a model fits the data it was generated from). Specifically, the algorithm adds or removes predictors to see if there is a significant change in the model fit determined by the AIC value and retains all predictors resulting in significant changes. The model with a smaller ACI is usually regarded as a better one. However, there are limitations as well. Firstly, stepwise regression often leads to overfitting. To be more specific, sometimes the dataset does not contain enough data samples to accurately represent all possible input data values, leading to poor generalization to new datasets. Furthermore, rather than relying on professional knowledge, the model relies on an automatic process of selecting predictive variables. Therefore, it may overlook a more comprehensive model.\nTo test the problem of overfitting, we implement the K-fold cross-validation, a method used for evaluating the model performance. To further explain this, in this scenario(k=5), the sample dataset is randomly divided into five folds for training and validation. During each run, one-fold is selected for validation, and the rest are used for training and further iterations. This process is repeated five times, each with a different fold serving as the validation set and the other four as the training set. After this process, we will get five different performance values for each fold, the average of which serves as a holistic performance metric to determine how generalizable our model is. In this scenario, we will use the root mean squared error (RMSE) as the referencing performance value to evaluate the model’s performance. The RMSE measures the average magnitude of errors between the predicted and observed values in a dataset. In other words, it tells us the standard deviation of the residuals (prediction errors).\nTurning to the discussion of the formula of the RMSE calculation, firstly, we need to get the SSE. In the formula below, Xi stands for the observed values, Xn stands for the corresponding predicted values. The SSE is calculated as:\n\\[\nSSE = \\sum_{i = 1}^{n}{(x_i- x_n )^2}\n\\] We can then get the mean squared error (MSE) by dividing SSE by the number of observations n: \\[\nMSE = \\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}\n\\]\nAfter taking the square root of the MSE, we get the value for RMSE: \\[\nRMSE =\\sqrt{\\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}}\n\\] In this study, we initially conduct cross-validation on the regression model incorporating all four predictors. Subsequently, for comparative purposes, we also perform cross-validation on a model using only PCTBANT(housing vacancy) and MEDHHINC(median household income) as predictors."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#software",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#software",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Software",
    "text": "Software\nThe software we used is R, a programming language with powerful statistical analysis capabilities."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-results",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#exploratory-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Results",
    "text": "Exploratory Results\n\nSummary Statistics\nFirst of all, we import the csv and shp data files. To see the fundamental statistical information of the original data, we perform summary() function to see the mean values, and the sd() function to see the standard deviation of each variable. The results are shown below in the table.\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\n\n\n\n\nDependent Variable\n\n\n\n\nMedian House Value\n66287.73\n60006.08\n\n\nPredictors\n\n\n\n\nN of Households Living in Poverty.\n189.7709\n164.3185\n\n\n% of Individuals with Bechelor’s Degrees or Higher\n16.08137\n17.76956\n\n\n% of Vacant Houses\n11.28853\n9.628472\n\n\n% of Single House Units\n9.226473\n13.24925\n\n\n\n\n# look at mean values in summary table\nsummary(data)\n\n    POLY_ID          AREAKEY                MEDHVAL          PCTBACHMOR    \n Min.   :   1.0   Min.   :421010000000   Min.   :  10000   Min.   : 0.000  \n 1st Qu.: 430.8   1st Qu.:421010000000   1st Qu.:  35075   1st Qu.: 4.847  \n Median : 860.5   Median :421010000000   Median :  53250   Median :10.000  \n Mean   : 860.5   Mean   :421010000000   Mean   :  66288   Mean   :16.081  \n 3rd Qu.:1290.2   3rd Qu.:421010000000   3rd Qu.:  78625   3rd Qu.:20.074  \n Max.   :1720.0   Max.   :421010000000   Max.   :1000001   Max.   :92.987  \n    MEDHHINC        PCTVACANT        PCTSINGLES        NBELPOV100    \n Min.   :  2499   Min.   : 0.000   Min.   :  0.000   Min.   :   0.0  \n 1st Qu.: 21060   1st Qu.: 4.372   1st Qu.:  2.110   1st Qu.:  72.0  \n Median : 29719   Median : 9.091   Median :  5.714   Median : 147.0  \n Mean   : 31542   Mean   :11.289   Mean   :  9.226   Mean   : 189.8  \n 3rd Qu.: 38750   3rd Qu.:16.282   3rd Qu.: 11.056   3rd Qu.: 257.0  \n Max.   :200001   Max.   :77.119   Max.   :100.000   Max.   :1267.0  \n\n# print out all the standard deviations\nsd(data$MEDHVAL)\n\n[1] 60006.08\n\nsd(data$NBELPOV100)\n\n[1] 164.3185\n\nsd(data$PCTBACHMOR)\n\n[1] 17.76956\n\nsd(data$PCTVACANT)\n\n[1] 9.628472\n\nsd(data$PCTSINGLES)\n\n[1] 13.24925\n\n\n\n\nHistograms and Log Transformation\nThe histograms below illustrate the distribution of the dependent variable and the four predictors, where all of these histograms are positively skewed. Consequently, we have applied Log transformation to the original variables to normalize their distributions. We have added 1 to the log-transformed data to avoid Log(0) which is undefined. Following that, we present new histograms depicting the distribution of the log-transformed variables.\n\n\n\n\n\n\n\n\nFrom the new histograms, we can see that LNPCTBACHMOR, LNPCTVACANT, LNPCTSINGLES all have zero inflation, which means that there are very high frequency of zero values in the histograms after log transformation. Keeping that in mind, we will only use the log Median House Value presented as LNMEDHVAL (dependent variable), and log Number of Household living in Poverty LNNBELPV100 (one predictor) for the following regression analysis, while keeping the other three variables original.\nOther assumptions for linear regression including checking the linear relationship between dependent variable y and each of the predictors x, homoscedasticity of the variance of residuals, independence of observations, and multicollinearity will also be examined in the following section 3.3\n\n\n\n\n\n\n\n\n\n\nChoropleth maps\nChoropleth maps of each variable, LNMEDHVAL, LNNBELPOV, PCTVACANT, PCTSINGLES, and PCTBACHMOR are presented below. We used 5 quantile breaks as the map representation method. From the maps, we can see that there are some clear overlaps between LNMEDHVAL map(Figure 1) and PCTBACHMOR map(Figure 5), showing a strong correlation of the predictor % of Bachelor’s Degree to the dependent variable Median House Value. Whereas the LNNBELPOV and PCTVACANT maps have completely different patterns from the MEDHVAL map, illustrating very weak correlations. PCTSINGLES however, is presenting a partially similar pattern to the MEDHVAL, which may have some extent of correlation to the dependent variable.\nAmong the predictors, there are no obvious similarities between the maps, while LNNBELPOV does show a little overlap with the PCTVACANT, we do not expect there to be severe multicollinearity between the predictors.\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nThe correlation matrix in the graph and table is shown below. It is used to test multicollinearity between the predictors. The table shows that the greatest correlation coefficients between predictors are -0.31, 0.24, and -0.29, which are not strongly correlated. Therefore, no severe multicollinearity between the predictors has been observed, which aligns with the expectations from previous map observations.\n\n\n\n\n\nLook at multicollinearity (exclude the dependent variable in the correlation matrix table)\n\n\n            LNNBELPOV PCTBACHMOR  PCTVACANT PCTSINGLES\nLNNBELPOV   1.0000000 -0.3197668  0.2495470 -0.2905159\nPCTBACHMOR -0.3197668  1.0000000 -0.2983580  0.1975461\nPCTVACANT   0.2495470 -0.2983580  1.0000000 -0.1513734\nPCTSINGLES -0.2905159  0.1975461 -0.1513734  1.0000000"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-results",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Results",
    "text": "Regression Results\nIn our analysis of median house values in Philadelphia, we used a linear regression model to investigate the relationships between several predictor variables and the natural logarithm of median house values(LNMEDHVAL).\nThe final equation is as follows:\n\\[ln(y) = LNMEDHVAL = \\beta_{0} + \\beta_{1}PCTVACANT + \\beta_{2}PCTSINGLES  + \\beta_{3}PCTBACHMOR + \\beta_{4}LNBELPOV + \\epsilon\\]\nFrom the statistical summary table, there are several key findings. Firstly, the F-statistic is high and the P-value is much smaller than 0.05. Therefore, we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Secondly, the coefficients for each predictor variable also provide some insights.\nNotably, a higher percentage of vacant housing units(PCTVACANT) is associated with a significant decrease in median house values, indicating the negative impact of housing vacancy on property values. That is to say, a 1% additional proportion of vacant housing units is associated with a $19 decrease in median house values. In addition, a higher number of households with incomes below 100% of the poverty level(LNNBELPOV) is associated with a significant decrease in median house values as well. As the number of households in poverty changes by 1%, the expected value of median house values changes by \\((1.01^{\\beta_1} - 1)*100 = (1.01^{-.079} - 1) * 100 = -0.0786 \\%\\) Conversely, the percentage of housing units that are detached single-family houses(PCTSINGLES) has a strong positive relationship with house values. 1% additional percentage of housing units that are detached single-family houses is associated with a $3 increase in median house values. Also, a higher proportion of residents with at least a bachelor’s degree(PCTBACHMOR) exhibits a strong positive relationship with house values, showing that areas with a well-educated population tend to have higher property values. Specifically, the expected change in median house values associated with 1 additional percentage of residents who has at least a bachelor’s degree is \\((e^{\\beta_1} - 1)*100\\% = (e^{.021} - 1) * 100 \\% = 2.12 \\%\\) Finally, the multiple R-square(0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The adjusted R-squared further takes into account the number of predictors in the model, which is 66.15% in this case.\n\n# run the 'lm' function\nlm1 &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, \n          data=data_cor)\n\n\n# print out the statistical summary table\nsummary(lm1)\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_cor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# check the anova result\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: LNMEDHVAL\n             Df  Sum Sq Mean Sq  F value                Pr(&gt;F)    \nPCTVACANT     1 180.383 180.383 1343.093 &lt; 0.00000000000000022 ***\nPCTSINGLES    1  24.543  24.543  182.741 &lt; 0.00000000000000022 ***\nPCTBACHMOR    1 235.111 235.111 1750.586 &lt; 0.00000000000000022 ***\nLNNBELPOV     1  11.692  11.692   87.054 &lt; 0.00000000000000022 ***\nResiduals  1715 230.332   0.134                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-assumption-checks",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#regression-assumption-checks",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Assumption Checks",
    "text": "Regression Assumption Checks\nIn this section, we’ll be talking about testing model assumptions. Through the exploratory analysis above, we already have a general understanding of the distribution of the variables through histograms. In this part, we will further test whether the assumption of the linear relationship between the dependent variable (LNMEDHVAL) and each predictor is valid.\n\nModel Assumptions: Linearity\nAs we can see from the scatter plots, none of the relationships between the dependent variable (LNMEDHVAL) and each of the predictors appear to be strictly linear. Except for the relationship between median home value (LNMEDHVAL) and the percentage of residents with at least a bachelor’s degree (PCTBACHMOR) which seems to be the most linear. The other scatter plots show data points either concentrated in the center or the lower left corner. Thus, with the exception of the relationship between LNMEDHVAL and PCTBACHMOR, the relationships between the dependent variable and most of the predictors deviate significantly from the assumption of strict linearity.\n\n\n\n\n\n\n\nNormality of residuals\nThe normality of residuals is important for point estimation, confidence intervals, and hypothesis tests only for small samples due to the central limit theorem. In our model, the number of observations reaches more than 1,400. Meanwhile, it’s easy to notice from the histogram that most of the residuals are clustered around 0 and the trend seems to be normal.\n\n\n\n\n\n\n\nAdditional Checks: Homoscedasticity\nStandardized residuals are residuals divided by their standard error. \\[\ne_i^* \\approx \\frac{\\epsilon_i}{s} \\approx \\frac{\\epsilon_i}{\\sqrt{\\frac{SSE}{n-2}}}\n\\] They are used to compare residuals for different observations to each other. If a particular standardized residual is 2, then the residual itself is 2 (estimated) standard deviations larger than what would be expected from fitting the “correct” model.\nBy examining the ‘Standardized Residual by Predicted Value’ scatter plot, the goal is to discern the presence of heteroscedasticity — a scenario where the variance of residuals differs for various fitted values. A clear pattern or funnel shape in this scatter plot would indicate heteroscedasticity, suggesting systematic under-predictions or over-predictions by the model for certain ranges of fitted values. Upon analysis, the scatter plot demonstrates a relatively consistent spread of residuals across the range of fitted values, pointing towards homoscedasticity.\nAdditionally, some points lie further from the dense cluster, potentially indicating outliers. These extremely standardized residuals can influence model estimates and might warrant further investigation.\n\n\n\n\n\n\n\nSpatial Autocorrelation\nObserving the maps of the dependent variable and the predictors, there’s a discernible spatial autocorrelation between the median house values and the percentage of residents holding at least a bachelor’s degree. Prominent clusters of higher values can be identified in the northwest, northeast, center city, and university city regions of Philadelphia. This suggests that block groups nearby tend to exhibit similar values, challenging the notion that these observations are spatially independent.\n\n\n\n\n\n\n\nStandardized regression residuals map\nFrom the map of the standardized regression residuals, there appear to be clusters of similar color. This suggests that there might be certain areas where the residuals are consistently high or low, which means the model might have systematically overestimated or underestimated the house values. In the map, given the clustering of similar colors in certain regions, there appears to be some degree of positive spatial autocorrelation in the residuals."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-models",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#additional-models",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Models",
    "text": "Additional Models\n\nusing stepwise regression and determine the best model\nAs is depicted in the result of the stepwise model, all 4 predictors in the original model are retained in the final model. To be more specific, compared with other models with some of the variables dropped, the original has the smallest AIC, -3448.16, indicating that the original model does the best prediction.\n\nbest_model &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, data=data_cor)\nstep &lt;- stepAIC(best_model, direction=\"both\")\n\nStart:  AIC=-3448.16\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                      230.33 -3448.2\n- PCTSINGLES    1     2.407 232.74 -3432.3\n- LNNBELPOV100  1    11.692 242.02 -3365.0\n- PCTVACANT     1    51.543 281.87 -3102.8\n- PCTBACHMOR    1   199.014 429.35 -2379.0\n\n# stepwise regression - Analysis of Variance (ANOVA)\nstep$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\nFinal Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n\n  Step Df Deviance Resid. Df Resid. Dev       AIC\n1                       1715   230.3317 -3448.162\n\n\n\n\nK-fold model\nAfter performing cross-validation on the models, we obtained the following results: the RMSE for the original regression model stands at 0.366, while the secondary model has an RMSE of 0.443.\n\nrmse1\n\n[1] 0.3664306\n\n\n\nrmse2\n\n[1] 0.442712"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#conclusion",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#conclusion",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, we examined the relationship between median house values and several neighborhood characteristics using Philadelphia data at the Census block group level. More specifically, linear regression was done between the dependent variable MEDHVAL (Median House Values) and the predictors PCTBACHMOR (Percentage of Bachelor’s Degree or Higher), NBELPOV100 (Number of Households living in Poverty), PCTVACANT (Percentage of Vacant Houses), and PCTSINGLES (Percentage of Single House Units). Because all variables are positively skewed, we applied log transformation to each variable, and decided to use log MEDHVAL, and log NBELPOV100 in our model, because although all variables are normalized after log transformation, the other three variables all have zero inflations, while NBELPOV100 and MEDHVAL do not have/have negligible frequency of zero values.\nAfter that, regression assumptions were checked: multicollinearity, linear relationship between dependent variables and predictors, homoscedasticity of variance of residuals, and normality of residuals, where all the assumption requirements are successfully met in this model. The result of linear regression presented that all four predictors are significant with p values far less than 0.05, which means that the null hypothesis of the beta coefficient equal to zero can be rejected, and all four predictors are significantly correlated with the dependent variable LNMEDHVAL. Within those, predictor PCTBACHMOR demonstrates the most significant association with the LNMEDHVAL.\nFinally, we applied stepwise regression and k-fold cross-validation to further validate our result. Overall, all four predictors are kept in the stepwise regression, and the rmse value (root mean squared error) in our model is significantly lower than the rmse value in the model that only has PCTVACANT and MEDHHINC (Median household income) as predictors. Therefore, it validates that our model performs better."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#model-quality",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#model-quality",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Model Quality",
    "text": "Model Quality\nThe conducted analysis indicates that the regression model is a good one overall. Firstly, based on the regression results, the high F-statistic means that we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Also, the multiple R-square (0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The result of the stepwise regression further supports the strength of the model. Specifically, all predictors in the original model are retained in the final model, which means that the original model is the best. All predictors have a statistically strong relationship with the dependent variable. Moreover, a cross-validation comparison reveals the original model’s superiority. When considering all four variables, the model achieves a lower RMSE of 0.366, while a model based solely on ‘housing vacancy’ and ‘median household income’ has a higher RMSE of 0.443. This lower RMSE implies that the comprehensive model offers better predictive accuracy and alignment with actual values.\nThis analysis confirms a robust relationship between median household value and factors including residents’ educational level, housing vacancy, the proportion of detached single-family houses, and number of households living in poverty. Specifically, housing vacancy rates and the percentage of detached single-family homes can be seen as reflections of housing quality and price trends. Meanwhile, poverty and education levels provide insight into the economic standing and purchasing power of potential homeowners.\nWhile the current model is insightful, there is potential to enhance the model’s comprehensiveness by introducing additional variables. For instance, the age of the housing stock exerts a profound influence on housing prices, and the number of bedrooms within a property can also significantly impact its market value. These considerations underscore the opportunity for enriching the model with a more comprehensive set of predictors."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#limitations",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#limitations",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Limitations",
    "text": "Limitations\nIn terms of the model’s limitations, it’s important to note that the relationships between predictors and the dependent variable are not strictly linear. Only the relationship between PCTBACHMOR and MEDHVAL appears to be relatively linear. This violation of linearity assumptions could potentially introduce bias into parameter estimates and result in inaccurate outcomes. While attempts were made to address this by transforming some predictors using logarithmic transformations, certain predictors still contained significant zero values, so they were retained in their original form. Additionally, some predictors are interrelated with each other; for instance, the percentage of residents with at least a bachelor’s degree is negatively correlated with the poverty status. Furthermore, an examination of the residuals map reveals the presence of spatial autocorrelation, which could lead to inefficient parameter estimates. As such, addressing nonlinear relationships, correlated observations, and spatial autocorrelation is crucial when modeling this data.\nAlso, for the NBELPOV100 variable, we use raw numbers instead of percentages, which might make it difficult to compare across different geographical regions or time periods, as it lacks contexts or normalization. This may further lead to misleading interpretations of the predictor’s effect. On the other hand, it’s also challenging to explain the practical implications of changes in the number of households in poverty without considering the total population or percentage of poverty."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#ridge-and-lasso-regression",
    "href": "MUSA500_Stats/MUSA500_HW1_PredictHouseValues.html#ridge-and-lasso-regression",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Ridge and LASSO Regression",
    "text": "Ridge and LASSO Regression\nRidge and LASSO regression are alternative regression techniques similar to each other that allow for multicollinearity, allow for a larger number of predictors than observations, and deal with overfitting by shrinking the coefficient of variables to 0. However, both Ridge and LASSO regression will result in biased predicted values while the variance becomes lower, and they will increase the complexity of the model and the way of interpretation. The problem of Ridge regression is that all k predictors will be included, that is, it cannot perform variable selection. LASSO regression on the other hand can do variable selection but still has other limitations. Normally, ridge/LASSO regression will be applied when there is severe multicollinearity, few observations relative to the number of predictors, or we would like a better fit for unseen data than with OLS regression. In this case, as our model does not have the problem of multicollinearity and our number of observations is larger than the number of predictors, we assume that it is unnecessary to perform those two regression methods."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "",
    "text": "Philadelphia, renowned for its rich history and pulsating present, is home to a continually evolving real estate environment. However, a study by the Economic League reveals a nuanced picture: the overall proportion of Philadelphia households grappling with housing cost burden experienced a decrease from 29.8% to 26.7% between 2016 and 2021(Economic League, 2023). Nevertheless, this alteration in cost burden manifested divergently across various income brackets. Considering housing is a fundamental human necessity, ensuring affordability is crucial for maintaining well-being and quality of life. Consequently, comprehending the factors that influence housing values is essential for exerting better control over the housing market and making more strategic, informed decisions.\nThis report aims to explore the relationship between median house values and various neighborhood characteristics within the city of Philadelphia. It is widely understood that property values are influenced by both the conditions of the housing and the economic status of the property owners. By analyzing data at the Census block group level, we aim to comprehend the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#data-cleaning",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#data-cleaning",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data focuses on a variety of demographic variables in the census data, starting with 1,816 census data by census block level. To further refine the dataset, a systematic data cleansing process is employed that ultimately cleans the dataset into 1,720 observations. Firstly, block groups with a population of less than 40, those without any housing units, and those with median house values lower than $10,000 are identified and flagged for further action. Additionally, an outlier block group in North Philadelphia, characterized by an unusually high median house value (over $800,000) and very low median household income (less than $8,000), is isolated. These identified anomalies are either removed from the dataset or corrected as needed, ensuring that the final dataset consists of 1720 clean and validated observations. Comprehensive documentation and quality checks are performed throughout the process to maintain data integrity and transparency."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-analysis",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nThe first step involves importing a dataset from “RegressionData.csv” into R, examining the distribution of the dependent variable (MEDHVAL) and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES) using histograms, and calculating their mean and standard deviation. Additionally, logarithmic transformations are applied to these variables, with a special transformation (log(1 + [VAR])) used for variables with zero values. The histograms of both the original and transformed variables are created to assess normality. Finally, a summary statistics table is constructed to present the mean and standard deviation of each variable.\nUnderstanding the characteristics of the data set and the distribution of the variables facilitates the assessment of linearity and normality. The method involves plotting scatter plots as well as using histograms to examine the distribution of the data. This process helps to determine the applicability of different regression models, as various models have different assumptions, such as the normality of residuals. This comprehensive approach enables a thorough exploration of the dataset’s characteristics, facilitates data normalization where necessary, and prepares the data for subsequent regression analysis. While it is possible for a non-normally distributed variable to have normally distributed values, it is more likely that if the variable itself is not normally distributed, its residuals will not be normally distributed either. This effort is consistent with the goal of creating interpretable regression models, as normally distributed variables and residuals are easier to interpret and comply with regression assumptions, ultimately improving the reliability and utility of the model for understanding relationships in the data.\nThe next step is to assess the linearity of the relationships between the dependent variable (MEDHVAL) and each of the predictors (PCBACHMORE, NBELPOV100, PCTVACANT, PCTSINGLES). The methodology involves creating four scatter plots, one for each predictor, to visually examine the patterns and associations between these variables. This process enables a qualitative assessment of whether the relationships appear to be linear or exhibit other types of trends, which is crucial for determining the suitability of a linear regression model for subsequent analysis. The scatter plots provide a visual representation of the data, aiding in the decision-making process regarding the choice of regression techniques and the understanding of how predictors may influence the dependent variable.\n\\[y = \\beta_{0} + \\beta_{1} * x + ε\\]\nThe third step is assessing the relationships between predictor variables by calculating Pearson correlations, with a focus on identifying multicollinearity among them. The methodology involves using the cor function in R to compute these correlations, producing a correlation matrix. The process entails examining the values in the correlation matrix to determine if any predictors exhibit strong pairwise correlations, which could indicate multicollinearity. Pearson’s correlation coefficient is from -1 to 1, where -1 indicates a strong negative linear relationship, 1 indicates a strong positive linear relationship, and 0 implies no linear relationship. Multicollinearity, where predictor variables are highly correlated with each other, can lead to unstable and unreliable regression results. The aim is to decide whether it’s appropriate to include all four variables as predictors in the regression model based on the observed correlations, ensuring a robust and interpretable model for subsequent analysis.\n\\[\nr = \\frac{\\sum((X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y}))}{\\sqrt{\\sum(X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum(Y_i - \\bar{Y})^2}}\n\\]\nFinally, visualizing spatial patterns and relationships within geographic data by creating choropleth maps for five variables. The methodology involves utilizing the R programming language and the sf package for importing and handling shapefile data, and the ggplot2 package for creating the choropleth maps. The process begins with importing the shapefile and then plotting each variable individually with color scales chosen for clarity and consistency. The final step combines all five maps into a single figure for presentation, facilitating a visual exploration of spatial distributions and correlations among these variables, and enhancing the understanding of geographic patterns in the dataset."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#multiple-regression-analysis",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#multiple-regression-analysis",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Multiple Regression Analysis",
    "text": "Multiple Regression Analysis\nOrdinary Least Squares (OLS) regression is a statistical technique to determine the relationships between a variable of interest, known as the dependent variable, and one or more independent explanatory variables, often referred to as predictors. It is often used to assess the strength and direction of the correlations between variables, indicating whether it’s positive, negative, or no correlation. It also evaluates how well the model fits the data, providing goodness of fit information. Each beta coefficient of the predictors demonstrates to what extent the dependent variable will change when one unit changes in one of the predictors, holding all other predictors constant. However, while significant predictor variables indicate a certain relationship, they do not establish causation between variables.\nWe use regression analysis to determine the correlation between the dependent variable, which is the natural log of median house value, represented as LNMEDHVAL, and the predictors which are a proportion of housing units that are vacant PCTVACANT, percent of housing units that are detached single-family houses PCTSINGLES, proportion of residents in Block Group with at least a bachelor’s degree PCTBACHMOR, and the natural log of number of households that income below 100% poverty level LNNBELPOV100. Our equation is shown as follows:\n\\[\nLNMEDHVAL = \\beta_0 + \\beta_1PCTVACANT\\ + \\beta_2PCTSINGLES + \\beta_3PCTBACHMOR + \\beta_4LNNBELPOV100 + \\epsilon\n\\]\nWhere \\(\\beta_0\\) is the y-intercept, interpreting the value of the dependent variable when the predictors are 0; \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\) are the slope coefficients of the predictors.\nFor a linear regression model, for any fixed value of independent variable x, there are parameters \\(\\beta_0\\), \\(\\beta_i\\), and 𝜎, where i = 1 in a simple regression and i&gt;1 in a multiple regression, such that y = \\(\\beta_0\\) + \\(\\beta_i\\)xi +\\(\\epsilon\\), where \\(\\epsilon\\) ~ N(0, \\(\\sigma^2\\)). The term \\(\\epsilon\\) is known as an error term or residual, and for each observation i, is defined as a vertical deviation (distance) between the observed value of y and the predicted value of y, denoted by ŷ. In addition, \\(\\epsilon\\)~N(0, \\(\\sigma^2\\)) means that the error terms have a normal distribution with a mean of 0 and variance \\(\\sigma^2\\). This holds for any given value of x, the average error term will be 0, and a typical deviation from the regression line will be \\(\\sigma^2\\) units.\nThere are several assumptions we have to make prior to the linear regression. 1. Check the linearity of each predictor and the dependent variable by creating scatter plots. If no linearity can be observed from the plots, variable transformation or polynomial regression might be better. 2. Examine the normality of residuals by plotting out the histogram. If the histogram is not normally distributed, log transformation may be used to normalize both the dependent variable and predictor. However, sometimes log transformation is not appropriate, especially when there are high zero inflations. 3. Confirm homoscedasticity, which means that the variance of residuals should be constant throughout the different values of x. 4. Predictors should not be strongly correlated with each other, which is also called to prevent multicollinearity. 5. No fewer than 10 observations per predictor.\nGiven n observations on y, and k predictors x1 … xk, the estimates \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), …. \\(\\beta_k\\) are chosen simultaneously to minimize the expression for the Error Sum of Squares (SSE), given by:\n\\[\nSSE=\\sum_{i = 1}^{n}{\\epsilon^2}=\\sum_{i = 1}^{n}{(y - \\hat{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_1i-\\hat{\\beta_2}x_2i-...-\\hat{\\beta_k}x_ki)^2}\n\\]\nwhere ŷ is the predicted y of the model, which equals \\(\\beta_0\\)+\\(\\beta_1\\)\\(x_1\\)+\\(\\beta_2\\)\\(x_2\\)+…..+\\(\\beta_k\\)\\(x_k\\), with the minus sign before it would be demonstrated as the equation in the bracket. SSE represents the sum of squared error, or the sum of squared residuals \\(\\epsilon\\), which is the amount of variability in y that is not explained when accounting for x in the model. There is another term SST, which means the total sum of squares, is demonstrated as the following equation:\n\\[\nSST=\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}=\\sum_{i = 1}^{n}{(y_i-\\frac{\\sum_{i = 1}^{n}{y_i}}{n})^2}\n\\]\nwhere \\(\\bar{y}\\) here represents the overall mean of y values, therefore SST is interpreted as the squared deviation of that observation from the overall mean of y, and then summing those squared deviations across all observations i, without any regard to the value of x.\n\\[\n\\hat{\\rho}=r=Corr(x,y)=\\frac{\\sum_{i = 1}^{n}{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i = 1}^{n}{(x_i-\\bar{x})^2}}{\\sqrt{\\sum_{i = 1}^{n}{(y_i-\\bar{y})^2}}}}\n\\]\nSample correlation coefficient R is a point estimator of the population correlation \\(\\rho\\).\nIf we use the formula 1 – SSE/SST, we can get the coefficient of determination \\(R^2\\), which is the proportion of observed variation in the dependent variable y that was explained by the model. Also, it always ranges between 0 and 1.\n\\[\nR^2 = 1-\\frac{SSE}{SST}\n\\]\nTo assess our model, we examine the F-statistic and its corresponding p-value. The F-test, often referred to as the omnibus test, evaluates whether any of the independent variables in the model significantly predict the dependent variable. It tests the null hypothesis that none of the independent variables are significant predictors against the alternative hypothesis that at least one of them is. A model that fails to reject the null hypothesis is typically considered less effective. We then focus on the p-value associated with each independent variable. If the p-value for a specific independent variable is below 0.05, we can reject the null hypothesis, indicating that this particular predictor significantly influences the dependent variable. In this case, our null hypothesis, or H0 is that the coefficient \\(\\beta\\) equals zero, and the alternative hypothesis, or Ha states that the coefficient \\(\\beta\\) does not equal zero, demonstrating as H0: \\(\\beta\\)=0 and Ha: \\(\\beta\\)$$0;"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#additional-analyses",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#additional-analyses",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Analyses",
    "text": "Additional Analyses\nTo further test the relationship between median house values and studied neighborhood characteristics, we also run the stepwise regression. Stepwise regression is a statistical method that allows us to understand the statistical relationship between independent and dependent variables. The process of stepwise regression screens candidate variables and automatically identifies influential variables. In this scenario, stepwise regression is used to examine the statistical relationship between the dependent variable (MEDHVAL), and predictors (PCBACHMORE, NBELPOV100, PCTVACANT, and PCTSINGLES) based on the Akaike Information Criterion (a mathematical method for evaluating how well a model fits the data it was generated from). Specifically, the algorithm adds or removes predictors to see if there is a significant change in the model fit determined by the AIC value and retains all predictors resulting in significant changes. The model with a smaller ACI is usually regarded as a better one. However, there are limitations as well. Firstly, stepwise regression often leads to overfitting. To be more specific, sometimes the dataset does not contain enough data samples to accurately represent all possible input data values, leading to poor generalization to new datasets. Furthermore, rather than relying on professional knowledge, the model relies on an automatic process of selecting predictive variables. Therefore, it may overlook a more comprehensive model.\nTo test the problem of overfitting, we implement the K-fold cross-validation, a method used for evaluating the model performance. To further explain this, in this scenario(k=5), the sample dataset is randomly divided into five folds for training and validation. During each run, one-fold is selected for validation, and the rest are used for training and further iterations. This process is repeated five times, each with a different fold serving as the validation set and the other four as the training set. After this process, we will get five different performance values for each fold, the average of which serves as a holistic performance metric to determine how generalizable our model is. In this scenario, we will use the root mean squared error (RMSE) as the referencing performance value to evaluate the model’s performance. The RMSE measures the average magnitude of errors between the predicted and observed values in a dataset. In other words, it tells us the standard deviation of the residuals (prediction errors).\nTurning to the discussion of the formula of the RMSE calculation, firstly, we need to get the SSE. In the formula below, Xi stands for the observed values, Xn stands for the corresponding predicted values. The SSE is calculated as:\n\\[\nSSE = \\sum_{i = 1}^{n}{(x_i- x_n )^2}\n\\] We can then get the mean squared error (MSE) by dividing SSE by the number of observations n: \\[\nMSE = \\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}\n\\]\nAfter taking the square root of the MSE, we get the value for RMSE: \\[\nRMSE =\\sqrt{\\frac{\\sum_{i = 1}^{n}{(x_i- x_n )^2}}{n}}\n\\] In this study, we initially conduct cross-validation on the regression model incorporating all four predictors. Subsequently, for comparative purposes, we also perform cross-validation on a model using only PCTBANT(housing vacancy) and MEDHHINC(median household income) as predictors."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#software",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#software",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Software",
    "text": "Software\nThe software we used is R, a programming language with powerful statistical analysis capabilities."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-results",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#exploratory-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Exploratory Results",
    "text": "Exploratory Results\n\nSummary Statistics\nFirst of all, we import the csv and shp data files. To see the fundamental statistical information of the original data, we perform summary() function to see the mean values, and the sd() function to see the standard deviation of each variable. The results are shown below in the table.\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\n\n\n\n\nDependent Variable\n\n\n\n\nMedian House Value\n66287.73\n60006.08\n\n\nPredictors\n\n\n\n\nN of Households Living in Poverty.\n189.7709\n164.3185\n\n\n% of Individuals with Bechelor’s Degrees or Higher\n16.08137\n17.76956\n\n\n% of Vacant Houses\n11.28853\n9.628472\n\n\n% of Single House Units\n9.226473\n13.24925\n\n\n\n\n# look at mean values in summary table\nsummary(data)\n\n    POLY_ID          AREAKEY                MEDHVAL          PCTBACHMOR    \n Min.   :   1.0   Min.   :421010000000   Min.   :  10000   Min.   : 0.000  \n 1st Qu.: 430.8   1st Qu.:421010000000   1st Qu.:  35075   1st Qu.: 4.847  \n Median : 860.5   Median :421010000000   Median :  53250   Median :10.000  \n Mean   : 860.5   Mean   :421010000000   Mean   :  66288   Mean   :16.081  \n 3rd Qu.:1290.2   3rd Qu.:421010000000   3rd Qu.:  78625   3rd Qu.:20.074  \n Max.   :1720.0   Max.   :421010000000   Max.   :1000001   Max.   :92.987  \n    MEDHHINC        PCTVACANT        PCTSINGLES        NBELPOV100    \n Min.   :  2499   Min.   : 0.000   Min.   :  0.000   Min.   :   0.0  \n 1st Qu.: 21060   1st Qu.: 4.372   1st Qu.:  2.110   1st Qu.:  72.0  \n Median : 29719   Median : 9.091   Median :  5.714   Median : 147.0  \n Mean   : 31542   Mean   :11.289   Mean   :  9.226   Mean   : 189.8  \n 3rd Qu.: 38750   3rd Qu.:16.282   3rd Qu.: 11.056   3rd Qu.: 257.0  \n Max.   :200001   Max.   :77.119   Max.   :100.000   Max.   :1267.0  \n\n# print out all the standard deviations\nsd(data$MEDHVAL)\n\n[1] 60006.08\n\nsd(data$NBELPOV100)\n\n[1] 164.3185\n\nsd(data$PCTBACHMOR)\n\n[1] 17.76956\n\nsd(data$PCTVACANT)\n\n[1] 9.628472\n\nsd(data$PCTSINGLES)\n\n[1] 13.24925\n\n\n\n\nHistograms and Log Transformation\nThe histograms below illustrate the distribution of the dependent variable and the four predictors, where all of these histograms are positively skewed. Consequently, we have applied Log transformation to the original variables to normalize their distributions. We have added 1 to the log-transformed data to avoid Log(0) which is undefined. Following that, we present new histograms depicting the distribution of the log-transformed variables.\n\n\n\n\n\n\n\n\nFrom the new histograms, we can see that LNPCTBACHMOR, LNPCTVACANT, LNPCTSINGLES all have zero inflation, which means that there are very high frequency of zero values in the histograms after log transformation. Keeping that in mind, we will only use the log Median House Value presented as LNMEDHVAL (dependent variable), and log Number of Household living in Poverty LNNBELPV100 (one predictor) for the following regression analysis, while keeping the other three variables original.\nOther assumptions for linear regression including checking the linear relationship between dependent variable y and each of the predictors x, homoscedasticity of the variance of residuals, independence of observations, and multicollinearity will also be examined in the following section 3.3\n\n\n\n\n\n\n\n\n\n\nChoropleth maps\nChoropleth maps of each variable, LNMEDHVAL, LNNBELPOV, PCTVACANT, PCTSINGLES, and PCTBACHMOR are presented below. We used 5 quantile breaks as the map representation method. From the maps, we can see that there are some clear overlaps between LNMEDHVAL map(Figure 1) and PCTBACHMOR map(Figure 5), showing a strong correlation of the predictor % of Bachelor’s Degree to the dependent variable Median House Value. Whereas the LNNBELPOV and PCTVACANT maps have completely different patterns from the MEDHVAL map, illustrating very weak correlations. PCTSINGLES however, is presenting a partially similar pattern to the MEDHVAL, which may have some extent of correlation to the dependent variable.\nAmong the predictors, there are no obvious similarities between the maps, while LNNBELPOV does show a little overlap with the PCTVACANT, we do not expect there to be severe multicollinearity between the predictors.\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\nThe correlation matrix in the graph and table is shown below. It is used to test multicollinearity between the predictors. The table shows that the greatest correlation coefficients between predictors are -0.31, 0.24, and -0.29, which are not strongly correlated. Therefore, no severe multicollinearity between the predictors has been observed, which aligns with the expectations from previous map observations.\n\n\n\n\n\nLook at multicollinearity (exclude the dependent variable in the correlation matrix table)\n\n\n            LNNBELPOV PCTBACHMOR  PCTVACANT PCTSINGLES\nLNNBELPOV   1.0000000 -0.3197668  0.2495470 -0.2905159\nPCTBACHMOR -0.3197668  1.0000000 -0.2983580  0.1975461\nPCTVACANT   0.2495470 -0.2983580  1.0000000 -0.1513734\nPCTSINGLES -0.2905159  0.1975461 -0.1513734  1.0000000"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#regression-results",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#regression-results",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Results",
    "text": "Regression Results\nIn our analysis of median house values in Philadelphia, we used a linear regression model to investigate the relationships between several predictor variables and the natural logarithm of median house values(LNMEDHVAL).\nThe final equation is as follows:\n\\[ln(y) = LNMEDHVAL = \\beta_{0} + \\beta_{1}PCTVACANT + \\beta_{2}PCTSINGLES  + \\beta_{3}PCTBACHMOR + \\beta_{4}LNBELPOV + \\epsilon\\]\nFrom the statistical summary table, there are several key findings. Firstly, the F-statistic is high and the P-value is much smaller than 0.05. Therefore, we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Secondly, the coefficients for each predictor variable also provide some insights.\nNotably, a higher percentage of vacant housing units(PCTVACANT) is associated with a significant decrease in median house values, indicating the negative impact of housing vacancy on property values. That is to say, a 1% additional proportion of vacant housing units is associated with a $19 decrease in median house values. In addition, a higher number of households with incomes below 100% of the poverty level(LNNBELPOV) is associated with a significant decrease in median house values as well. As the number of households in poverty changes by 1%, the expected value of median house values changes by \\((1.01^{\\beta_1} - 1)*100 = (1.01^{-.079} - 1) * 100 = -0.0786 \\%\\) Conversely, the percentage of housing units that are detached single-family houses(PCTSINGLES) has a strong positive relationship with house values. 1% additional percentage of housing units that are detached single-family houses is associated with a $3 increase in median house values. Also, a higher proportion of residents with at least a bachelor’s degree(PCTBACHMOR) exhibits a strong positive relationship with house values, showing that areas with a well-educated population tend to have higher property values. Specifically, the expected change in median house values associated with 1 additional percentage of residents who has at least a bachelor’s degree is \\((e^{\\beta_1} - 1)*100\\% = (e^{.021} - 1) * 100 \\% = 2.12 \\%\\) Finally, the multiple R-square(0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The adjusted R-squared further takes into account the number of predictors in the model, which is 66.15% in this case.\n\n# run the 'lm' function\nlm1 &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, \n          data=data_cor)\n\n\n# print out the statistical summary table\nsummary(lm1)\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_cor)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# check the anova result\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: LNMEDHVAL\n             Df  Sum Sq Mean Sq  F value                Pr(&gt;F)    \nPCTVACANT     1 180.383 180.383 1343.093 &lt; 0.00000000000000022 ***\nPCTSINGLES    1  24.543  24.543  182.741 &lt; 0.00000000000000022 ***\nPCTBACHMOR    1 235.111 235.111 1750.586 &lt; 0.00000000000000022 ***\nLNNBELPOV     1  11.692  11.692   87.054 &lt; 0.00000000000000022 ***\nResiduals  1715 230.332   0.134                                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#regression-assumption-checks",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#regression-assumption-checks",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Regression Assumption Checks",
    "text": "Regression Assumption Checks\nIn this section, we’ll be talking about testing model assumptions. Through the exploratory analysis above, we already have a general understanding of the distribution of the variables through histograms. In this part, we will further test whether the assumption of the linear relationship between the dependent variable (LNMEDHVAL) and each predictor is valid.\n\nModel Assumptions: Linearity\nAs we can see from the scatter plots, none of the relationships between the dependent variable (LNMEDHVAL) and each of the predictors appear to be strictly linear. Except for the relationship between median home value (LNMEDHVAL) and the percentage of residents with at least a bachelor’s degree (PCTBACHMOR) which seems to be the most linear. The other scatter plots show data points either concentrated in the center or the lower left corner. Thus, with the exception of the relationship between LNMEDHVAL and PCTBACHMOR, the relationships between the dependent variable and most of the predictors deviate significantly from the assumption of strict linearity.\n\n\n\n\n\n\n\nNormality of residuals\nThe normality of residuals is important for point estimation, confidence intervals, and hypothesis tests only for small samples due to the central limit theorem. In our model, the number of observations reaches more than 1,400. Meanwhile, it’s easy to notice from the histogram that most of the residuals are clustered around 0 and the trend seems to be normal.\n\n\n\n\n\n\n\nAdditional Checks: Homoscedasticity\nStandardized residuals are residuals divided by their standard error. \\[\ne_i^* \\approx \\frac{\\epsilon_i}{s} \\approx \\frac{\\epsilon_i}{\\sqrt{\\frac{SSE}{n-2}}}\n\\] They are used to compare residuals for different observations to each other. If a particular standardized residual is 2, then the residual itself is 2 (estimated) standard deviations larger than what would be expected from fitting the “correct” model.\nBy examining the ‘Standardized Residual by Predicted Value’ scatter plot, the goal is to discern the presence of heteroscedasticity — a scenario where the variance of residuals differs for various fitted values. A clear pattern or funnel shape in this scatter plot would indicate heteroscedasticity, suggesting systematic under-predictions or over-predictions by the model for certain ranges of fitted values. Upon analysis, the scatter plot demonstrates a relatively consistent spread of residuals across the range of fitted values, pointing towards homoscedasticity.\nAdditionally, some points lie further from the dense cluster, potentially indicating outliers. These extremely standardized residuals can influence model estimates and might warrant further investigation.\n\n\n\n\n\n\n\nSpatial Autocorrelation\nObserving the maps of the dependent variable and the predictors, there’s a discernible spatial autocorrelation between the median house values and the percentage of residents holding at least a bachelor’s degree. Prominent clusters of higher values can be identified in the northwest, northeast, center city, and university city regions of Philadelphia. This suggests that block groups nearby tend to exhibit similar values, challenging the notion that these observations are spatially independent.\n\n\n\n\n\n\n\nStandardized regression residuals map\nFrom the map of the standardized regression residuals, there appear to be clusters of similar color. This suggests that there might be certain areas where the residuals are consistently high or low, which means the model might have systematically overestimated or underestimated the house values. In the map, given the clustering of similar colors in certain regions, there appears to be some degree of positive spatial autocorrelation in the residuals."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#additional-models",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#additional-models",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Additional Models",
    "text": "Additional Models\n\nusing stepwise regression and determine the best model\nAs is depicted in the result of the stepwise model, all 4 predictors in the original model are retained in the final model. To be more specific, compared with other models with some of the variables dropped, the original has the smallest AIC, -3448.16, indicating that the original model does the best prediction.\n\nbest_model &lt;- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, data=data_cor)\nstep &lt;- stepAIC(best_model, direction=\"both\")\n\nStart:  AIC=-3448.16\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n               Df Sum of Sq    RSS     AIC\n&lt;none&gt;                      230.33 -3448.2\n- PCTSINGLES    1     2.407 232.74 -3432.3\n- LNNBELPOV100  1    11.692 242.02 -3365.0\n- PCTVACANT     1    51.543 281.87 -3102.8\n- PCTBACHMOR    1   199.014 429.35 -2379.0\n\n# stepwise regression - Analysis of Variance (ANOVA)\nstep$anova\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\nFinal Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100\n\n\n  Step Df Deviance Resid. Df Resid. Dev       AIC\n1                       1715   230.3317 -3448.162\n\n\n\n\nK-fold model\nAfter performing cross-validation on the models, we obtained the following results: the RMSE for the original regression model stands at 0.366, while the secondary model has an RMSE of 0.443.\n\nrmse1\n\n[1] 0.3664306\n\n\n\nrmse2\n\n[1] 0.442712"
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#conclusion",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#conclusion",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, we examined the relationship between median house values and several neighborhood characteristics using Philadelphia data at the Census block group level. More specifically, linear regression was done between the dependent variable MEDHVAL (Median House Values) and the predictors PCTBACHMOR (Percentage of Bachelor’s Degree or Higher), NBELPOV100 (Number of Households living in Poverty), PCTVACANT (Percentage of Vacant Houses), and PCTSINGLES (Percentage of Single House Units). Because all variables are positively skewed, we applied log transformation to each variable, and decided to use log MEDHVAL, and log NBELPOV100 in our model, because although all variables are normalized after log transformation, the other three variables all have zero inflations, while NBELPOV100 and MEDHVAL do not have/have negligible frequency of zero values.\nAfter that, regression assumptions were checked: multicollinearity, linear relationship between dependent variables and predictors, homoscedasticity of variance of residuals, and normality of residuals, where all the assumption requirements are successfully met in this model. The result of linear regression presented that all four predictors are significant with p values far less than 0.05, which means that the null hypothesis of the beta coefficient equal to zero can be rejected, and all four predictors are significantly correlated with the dependent variable LNMEDHVAL. Within those, predictor PCTBACHMOR demonstrates the most significant association with the LNMEDHVAL.\nFinally, we applied stepwise regression and k-fold cross-validation to further validate our result. Overall, all four predictors are kept in the stepwise regression, and the rmse value (root mean squared error) in our model is significantly lower than the rmse value in the model that only has PCTVACANT and MEDHHINC (Median household income) as predictors. Therefore, it validates that our model performs better."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#model-quality",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#model-quality",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Model Quality",
    "text": "Model Quality\nThe conducted analysis indicates that the regression model is a good one overall. Firstly, based on the regression results, the high F-statistic means that we can reject the null hypothesis that none of the independent variables in the model is a significant predictor of the dependent variable. Also, the multiple R-square (0.6623) indicates that approximately 66.23% of the variance in median house values can be explained by the model. The result of the stepwise regression further supports the strength of the model. Specifically, all predictors in the original model are retained in the final model, which means that the original model is the best. All predictors have a statistically strong relationship with the dependent variable. Moreover, a cross-validation comparison reveals the original model’s superiority. When considering all four variables, the model achieves a lower RMSE of 0.366, while a model based solely on ‘housing vacancy’ and ‘median household income’ has a higher RMSE of 0.443. This lower RMSE implies that the comprehensive model offers better predictive accuracy and alignment with actual values.\nThis analysis confirms a robust relationship between median household value and factors including residents’ educational level, housing vacancy, the proportion of detached single-family houses, and number of households living in poverty. Specifically, housing vacancy rates and the percentage of detached single-family homes can be seen as reflections of housing quality and price trends. Meanwhile, poverty and education levels provide insight into the economic standing and purchasing power of potential homeowners.\nWhile the current model is insightful, there is potential to enhance the model’s comprehensiveness by introducing additional variables. For instance, the age of the housing stock exerts a profound influence on housing prices, and the number of bedrooms within a property can also significantly impact its market value. These considerations underscore the opportunity for enriching the model with a more comprehensive set of predictors."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#limitations",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#limitations",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Limitations",
    "text": "Limitations\nIn terms of the model’s limitations, it’s important to note that the relationships between predictors and the dependent variable are not strictly linear. Only the relationship between PCTBACHMOR and MEDHVAL appears to be relatively linear. This violation of linearity assumptions could potentially introduce bias into parameter estimates and result in inaccurate outcomes. While attempts were made to address this by transforming some predictors using logarithmic transformations, certain predictors still contained significant zero values, so they were retained in their original form. Additionally, some predictors are interrelated with each other; for instance, the percentage of residents with at least a bachelor’s degree is negatively correlated with the poverty status. Furthermore, an examination of the residuals map reveals the presence of spatial autocorrelation, which could lead to inefficient parameter estimates. As such, addressing nonlinear relationships, correlated observations, and spatial autocorrelation is crucial when modeling this data.\nAlso, for the NBELPOV100 variable, we use raw numbers instead of percentages, which might make it difficult to compare across different geographical regions or time periods, as it lacks contexts or normalization. This may further lead to misleading interpretations of the predictor’s effect. On the other hand, it’s also challenging to explain the practical implications of changes in the number of households in poverty without considering the total population or percentage of poverty."
  },
  {
    "objectID": "MUSA500_Stats/Homework1_OLSRegression.html#ridge-and-lasso-regression",
    "href": "MUSA500_Stats/Homework1_OLSRegression.html#ridge-and-lasso-regression",
    "title": "MUSA500 Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "Ridge and LASSO Regression",
    "text": "Ridge and LASSO Regression\nRidge and LASSO regression are alternative regression techniques similar to each other that allow for multicollinearity, allow for a larger number of predictors than observations, and deal with overfitting by shrinking the coefficient of variables to 0. However, both Ridge and LASSO regression will result in biased predicted values while the variance becomes lower, and they will increase the complexity of the model and the way of interpretation. The problem of Ridge regression is that all k predictors will be included, that is, it cannot perform variable selection. LASSO regression on the other hand can do variable selection but still has other limitations. Normally, ridge/LASSO regression will be applied when there is severe multicollinearity, few observations relative to the number of predictors, or we would like a better fit for unseen data than with OLS regression. In this case, as our model does not have the problem of multicollinearity and our number of observations is larger than the number of predictors, we assume that it is unnecessary to perform those two regression methods."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html",
    "href": "MUSA500_Stats/MUSA500_HW2.html",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "",
    "text": "Philadelphia, celebrated for its historical depth and vibrant present, boasts an ever-changing real estate landscape. Yet, a report from the Economic League paints a more intricate image: between 2016 and 2021, the percentage of Philadelphia households struggling with housing costs dipped from 29.8% to 26.7% (Economic League, 2023). Given the essential nature of housing as a basic human need, ensuring its affordability is paramount to sustaining quality of life. Thus, understanding the elements that shape housing prices is key to navigating the housing market more effectively and making wiser, informed choices.\nIn our previous exploration of Philadelphia’s housing landscape, we used Ordinary Least Squares (OLS) regression to examine the relationship between median house value and several neighborhood characteristics, including the proportion of residents in the Block Group with at least a bachelor’s degree, housing vacancy, percentage of housing units that are detached single-family houses, and number of households living poverty. While OLS provided valuable insights, it operates on the assumption of no spatial autocorrelation. However, the real world, especially in the domain of housing and neighborhood dynamics, often defies this assumption as there is the phenomena of spatial autocorrelation, which can lead to biased and inefficient estimates if not addressed in regression models. To confront this inherent spatial nature of our data, in this report, we will venture into spatial lag, spatial error, and geographically weighted regression methodologies to understand if these models can better account for the spatial dependencies lurking in our OLS residuals, offering a more holistic and accurate picture of Philadelphia’s housing valuation dynamics."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#a-description-of-the-concept-of-spatial-autocorrelation",
    "href": "MUSA500_Stats/MUSA500_HW2.html#a-description-of-the-concept-of-spatial-autocorrelation",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "A Description of the Concept of Spatial Autocorrelation",
    "text": "A Description of the Concept of Spatial Autocorrelation\nThere’s a saying: “everything is related to everything else, but near things are more related than distant things.” This adage, known as Tobler’s First Law of Geography, brings to the fore the concept of spatial autocorrelation – the phenomenon where geographically close observations influence each other. In other words, the attributes of places (or events) become more dissimilar as they are located further apart.\nTo evaluate spatial autocorrelation, we use Moran’s I, a correlation coefficient that measures the spatial relationships within a dataset. Essentially, it assesses the similarity of an object to its neighbors.\nTurning to the formula of Moran’s I, the Moran’s I can be calculated as: \\[\nI=\\frac{N\\sum_{i = 1}\\sum_{j = 1}^{n}wij{(x_i-\\bar{x})(x_j-\\bar{x})}}{\\sum_{i = 1}\\sum_{j = 1}^{n}wij{\\sum_{i = 1}^{n}{(x_i-\\bar{x})^2}}}\n\\] In this formula: N is the number of spatial units, Xi is the variable of interest, Xj is the variable value at another location j, X- is the mean of the variable X, wij is a matrix of spatial weights between spatial units i and j.\nIn this report, the weight matrix we use is queen contiguity. Based on this spatial matrix, a unit is considered adjacent (or a neighbor) to another if it shares either a border or a vertex (corner) with the other unit. This concept draws parallels to the movement of a queen in chess, which can traverse any number of squares in vertical, horizontal, or diagonal directions. For a dataset with n observations, this leads to an n x n matrix, commonly referred to as the weight or link matrix, which captures the pairwise spatial associations across the data. In this matrix, a ‘1’ denotes neighboring spatial units, while a ‘0’ signifies non-neighboring units. For this report, we will consistently use this weight matrix. However, it’s generally advisable to test multiple weight matrices to ensure that our findings aren’t solely influenced by the specific matrix chosen.\nTo determine if the spatial autocorrelation, as measured by Moran’s I, is significant, we perform random permutations, which tests the null hypothesis that there is no spatial autocorrelation against the alternative hypothesis that there is significant spatial autocorrelation. In this process, the observed value of Moran’s I is compared to a distribution of Moran’s I values generated from many random permutations of the spatial data. To elaborate, the observed house price values undergo 999 random shuffles, producing a corresponding 999 Moran’s I values from these permutations. Next, we arrange the 1000 Moran’s I values in decreasing order to determine the position of the Moran’s I value for the observed house price variable in relation to the values from the random permutations. If our observed value is in the extreme ends of this distribution (either very high or very low), we reject the null hypothesis, indicating that the observed spatial autocorrelation is significant.\nWhile Moran’s I provides a global measure of spatial autocorrelation, it doesn’t tell us where the local clusters and local spatial outliers are. Local spatial autocorrelation, like Local Indicators of Spatial Association (LISA), allow us to identify specific areas of significant clustering or dispersion. For each block, by looking at the deviations of its housing value from the mean housing values(zi) and that of its neighbors(zj), spatial weights between i and j, and the total number of observations(n), we can then determine if it’s part of a significant cluster of similar values (high-high or low-low) or if it’s an outlier in its neighborhood (high-low or low-high). To be more specific, a positive value indicates that housing value of block i is surrounded by blocks with similar housing values, either all high or all low. A negative value indicates that housing value of block i a positive value indicates that housing value of block I is surrounded by blocks with similar housing values, either all high or all low. Also, a value near zero indicates no significant local spatial autocorrelation.\nSignificance tests for local spatial autocorrelation are based on Monte Carlo permutation approach, which tests the null hypothesis that there is no local spatial autocorrelation at location I against the alternative hypothesis that the local spatial autocorrelation is significant. During the permutation, the housing values of each block will be randomly shuffled for 999 times, based on which we calculate the new Moran’s I value for every location for each permutation. The value of Moran’s I at location i for the original dataset is ranked relative to the list of the values produced by the reshufflings. When values of the Moran’s I at location i for the original dataset are very low or very high relative to the list of results produced by the shuffling procedure, they are significant. A pseudo significance level can be ascertained by observing the rank of the observed value in comparison to the permuted outcomes. For instance, if the value of Moran’s I at location i from the original configuration ranks as the 88th highest out of 999 permutations, it’s viewed as a 88 in 1000 event with a pseudosignificance of p ~ 0.088."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions",
    "href": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "A Review of OLS Regression and Assumptions",
    "text": "A Review of OLS Regression and Assumptions\nIn our OLS regression model, we found that the predictors—PCTVACANT, PCTSINGLES, PCTBACHMOR, and LNNBELPOV100, are significantly correlated with the dependent variable LNMEDHVAL, and we rejected the null hypothesis that all beta coefficients are zero. We thoroughly assessed the regression assumptions for OLS regression, including the normality of residuals, homoscedasticity, absence of multicollinearity, linearity between the dependent variable y and each predictor x, and the independence of observations. While some assumptions are met in our model, there are some that must be challenged. We examined the spatial autocorrelation simply by plotting choropleth graphs, revealing noticeable spatial autocorrelation between the dependent variable and one of the predictors. Furthermore, the standardized regression residuals map exhibits some degree of spatial autocorrelation, challenging the assumption that residuals are random, and the observations are independent.\nTherefore, we are using statistical method to test the spatial autocorrelation, which is called Moran’s I value, indicating whether spatial autocorrelation exists or not. Moran’s I value is between -1 to +1, and the more positive (approaching to +1) the number is, the stronger positive spatial autocorrelations there would be, and more negative (approaching to -1) the more negative spatial autocorrelations.\nTo further assess spatial autocorrelation within OLS residuals, we employed an additional method that involves regressing these residuals against those of nearby locations. For this analysis, two distinct approaches were used to define neighbors, the Rook Neighbor and the Queen Neighbor Matrix. While rook neighbor method only considers the immediate neighbors in the four cardinal directions with directly shared boundaries, queen neighbor accounts for neighbors with shared corners or intersections, thereby considering a broader range of spatial relationships. Generally, the Queen Neighbor Matrix is preferred due to its broader scope.\nIn this process, the resulting residuals are calculated as the average of the residuals from these neighboring locations. We then conducted a linear regression using OLS residuals against these averaged neighbor residuals. The focus of this analysis was on the significance of the relationship and the magnitude of the slope coefficient. A p-value less than 0.05 would lead us to reject the null hypothesis, thereby confirming the presence of spatial autocorrelation, if the slope coefficient is larger than 0.\nHeteroscedasticity is defined as the dispersion of residuals varies by level of predicted variable. To test the assumption of homoscedasticity, we have three tests that can be used in R: the Breusch-Pagan Test, Koenker-Bassett Test, and the White test. The null hypothesis here is that of homoscedasticity. If the p-value is less than 0.05, then we can reject the null hypothesis for the alternate hypothesis of heteroscedasticity.\nAnother assumption, normality of errors, meaning that the errors should be random noise, and they also should be normally distributed. The Jarque-Bera test in R examines the null hypothesis that the residuals are from a normal distribution, whereas the null hypothesis is that the errors are normal while the alternative hypothesis of non-normality."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression",
    "href": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Spatial Lag and Spatial Error Regression",
    "text": "Spatial Lag and Spatial Error Regression\nIn this assignment, we will use R for running spatial lag and spatial error regressions. Spatial lag regression assumes that the value of the dependent variable at one location is associated with the values of that variable in nearby locations.\n\\[\n\\begin{aligned}\nLNMEDHVAL = rho * W_y + \\beta_0 + \\beta_1 * PCTVACANT + \\beta_2 * PCTSINGLE + \\\\\n\\beta_3 * PCTBACHMOR + \\beta_4 * LNNBELPOV100 + \\epsilon\n\\end{aligned}\n\\]\n1)Here, ρ (rho) is the coefficient of the y-lag variable Wy, similar to how β1 is the coefficient of the variable X1. ( ρ is constrained between -1 and 1.) ρ (rho) is the spatial lag of the dependent variable entered as a predictor of the dependent variable. That is, it’s the value of the dependent variable in nearby areas. 2) B0 coefficient is the intercept or constant term which represents the expected mean value of y when all the predictor variables are equal to zero and also when spatially lagged dependent variable is equal to zero. B coefficients are the slope coefficients corresponding to each predictor variable(PCTVACANT, PCBACHOMORE, PCTSINGLES,NBELPOV100) 3) ε is the residuals, which represents the portion of y that cannot be explained by the model, capturing the effects of all other factors affecting y that are not included in the model. Spatial error regression assumes that the residual at one location is associated with residuals at nearby locations. Nearby is as defined by the weighs matrix W(rook, queen, within a certain distance of one another)\n\\[\n\\begin{aligned}\nLNMEDHVAL = \\beta_0 + \\beta_1 * PCTVACANT + \\beta_2 * PCTSINGLE + \\\\\n\\beta_3 * PCTBACHMOR + \\beta_4 * LNNBELPOV100 + \\lambda * W _\\epsilon + u\n\\end{aligned}\n\\]\nSimilarly with spatial lag model. What’s different is that the λ is the coefficient of spatially lagged residuals. Also, the u is simply random noise. We still assume that each of the predictors is linearly related with the dependent variable, that the residuals are normal, and that there should not be multicollinearity. The goal of spatial lag and spatial error regression is to take into consideration the fact there may be spatial dependencies in the residuals/the data. And through these two methods, the residuals will no longer be spatially autocorrelated and less heteroscedastic. Then, we’ll compare the results of spatial lag regression with OLS and the results of spatial error regression with OLS. After which we’ll further talk about how to pick the spatial models that perform the best. When comparing between spatial models, a number of measures are used for model comparability. These criteria include Akaike Information Criterion/Schwarz Criterion; Log Likelihood; Likelihood Ratio Test.\nFirstly, Akaike Information Criterion (AIC) and Schwartz Criterion (SC) are measures of the goodness of fit of an estimated statistical model. They are relative measures of the information that is lost when a given model is used to describe reality and can be said to describe the tradeoff between precision and complexity of the model. Typically, the lower AIC and SC, the better the fit. Secondly, the Log Likelihood is associated with the maximum likelihood method of fitting a statistical model to the data and estimating model parameters. Maximum likelihood picks the values of the model parameters that make data more likely than any other values of the parameters would make them. The higher the log likelihood, the better the model fit. However, this measure should only be used for comparing nested models. In this sense, OLS is a special case of spatial lag and spatial error models, where the coefficient of the weighted residuals term is zero. While spatial lag and spatial error are not a special case of each other. As such, we can’t use the log likelihood ratio to compare them. Thirdly, the Likelihood ratio test compares the OLS model with the spatial model. The null hypothesis is the spatial lag(error) model is not a better specification than the OLS model. If p &lt; 005, we reject the null hypothesis, and state that the spatial lag(error) model is doing a better job than the OLS model. Another way to compare is by examining the Moran’s I statistic applied to the regression residuals in the first place. Moran’s I measures whether the residuals from a regression model exhibit spatial autocorrelation, which implies that the model may not adequately account for the spatial relationships among observations. If the Moran’s I is significantly different from zero, it suggests that there is spatial autocorrelation, indicating that the OLS model may not fully address the spatial aspects.\nThere’s also a different approach to compare OLS results with spatial lag and spatial error results by looking at the Lagrange Multiplier Diagnostics in the regression output. This method provides a strategy for finding the maximum/minimum of a function subject to constraints. Specifically, we can compare the LM(error) vs LM(lag) statistics. If neither of them is statistically significant(p&gt;0.05 for both), it suggests that OLS is a better fit. However, if one of the LM statistics is more statistically significant, indicated by a lower p-value or a higher test statistic value, then that model may be a better choice."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression",
    "href": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Geographically Weighted Regression",
    "text": "Geographically Weighted Regression\nFor this assignment, we will conduct Geographically Weighted Regression (GWR) entirely in R.\nGeographically weighted regression (GWR) is a spatial analysis method that takes non-stationary variables, for example demographic factors in our case, into consideration and models the local regression between these predictors and an outcome of interest (Columbia University, 2023). Simpson’s paradox states that when the population is divided into smaller sub-groups, the relationship between two variables within a population changes, disappears, or even reverses (Sprenger & Weinberger, 2021).\nLocal regression refers to regression for each location, where you will need multiple observations to run a regression not just a single observation. GWR uses other observations in the dataset to run the regression, observations that are close to location I are given greater weights.\nThe equation for GWR model is written for each observation i=1…n:\n\\[\ny_i = \\beta_{i0} + \\beta_{i1} x_{i1} + \\beta_{i2} x_{i2} + \\cdots + \\beta_{im} x_{im} + \\varepsilon_i = \\beta_{i0} + \\sum_{k=1}^{m} \\beta_{ik} x_{ik} + \\varepsilon_i\n\\]\nSubscript i in the equation above indicates that the regression model describes the relationship between the dependent variable y and predictors xk, (k=1…m) around the location of observation i, and that the relationship is specific to that location.\nTo run a local regression, multiple observations (locations) are needed, not just a single observation (location) i. GWR uses other observations in the dataset to run the regression, observations that are close to location i are given greater weights. The weight of an observation varies with location i, observations closer to I have a stronger influence on the estimation of the parameters for location i.\nBandwidth is the distance h to express how farther the weighing kernel is covering. Fixed bandwidth means that although the number of observations will vary around each point I, the bandwidth distance h (and the area) will remain constant. Adaptive bandwidth means that the number of observations will remain fixed, but the area will not be the same. In this case we are going to use adaptive bandwidth, as the fixed bandwidth is more appropriate in a setting where the distribution of the observations is relatively stable across space, while here the polygons are heterogeneously shaped or sized, so adaptive bandwidth is selected.\nMost of the assumptions in OLS still hold in GWR, including the normality of residuals, homoscedasticity, no multicollinearity. Here for multicollinearity, we would look at the condition number in the attribute table, which indicates when the results are unstable due to local multicollinearity. The rule is, the results may not be reliable when the condition number is greater than 30, equal to null, or equal to -1.79769e+308. In addition to those, GWR also requires lots more observations, with at least 300.\nP-value, which is usual to test whether the parameter estimates are significantly different from zero, is not that important in GWR model. As there is one set of parameters associated with each regression point, as well as one set of standard errors, then there are potentially hundreds or thousands of tests that would be required to determine whether parameters are locally significant."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#spatial-autocorrelation",
    "href": "MUSA500_Stats/MUSA500_HW2.html#spatial-autocorrelation",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nThe very premise of spatial autocorrelation lies in the observation that near things are more similar to one another than to things farther away. This is Tobler’s First Law of Geography. Therefore, we begin by defining neighbors for each of the block groups in Philadelphia. Here, we will be using Queen Neighbors.\n\n\nNeighbour list object:\nNumber of regions: 1720 \nNumber of nonzero links: 10526 \nPercentage nonzero weights: 0.3558004 \nAverage number of links: 6.119767 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  27 \n  4  16  52 175 348 493 344 177  62  28  10   4   2   1   1   1   1   1 \n4 least connected regions:\n441 708 1391 1665 with 1 link\n1 most connected region:\n1636 with 27 links\n\n\n\n\n\n\n\n\n\n\n\nGlobal Moran’s I\nAfter conducting thorough computations in R, we obtained a global Moran’s I statistic of 0.79 for our dependent variable LNMEDHVAL. This figure is notably high, indicating a strong level of spatial autocorrelation. To further substantiate this finding, we employed a random permutation test. In this process, we randomly shuffled the values of LNMEDHVAL, recalculated Moran’s I for each shuffle, and repeated this procedure 999 times. This approach helps us to test our null hypothesis (H0) that suggests no spatial correlation, against the alternative hypotheses: Ha1, which posits positive spatial correlation, and Ha2, which suggests negative spatial autocorrelations. From the random permutations test results, we can also observe that with 1000 permutation, the p-value of is far less than 0.05. Moreover, from the histogram we can see that the original Moran’s I value is apparently separate from the other Moran’s I values from the 999 permutations, which means that it is much higher than the other Moran’s I values, indicating that there is significant spatial autocorrelation for our dependent variable LNMEDHVAL.\n\n\n[1] 0.793565\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  data_geom$LNMEDHVAL \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.79356, observed rank = 1000, p-value &lt;\n0.00000000000000022\nalternative hypothesis: two.sided\n\n\n\n\n\nIf the above were not sufficiently convincing, we can also create a plot to visualize the relationship between the LNMEDHVAL of the block groups and their neighbors. If there is no spatial autocorrelation, where there is no relationship between block group observations and those of their neighbors, a clear pattern should not be presented in the plot below. However, we observe that this is not the case.\n\n\n\n\n\n\n\nLocal Moran’s I\nMoving to local Moran’s I, which assesses the similarity of a location to nearby neighbors, we examined both the significance map and the cluster map. The presence of ‘high-high’ and ‘low-low’ clusters signifies strong spatial autocorrelation, indicating that both a location and its neighbors deviate from the global mean in a similar manner (either both above or both below the mean). Conversely, ‘high-low’ and ‘low-high’ clusters suggest dissimilarity between a location and its neighbors, implying an absence of spatial autocorrelation. Lastly, areas marked as ‘not significant’ indicate that the local Moran’s I value does not achieve statistical significance at the 0.05 level.\n\n\n          Ii            E.Ii       Var.Ii     Z.Ii  Pr(z != E(Ii))\n1 5.35196819 -0.003049833231 1.3051455983 4.687394 0.0000027670601\n2 4.41225942 -0.002216601273 0.7590492452 5.066922 0.0000004043007\n3 3.50068095 -0.003049833231 0.7444928827 4.060696 0.0000489266898\n4 2.44447746 -0.000843880799 0.2410048919 4.981074 0.0000006323230\n5 1.88349103 -0.001094174334 0.6259107138 2.382098 0.0172143256212\n6 0.09949306 -0.000001607927 0.0009208032 3.278811 0.0010424535943"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions-results",
    "href": "MUSA500_Stats/MUSA500_HW2.html#a-review-of-ols-regression-and-assumptions-results",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "A Review of OLS Regression and Assumptions: Results",
    "text": "A Review of OLS Regression and Assumptions: Results\n\nOLS Regression: Review\nIn our analysis based on the OLS (Ordinary Least Squares) output from Assignment 1, we concluded that the null hypothesis - which assumes that the beta coefficients of the predictors are zero - can be confidently rejected. This implies that each predictor exhibits a significant correlation with the dependent variable MEDHVAL. Notably, PCTBACHMOR shows the strongest positive correlation, indicating that a 1% increase in the proportion of residents with at least a bachelor’s degree is associated with an approximate 2.09% increase in median house values. Furthermore, the OLS model’s multiple and adjusted R-squared values, approximately 0.66, suggest that our model accounts for about 66% of the variance in LNMEDHVAL.\n\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTSINGLES + PCTVACANT + PCTBACHMOR + \n    LNNBELPOV, data = data_geom)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\n'log Lik.' -711.4933 (df=6)\n\n\n\n\nOLS Regression: Heteroscedasticity\nDuring our diagnostic checks for the OLS model, we identified a bit of heteroscedasticity through the residual plot. To investigate this further, we applied three statistical tests for heteroscedasticity: the Breusch-Pagan test, the Koenker-Bassett test, and the White test. The null hypothesis is that of homoscedasticity, and if the p-value is less than 0.05, then we can reject the null hypothesis for the alternate hypothesis of heteroscedasticity. From the results, we can see that all three tests are giving p values far less than 0.05, indicating that there is a problem with heteroscedasticity. This finding aligns with our earlier observations from Homework 1, where we noticed heteroscedasticity in the plot of standardized residuals versus predicted values.\n\n\n\n    Breusch-Pagan test\n\ndata:  reg\nBP = 113.19, df = 4, p-value &lt; 0.00000000000000022\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  reg\nBP = 42.868, df = 4, p-value = 0.00000001102\n\n\nWhite's test results\n\nNull hypothesis: Homoskedasticity of the residuals\nAlternative hypothesis: Heteroskedasticity of the residuals\nTest Statistic: 43.94\nP-value: 0\n\n\n\n\n\n\n\n\n\nOLS Regression: Normality of errors\nIn addition to heteroscedasticity, we also check the normality of errors using a statistical test called Jarque-Bera test, which examines whether the errors are random noise or not, giving that the null hypothesis of normal distribution of residuals. From the test results, we can conclude that the errors are non-normal as p-value is less than 0.05, rejecting our null hypothesis. However, this statistical test result is differing from the conclusion we obtained in HW1, where we observed a normally distributed histogram of the standardized residuals. This discrepancy might be due to the highly conservative nature of the Jarque-Bera test, which is sensitive to even minor deviations from normality, often leading to their identification as statistically significant. Therefore, even though the histogram of residuals seems to be normal, there still might be slight non-normality existing, which is hard to observe by human eyes.\n\n\n\n    Jarque Bera Test\n\ndata:  reg$residuals\nX-squared = 778.96, df = 2, p-value &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\nOLS Regression: Residuals and Neighbor Residuals\nTo delve deeper into the issue of spatial autocorrelation, we conducted a regression analysis between OLS residuals and Spatial Lag residuals, using a Queen Weighted Matrix. The results revealed a significant beta coefficient with a relatively high value of 0.73. This suggests a strong statistical association between neighboring residuals, where a unit change in the lagged residual corresponds to a 0.73 unit change in the residual. Then we performed the Moran’s I test showing the Moran’s I value of the standardized residuals, which is significant and showing a value of 0.31.Within the scatter plot of spatially lagged residuals against standardized residuals, it also presents a linear correlated pattern, which is telling that there is a positive spatial autocorrelation in the OLS residuals which is problematic.\nWe will test the presence of spatial autocorrelation in two ways: 1) by regressing residuals on their queen neighbors, and 2) by looking at the Moran’s I of the residuals.\n\n\n\n\n\nFirst, let’s regress the OLS standardized residuals on the spatial lag of the OLS residuals (i.e., OLS residuals at the queen neighbors). We can see that the beta coefficient of the lagged residuals is significant and positive (0.732, p&lt;&lt;0.05), meaning that there’s a significant level of spatial autocorrelation in the residuals. This is consistent with Moran’s I of the residuals we see below.\n\n\n\nCall:\nlm(formula = standardised ~ resnb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3685 -0.4450  0.0585  0.4618  5.4435 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) -0.01281    0.02121  -0.604               0.546    \nresnb        0.73235    0.03244  22.576 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8793 on 1718 degrees of freedom\nMultiple R-squared:  0.2288,    Adjusted R-squared:  0.2283 \nF-statistic: 509.7 on 1 and 1718 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\n\nAgain, we can use moran.mc to generate a Moran’s I statistic and a pseudo p-value.\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  standardised \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.3124, observed rank = 1000, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\n\n\n\n\nFrom the above, it is strongly apparent that spatial autocorrelation exists among the regression residuals of the OLS Model. The p-value is very small indicating that Moran’s I is significant. Because there’s clearly spatial autocorrelation in OLS residuals, the OLS Model is inappropriate and we need to consider another method. Here, we will attempt to run the Spatial Lag Model, the Spatial Error Model, and Geographically Weighted Regression."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression-results",
    "href": "MUSA500_Stats/MUSA500_HW2.html#spatial-lag-and-spatial-error-regression-results",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Spatial Lag and Spatial Error Regression Results",
    "text": "Spatial Lag and Spatial Error Regression Results\n\nRegression Analysis: Spatial Lag Regression\n\n\n\nCall:lagsarlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_geom, listw = queenlist)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1.655421 -0.117248  0.018654  0.133126  1.726436 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value              Pr(&gt;|z|)\n(Intercept)  3.89845489  0.20111357  19.3843 &lt; 0.00000000000000022\nPCTVACANT   -0.00852940  0.00074367 -11.4694 &lt; 0.00000000000000022\nPCTSINGLES   0.00203342  0.00051577   3.9425         0.00008063503\nPCTBACHMOR   0.00851381  0.00052193  16.3120 &lt; 0.00000000000000022\nLNNBELPOV   -0.03405466  0.00629287  -5.4116         0.00000006246\n\nRho: 0.6511, LR test value: 911.51, p-value: &lt; 0.000000000000000222\nAsymptotic standard error: 0.01805\n    z-value: 36.072, p-value: &lt; 0.000000000000000222\nWald statistic: 1301.2, p-value: &lt; 0.000000000000000222\n\nLog likelihood: -255.74 for lag model\nML residual variance (sigma squared): 0.071948, (sigma: 0.26823)\nNumber of observations: 1720 \nNumber of parameters estimated: 7 \nAIC: 525.48, (AIC for lm: 1435)\nLM test for residual autocorrelation\ntest value: 67.737, p-value: 0.00000000000000022204\n\n\n\n    Likelihood ratio for spatial linear models\n\ndata:  \nLikelihood ratio = 911.51, df = 1, p-value &lt; 0.00000000000000022\nsample estimates:\nLog likelihood of lagreg    Log likelihood of reg \n               -255.7400                -711.4933 \n\n\n\n    Breusch-Pagan test\n\ndata:  \nBP = 210.76, df = 4, p-value &lt; 0.00000000000000022\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  \nBP = 51.411, df = 4, p-value = 0.0000000001832\n\n\n\n    Jarque Bera Test\n\ndata:  lagreg$residuals\nX-squared = 2756.9, df = 2, p-value &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reslag \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.082412, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\n\nThe spatial lag model yields several key insights. Firstly, the W_LNMEDHAL is 0.6511, signifying a substantial and significant spatial relationship: median house values in one area are moderately strongly linked with those in adjacent areas. Moreover, the predictors, including LNNBELPOV, PCTBACHMOR, PCTSINGLES, and PCTVACNT, have p-values far below the 0.05 threshold, confirming their statistical significance. When comparing with the Ordinary Least Squares (OLS) model, there’s a slight uptick in the p-values for these predictors, hinting at a greater likelihood that the associations between median housing values and these predictors could be attributed to chance.\nAt the same time, as suggested by the result of the Breusch-Pagan test, the p value is way much smaller than 0.05, which suggests that we reject the null hypothesis and acknowledge the presence of heteroscedasticity.\nIn comparing the OLS and Spatial Lag regressions, we turn to the Akaike Information Criterion (AIC), Log Likelihood, and the Likelihood Ratio Test. The Spatial Lag regression yields an AIC of 525.48—markedly lower than the OLS regression’s AIC of 1435—and a Log Likelihood of -255.74, which surpasses the OLS regression’s -711.49. The Likelihood Ratio Test’s p-value is well below 0.05, leading us to discard the null hypothesis that the spatial lag model does not offer a better fit than the OLS model. Additionally, the Moran’s I value for the spatial lag model is a minimal -0.082412, indicating significantly reduced spatial autocorrelation in the residuals compared to the OLS regression. This evidence suggests that the Spatial Lag model provides a better fit.\n\n\nRegression Analysis: Spatial Error Regression\n\n\n\nCall:errorsarlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = data_geom, listw = queenlist)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-1.926477 -0.115408  0.014889  0.133852  1.948664 \n\nType: error \nCoefficients: (asymptotic standard errors) \n               Estimate  Std. Error  z value              Pr(&gt;|z|)\n(Intercept) 10.90643427  0.05346777 203.9815 &lt; 0.00000000000000022\nPCTVACANT   -0.00578308  0.00088670  -6.5220      0.00000000006937\nPCTSINGLES   0.00267792  0.00062083   4.3134      0.00001607389089\nPCTBACHMOR   0.00981293  0.00072896  13.4615 &lt; 0.00000000000000022\nLNNBELPOV   -0.03453409  0.00708933  -4.8713      0.00000110881162\n\nLambda: 0.81492, LR test value: 677.61, p-value: &lt; 0.000000000000000222\nAsymptotic standard error: 0.016373\n    z-value: 49.772, p-value: &lt; 0.000000000000000222\nWald statistic: 2477.2, p-value: &lt; 0.000000000000000222\n\nLog likelihood: -372.6904 for error model\nML residual variance (sigma squared): 0.076551, (sigma: 0.27668)\nNumber of observations: 1720 \nNumber of parameters estimated: 7 \nAIC: NA (not available for weighted model), (AIC for lm: 1435)\n\n\n\n    Likelihood ratio for spatial linear models\n\ndata:  \nLikelihood ratio = 677.61, df = 1, p-value &lt; 0.00000000000000022\nsample estimates:\nLog likelihood of errreg    Log likelihood of reg \n               -372.6904                -711.4933 \n\n\n\n    Breusch-Pagan test\n\ndata:  \nBP = 23.213, df = 4, p-value = 0.0001148\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  \nBP = 5.1627, df = 4, p-value = 0.271\n\n\n\n    Jarque Bera Test\n\ndata:  errreg$residuals\nX-squared = 3507, df = 2, p-value &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reserr \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.094532, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\n\nNow we will also look at the results of Spatial Error regression. Here, we see that lambda has the value of 0.81492 and is significant, indicating that the median house value in an area has a strong relationship with median house value in surrounding areas. Also, the corresponding p-value of all the predictors are smaller than 0.05, indicating that all of them are significant. All predictors maintain p-values under 0.05, underscoring their significance. Yet, relative to the OLS regression, the predictors’ p-values have risen, signaling a diminution in the strength of their statistical significance.\nAt the same time, as suggested by the result of the Breusch-Pagan test, the p value is way much smaller than 0.05, pointing to heteroscedasticity within the Spatial Lag regression residuals.\nWhen we consider the AIC and Log Likelihood for the Spatial Error regression, we find an AIC of 754.985 and a Log Likelihood of -372.6904—both figures are more favorable than those from the OLS model. The Moran’s I value for the Spatial Error regression is just -0.094532, indicating even weaker spatial autocorrelation compared with the OLS model. Given that the Spatial Lag model has the lowest AIC, it stands as the best-performing model.\nLastly, in comparing the Spatial Lag and Spatial Error models, we note that the AIC for the Spatial Lag is lower than that of the Spatial Error Model, suggesting the former as the more predictive model. Since the Spatial Lag and Spatial Error models are not nested, direct comparison using the log-likelihood ratio is not applicable, thus we rely on the AIC for this assessment."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression-results",
    "href": "MUSA500_Stats/MUSA500_HW2.html#geographically-weighted-regression-results",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Geographically Weighted Regression Results",
    "text": "Geographically Weighted Regression Results\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\n\n\n\nBandwidth: 0.381966 AIC: 1278.637 \nBandwidth: 0.618034 AIC: 1333.984 \nBandwidth: 0.236068 AIC: 1209.441 \nBandwidth: 0.145898 AIC: 1115.788 \nBandwidth: 0.09016994 AIC: 1007.261 \nBandwidth: 0.05572809 AIC: 910.3448 \nBandwidth: 0.03444185 AIC: 821.2049 \nBandwidth: 0.02128624 AIC: 737.5153 \nBandwidth: 0.01315562 AIC: 681.5228 \nBandwidth: 0.008130619 AIC: 660.7924 \nBandwidth: 0.005024999 AIC: 714.1722 \nBandwidth: 0.009856235 AIC: 666.9998 \nBandwidth: 0.006944377 AIC: 667.5033 \nBandwidth: 0.008427513 AIC: 661.6706 \nBandwidth: 0.007677515 AIC: 663.5923 \nBandwidth: 0.008171309 AIC: 660.8446 \nBandwidth: 0.008052658 AIC: 661.0577 \nBandwidth: 0.008130619 AIC: 660.7924 \n\n\n\n\nBandwidth: 47374.26 AIC: 1380.089 \nBandwidth: 76576.63 AIC: 1412.319 \nBandwidth: 29326.2 AIC: 1314.423 \nBandwidth: 18171.89 AIC: 1205.382 \nBandwidth: 11278.14 AIC: 1056.784 \nBandwidth: 7017.572 AIC: 904.0994 \nBandwidth: 4384.396 AIC: 773.8094 \nBandwidth: 2757.003 AIC: 701.2702 \nBandwidth: 1751.22 AIC: 920.906 \nBandwidth: 3378.612 AIC: 714.9353 \nBandwidth: 2578.424 AIC: 707.7338 \nBandwidth: 2916.626 AIC: 700.5588 \nBandwidth: 2860.559 AIC: 700.3531 \nBandwidth: 2865.211 AIC: 700.3527 \nBandwidth: 2863.515 AIC: 700.3524 \nBandwidth: 2863.494 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.493 AIC: 700.3524 \nBandwidth: 2862.372 AIC: 700.3525 \nBandwidth: 2863.064 AIC: 700.3524 \nBandwidth: 2863.329 AIC: 700.3524 \nBandwidth: 2863.43 AIC: 700.3524 \nBandwidth: 2863.468 AIC: 700.3524 \nBandwidth: 2863.483 AIC: 700.3524 \nBandwidth: 2863.489 AIC: 700.3524 \nBandwidth: 2863.491 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \nBandwidth: 2863.492 AIC: 700.3524 \n\n\n\n\n[1] 0.008130619\n\n\n[1] 2863.492\n\n\n\n\nCall:\ngwr(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = shps, gweight = gwr.Gauss, adapt = bw, \n    hatmatrix = TRUE, se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.008130619 (about 13 of 1720 data points)\nSummary of GWR coefficient estimates at data points:\n                   Min.    1st Qu.     Median    3rd Qu.       Max.  Global\nX.Intercept.  9.6727618 10.7143173 10.9542384 11.1742009 12.0831381 11.1138\nPCTVACANT    -0.0317407 -0.0142383 -0.0089599 -0.0035770  0.0167916 -0.0192\nPCTSINGLES   -0.0249706 -0.0075550 -0.0016626  0.0042280  0.0143340  0.0030\nPCTBACHMOR    0.0010974  0.0101380  0.0149279  0.0202187  0.0347258  0.0209\nLNNBELPOV    -0.2365244 -0.0733572 -0.0401186 -0.0126657  0.0948768 -0.0789\nNumber of data points: 1720 \nEffective number of parameters (residual: 2traceS - traceS'S): 360.5225 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 1359.477 \nSigma (residual: 2traceS - traceS'S): 0.2762201 \nEffective number of parameters (model: traceS): 257.9061 \nEffective degrees of freedom (model: traceS): 1462.094 \nSigma (model: traceS): 0.2663506 \nSigma (ML): 0.245571 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 660.7924 \nAIC (GWR p. 96, eq. 4.22): 308.7123 \nResidual sum of squares: 103.7248 \nQuasi-global R2: 0.8479244 \n\n\n\n\nCall:\ngwr(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = shps, bandwidth = bw_fixed, gweight = gwr.Gauss, \n    hatmatrix = TRUE, se.fit = TRUE)\nKernel function: gwr.Gauss \nFixed bandwidth: 2863.492 \nSummary of GWR coefficient estimates at data points:\n                   Min.    1st Qu.     Median    3rd Qu.       Max.  Global\nX.Intercept.  9.9111183 10.7329171 10.9397426 11.1639961 14.1200775 11.1138\nPCTVACANT    -0.0469926 -0.0137374 -0.0088796 -0.0038447  0.0778856 -0.0192\nPCTSINGLES   -0.0238330 -0.0073895 -0.0025702  0.0040499  0.0189995  0.0030\nPCTBACHMOR   -0.0860913  0.0118750  0.0168149  0.0213553  0.0306653  0.0209\nLNNBELPOV    -0.4449896 -0.0737744 -0.0433084 -0.0171174  0.1491700 -0.0789\nNumber of data points: 1720 \nEffective number of parameters (residual: 2traceS - traceS'S): 346.718 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 1373.282 \nSigma (residual: 2traceS - traceS'S): 0.2785229 \nEffective number of parameters (model: traceS): 255.6033 \nEffective degrees of freedom (model: traceS): 1464.397 \nSigma (model: traceS): 0.2697189 \nSigma (ML): 0.2488723 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 700.3524 \nAIC (GWR p. 96, eq. 4.22): 352.3471 \nResidual sum of squares: 106.5324 \nQuasi-global R2: 0.843808 \n\n\n\n\nObject of class SpatialPolygonsDataFrame\nCoordinates:\n        min       max\nx 2660604.8 2750171.3\ny  207610.6  304858.8\nIs projected: NA \nproj4string : [NA]\nData attributes:\n     sum.w        X.Intercept.      PCTVACANT           PCTSINGLES       \n Min.   :16.03   Min.   : 9.673   Min.   :-0.031741   Min.   :-0.024971  \n 1st Qu.:24.47   1st Qu.:10.714   1st Qu.:-0.014238   1st Qu.:-0.007555  \n Median :26.64   Median :10.954   Median :-0.008960   Median :-0.001663  \n Mean   :27.48   Mean   :10.937   Mean   :-0.009192   Mean   :-0.002074  \n 3rd Qu.:29.45   3rd Qu.:11.174   3rd Qu.:-0.003577   3rd Qu.: 0.004228  \n Max.   :86.70   Max.   :12.083   Max.   : 0.016792   Max.   : 0.014334  \n   PCTBACHMOR         LNNBELPOV        X.Intercept._se    PCTVACANT_se     \n Min.   :0.001097   Min.   :-0.23652   Min.   :0.09911   Min.   :0.001821  \n 1st Qu.:0.010138   1st Qu.:-0.07336   1st Qu.:0.19114   1st Qu.:0.004201  \n Median :0.014928   Median :-0.04012   Median :0.23474   Median :0.005458  \n Mean   :0.015267   Mean   :-0.04485   Mean   :0.25013   Mean   :0.006536  \n 3rd Qu.:0.020219   3rd Qu.:-0.01267   3rd Qu.:0.29127   3rd Qu.:0.007381  \n Max.   :0.034726   Max.   : 0.09488   Max.   :0.54791   Max.   :0.030192  \n PCTSINGLES_se      PCTBACHMOR_se        LNNBELPOV_se         gwr.e         \n Min.   :0.001177   Min.   :0.0007667   Min.   :0.01707   Min.   :-1.50370  \n 1st Qu.:0.003560   1st Qu.:0.0025261   1st Qu.:0.03521   1st Qu.:-0.09867  \n Median :0.005214   Median :0.0048373   Median :0.04198   Median : 0.01654  \n Mean   :0.005118   Mean   :0.0049127   Mean   :0.04413   Mean   : 0.01099  \n 3rd Qu.:0.006596   3rd Qu.:0.0066118   3rd Qu.:0.05035   3rd Qu.: 0.12800  \n Max.   :0.010560   Max.   :0.0151900   Max.   :0.09856   Max.   : 1.67766  \n      pred           pred.se           localR2       X.Intercept._se_EDF\n Min.   : 9.578   Min.   :0.02931   Min.   :0.1337   Min.   :0.1028     \n 1st Qu.:10.476   1st Qu.:0.05601   1st Qu.:0.5231   1st Qu.:0.1982     \n Median :10.831   Median :0.06773   Median :0.6342   Median :0.2434     \n Mean   :10.871   Mean   :0.07462   Mean   :0.6186   Mean   :0.2594     \n 3rd Qu.:11.232   3rd Qu.:0.08449   3rd Qu.:0.7312   3rd Qu.:0.3021     \n Max.   :13.307   Max.   :0.23204   Max.   :0.8863   Max.   :0.5682     \n PCTVACANT_se_EDF   PCTSINGLES_se_EDF  PCTBACHMOR_se_EDF   LNNBELPOV_se_EDF \n Min.   :0.001889   Min.   :0.001221   Min.   :0.0007951   Min.   :0.01770  \n 1st Qu.:0.004357   1st Qu.:0.003692   1st Qu.:0.0026197   1st Qu.:0.03651  \n Median :0.005661   Median :0.005407   Median :0.0050166   Median :0.04354  \n Mean   :0.006778   Mean   :0.005307   Mean   :0.0050947   Mean   :0.04576  \n 3rd Qu.:0.007654   3rd Qu.:0.006841   3rd Qu.:0.0068568   3rd Qu.:0.05221  \n Max.   :0.031311   Max.   :0.010951   Max.   :0.0157529   Max.   :0.10222  \n   pred.se.1      \n Min.   :0.03040  \n 1st Qu.:0.05808  \n Median :0.07024  \n Mean   :0.07739  \n 3rd Qu.:0.08762  \n Max.   :0.24063  \n\n\n\nGlobal GWR Results\n\nThe overall R-squared: GWR vs OLS regression By comparing the R2 from OLS (0.662) with the Quasi-global R2 from GWR (0.848), we can see that GWR yields a better fit as over 84.8% of variance in the dependent variable can be explained by the predictors, while the OLS can only explain over 66.2%.\nCompare the AIC: GWR vs OLS, Spatial Lag, Special Error We can also compare the Akaike Information Criteria(AIC) among GWR, OLS, Spatial Lag and Spatial Error models. The GWR model yields a lowest AIC with the value of 308.7, which is substantially smaller than the AIC for OLS, Spatial Error and Spatial Lag, which are 1443.4, 754.985 and 525.48 respectively. Therefore, the GWR model seems to be a better fit.\nMoran’s I scatterplot of GWR residuals\n\nFrom the MC simulation of Moran’s I for GWR model, we can see that there is a significant, though potentially weak, spatial autocorrelation present in the model. Specifically, the significance(p-value = 0.006) implies that there is a statistically significant spatial autocorrelation. Although the Moran’s I value is close to zero(0.03), it’s still suggested that there is a tendency for similar values to be clustered together than would be expected by random chance.\nWhen comparing the scatter plots of OLS, spatial lag and spatial error, we can see that the spatial autocorrelation of the OLS model is the most significant, and the distribution of its points is clearly skewed towards the first and third quadrants, suggesting that the variables may exhibit a clear spatial clustering pattern. The other three models have weaker spatial autocorrelation, and the distribution of points is more random and close to the centerline, indicating that the distribution of these variables in space may be more random.\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  resgwr \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.033425, observed rank = 992, p-value = 0.016\nalternative hypothesis: two.sided\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  standardised \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.3124, observed rank = 1000, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reslag \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.082412, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  reserr \nweights: queenlist  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.094532, observed rank = 1, p-value = 0.002\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\nLocal Regression Results\nThe map below shows the standardized coefficients which show the relationship between the predictor and the dependent variable to help us understand regional variation in the model explanatory variables. As we can see, areas with dark blue or red show either negative or positive relationship with the median house value that’s possibly significant. Specifically, the % of housing units with detached single family houses is positively significant with the median household value in Northwest and Northeast of the city, while the relationship become gradually negative when it comes to south Philadelphia near Schuylkill River. For the % of bachelor degree, there is a positive relationship with the dependent variable that’s possibly significant in Northwest of the city, center city, as well as the university city. As for the % of vacant housing and % of households living in poverty, there are negative relationships with the Median Household Value throughout the city as well.\n\n\n\n\n\nWe can also look at the spatial distribution of the local R-squares. We can see that the four predictors (PCBACHMORE,PCTSINGLES,LNNBELPOV,PCTVACANT) do a good job explaining the variance in our dependent variable in most parts of Philadelphia, except the central city & its north, as well as some northeast, west parts of Philadelphia.\n\ntm_shape(shps)+\n  tm_fill(col='localR2',  breaks=c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), n=5, palette = 'Blues')+\n  tm_layout(frame=FALSE)"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#model-comparison",
    "href": "MUSA500_Stats/MUSA500_HW2.html#model-comparison",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Model Comparison",
    "text": "Model Comparison\nGWR vs Spatial Error vs Spatial Lag vs OLS: By looking at the Moran’s I of the residuals & the AIC values from each model in the previous section, we can find that the GWR yields a lowest value in terms of Moran’s I and AIC value. As such, GWR is selected as the best model.\n\nSpatial Lag Model vs OLS: Again, Spatial Lag model yields a lower AIC value and a higher Log Likelihood than the OLS model. In the meantime, the likelihood ratio test is significant, indicating that the Spatial Lag model is better than OLS model.\nSpatial Error Model to OLS: Similarly, Spatial Error model has a lower AIC value and Higher Log Likelihood than the OLS model. And the likelihood ratio test is also significant. Therefore, it stands as the better-performing model.\nGWR to OLS: By comparing the R2 from OLS (0.662) with the Quasi-global R2 from GWR (0.848), we can also come up with the conclusion that the GWR is the better model as well. Also, there are hot spots across space in the standardized coefficients, suggesting that spatial variability is present. It also means that the OLS might not capture the varying spatial relationships between the dependent variable and the four predictors as it violates the assumption of stationarity. In summary, the GWR model is more appropriate than OLS as it can capture the local variations in relationships that a single global model cannot."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#limitations",
    "href": "MUSA500_Stats/MUSA500_HW2.html#limitations",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Limitations",
    "text": "Limitations\n\nSpatial Lag & Regression Models Based on the result of the spatial regressions, there are still limitations of heteroscedasticity. To be more specific, the variance of the residuals is not constant across the range of the independent variables. This suggests that the standard errors of the regression coefficients may be biased, meaning that p-values can be unreliable. To improve the model, there is potential to reference alternative modeling approaches such as weighted least squares or transforming the dependent variable to stabilize the variance of the errors.\nGWR Model GWR model also holds some similar assumptions like the normality of residuals, homoscedasticity and no multicollinearity. Based on our last assignment, the trends of residuals are close to normal. However, there is a concentration of data points towards the lower end of predicted values when checking homoscedasticity which might be an issue.\n\n\n\n\n\n\nAlso,there are also signs of multicollinearity in certain part of the area, for example the Northwest of the city. Appearently, there’re 2+ variables that have similar patterns of high-value clusters in that region. Also, the significance(p-value = 0.006) of the GWR model implies that there is a statistically significant spatial autocorrelation. This can lead to biased standard error estimates and inefficient estimators."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW2.html#clarifications",
    "href": "MUSA500_Stats/MUSA500_HW2.html#clarifications",
    "title": "MUSA500 Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups",
    "section": "Clarifications",
    "text": "Clarifications\nWeighted residuals are the ones from a regression model that have been adjusted for spatial autocorrelation in the error terms, among which the spatial weight matrices are used in the estimation of spatial regression. Spatial Lag model residuals are the differences between the observed values of the dependent variable and the values predicted by the spatial lag model. They are expected to have no autocorrelation if the model can explain the spatial dependence.\nAlso, in the current and earlier versions of ArcGIS Pro, some local R-squares are negative, which obviously makes no sense. Therefore, we mainly use R to do this assignment."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#why-not-ols-regression-for-dv-is-binary",
    "href": "MUSA500_Stats/MUSA500_HW3.html#why-not-ols-regression-for-dv-is-binary",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Why not OLS Regression for DV is binary?",
    "text": "Why not OLS Regression for DV is binary?\nPreviously, we have performed Ordinary Least Squares (OLS) regression for continuous dependent variables, where the beta coefficients represent the amount of changes in the dependent variable Y corresponding to a one-unit increase in predictor X. However, in the current context, the dependent variable is binary, taking on values of either 0 or 1. Consequently, a one-unit increase in X does not translate to a proportional change in Y, since Y can only switch between the binary states of 0 and 1. Therefore, the concept of a beta coefficient increase in Y as used in OLS is inapplicable here."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-overview",
    "href": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-overview",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Logistic Regression: Overview",
    "text": "Logistic Regression: Overview\n\nThe Logit Function\nInstead, predicting P(Y=1|X=x), the probability that Y=1 could be an alternative, with a translator function such that the closer the predicted y value from linear regression model is to negative infinite, the closer our predicted probability is to 0, and the closer the y predicted value is to positive infinite, the closer predicted probability is to 1, with no predicted probabilities are less than 0 or greater than 1. This translator function here we are going to use is the Logit Function, which has an equation looks like this for one predictor:\n\\[\n\\ln\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\varepsilon\n\\] Where p = P(Y=1), and the quantity ‘p/(1-p)’ is called the odds, and the quantity ln(p/(1-p)) is called the log odds, or logit. In another words, the odds may be calculated as the desirable outcomes divided by the undesirable outcomes, showing as follows: \\[\nOdds = \\frac{\\#\\ \\text{desirable outcomes}}{\\#\\ \\text{undesirable outcomes}}\n\\]\nLogistic function, or the inverse-logit function, is another form of logit function, which can be expressed as: \\[\np = \\frac{e^{\\beta_{0} + \\beta_{1} x_{1}}}{1 + e^{\\beta_{0} + \\beta_{1} x_{1}}} = \\frac{1}{1 + e^{-\\left(\\beta_{0} + \\beta_{1} x_{1}\\right)}}\n\\] Express the logistic function in the form of line graph, it looks like a ‘S’ shape, which is plotted below:\n\n\n\n\n\nFrom the graph we can see that when \\(\\beta_0\\) + \\(\\beta_1\\)X1 equals to zero, the probability of Y=1 is 50%; As \\(\\beta_0\\) + \\(\\beta_1\\)X1 becomes larger, p approaches to 1; As \\(\\beta_0\\) + \\(\\beta_1\\)X1 becomes smaller, p approaches to 0.\n\n\nRegression Equation for Logit Model\nOur car crash data set has 43,364 observations. The relevant variables are described below:\n\nDependent Variable: DRINKING_D, Drinking driver indicator (1 = Yes, 0 = No)\nPredictor: FATAL_OR_M, Crash resulted in fatality or major injury (1 = Yes, 0 = No)\nPredictor: OVERTURNED, Crash involved an overturned vehicle (1 = Yes, 0 = No)\nPredictor: CELL_PHONE, Driver was using cell phone (1= Yes, 0 = No)\nPredictor: SPEEDING, Crash involved speeding car (1 = Yes, 0 = No)\nPredictor: AGGRESSIVE, Crash involved aggressive driving (1 = Yes, 0 = No)\nPredictor: DRIVER1617, Crash involved at least one driver who was 16 or 17 years old (1 = Yes, 0 = No)\nPredictor: DRIVER65PLUS, Crash involved at least one driver who was at least 65 years old (1 = Yes, 0 = No)\nPredictor: PCTBACHMOR,% of individuals 25 years of age or older who have at least a bachelor’s degree in the Census Block Group where the crash took place\nPredictor: MEDHHINC, Median household income in the Census Block Group where the crash took place\n\nThe logit function of the regression model, which incorporates multiple predictors (our model includes nine predictors, of which seven are binary and two are continuous), can be articulated as follows:\n\\[\n\\begin{aligned}\nODDS(DRINKING\\_D=1) = \\beta_0 + \\beta_1FATAL\\_OR\\_M + \\beta_2OVERTURNED + \\beta_3CELL\\_PHONE \\\\\n+ \\beta_4SPEEDING + \\beta_5AGGRESSIVE + \\beta_6DRIVER1617\n+ \\beta_7DRIVER65PLUS + \\beta_8PCTBACHMOR \\\\\n+ \\beta_9 MEDHHINC + \\varepsilon\n\\end{aligned}\n\\]\nThe equation represents the log odds of the event where Y equals 1, corresponding to the dependent variable DRINKING_D being 1. This indicates the scenario where a drinking driver is involved. It’s calculated as the ratio of the probability of DRINKING_D equals to 1 (presence of a drinking driver) to the probability of DRINKING_D does not equals to 1 (absence of a drinking driver).\nWhen Beta coefficient is positive, it indicates that as the predictor variable increases, the log odds of the outcome occurring increases, meaning the outcome becomes more likely, when beta coefficient is negative, the log odds of the outcome occurring decreases when predictor variable increases, and the outcome becomes less likely.\nSolving the equation of logit function to make p=P(Y=1), we can get a function generally known as the inverse logit, or the logistic function, in which it has the equation like this:\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x_1}}{1 + e^{\\beta_0 + \\beta_1 x_1}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1)}}\n\\]\nPlugging in the variables that we are interested in, in this case are the car crash model variables:\n\\[\n\\begin{aligned}\np &= P(DRINKING\\_D=1) \\\\\n  &= \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1FATAL\\_OR\\_M + \\beta_2 OVERTURNED + \\ldots + \\beta_9 MEDHHINC)}} \\\\\n  &= \\frac{1}{1 + e^{-\\left(\\beta_0 + \\sum_{i=1}^{9} \\beta_{i} \\times X_{i}\\right)}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#hypothesis-testing",
    "href": "MUSA500_Stats/MUSA500_HW3.html#hypothesis-testing",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nIn executing the logistic regression, our initial step involves testing the null hypothesis (H0), which considers that \\(\\beta_i\\) = 0. This implies that the ith independent variable has no impact on the log-odds of the outcome. We contrast this with the alternative hypothesis (Ha), which asserts that \\(\\beta_i\\) \\(\\neq\\) 0, suggesting that the independent variable in question does indeed affect the log-odds of the outcome. This framework is crucial for determining the significance of each predictor in the model. Then, we will look at the quantity, which is sometimes called the Wald statistic within the context of logistic regression, to examine the hypothesis test. The Wald statistic follows a standard normal distribution under the null hypothesis. Thus, the quantity of the Wald statistics is equivalent to a z-score in a standard normal distribution. Then, we can find the p-value associated with the Wald statistics referencing the standard normal (z) distribution tables. If the Wald statistic is far from zero, which corresponds to a small p-value in the standard normal (z) distribution tables, we can reject the null hypothesis and get the conclusion that the independent variable has a significant effect on the outcome.\nAt the same time, in the context of logistic regression, while the estimated \\(\\beta\\) coefficients provide valuable information about the direction and magnitude of the effect a predictor variable has on the log-odds of the outcome, many statisticians and researchers also interpret the results based on odds ratios. The reason is that odds ratios offer a clear and interpretable measure of the strength and direction of the association between the predictor variables and the binary outcome. An odds ratio (OR) is the exponentiated form of the logistic regression coefficient. This transformation is particularly useful because it translates the coefficients into a multiplicative effect on the odds of the outcome occurring for a one-unit increase in the predictor variable. When the odds ratio is: Greater than 1, it indicates that as the predictor increases by one unit, the odds of the outcome occurring increase. Less than 1, it indicates that as the predictor increases by one unit, the odds of the outcome occurring decrease. Exactly 1, it indicates that as the predictor has no effect on the odds of the outcome occurring."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#quality-of-model-fit",
    "href": "MUSA500_Stats/MUSA500_HW3.html#quality-of-model-fit",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Quality of Model Fit",
    "text": "Quality of Model Fit\nUnlike OLS regression, R-Squared can’t be interpreted in logistic regression as the % of variance explained by the model. As there is no continuous outcome variable to explain variance for. Instead, it models the probability of an event occurring based on other independent variables.\nAkaike Information Criterion (AIC) is a measure of the goodness of fit of an estimated statistical model. It’s a relative measure of the information that is lost when a given model is used to describe reality and can be said to describe the tradeoff between precision and complexity of the model. Typically, the lower AIC, the better the fit.\nSensitivity (True Positive Rate) measures the proportion of actual positives which are correctly identified as such and is complementary to the False Negative Rate. Specificity (Ture Negative Rate) measures the proportion of negatives which are correctly identified as such and is complementary to the False Positive Rate. Misclassification rate refers to both False Negative Rate and False Positive Rate. Technically, we’re looking for more correct predictions, for example, being able to predict a high probability of Y=1 if Y is actually 1, and a low probability of Y=1 if Y is actually 0. That’s being said, higher values of sensitivity and specificity are better. The fitted values 𝑦̂, which refer to probabilities that Y=1.\n\\[\nP(Y = 1) = \\hat{y}_i = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\ldots + \\hat{\\beta}_3 x_{3i}}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\ldots + \\hat{\\beta}_3 x_{3i}}}\n\\]\nAnd similarly, \\[\n\\epsilon = y_i - \\hat{y}_i\n\\]\nThe choice of cut-off value demonstrates the trade-offs between different specificity and sensitivity and really depends on different scenarios. Using a cut-off of 0.5 may not be suitable for imbalanced datasets. Ideally, we should choose a cut-off value that will somehow balance and optimize the sensitivity and specificity.\nROC curve is a way to plot sensitivity (True Positive Rate) against Specificity (False Positive Rate), it can be used to examine predictive quality of the model and determine a best cut-off value by optimizing sensitivity and specificity. When interpreting the ROC Curve plot, a cut-off for which the ROC curve has the minimum distance from the upper left corner of the graph maximizes sensitivity and specificity. We can also look at the Youden index, a cut-off for which sensitivity and specificity is maximized. For area under ROC Curve(AUC), it is a measure of prediction accuracy of the model. Usually, higher AUCs mean that we can find a cut-off value for which both sensitivity and specificity of the model are relatively high. The possible values range between 0.5 and 1, a rough guide for classifying the accuracy is: * .90-1 = excellent; * .80-.90 = good; * .70-.80 = fair; * .60-.70 = poor; * .50-.60 = fail; AUC may be interpreted as the probability that the model correctly ranks two randomly selected observations where one has Y=1 and the other one has Y=0."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#assumptions-of-logistic-regression",
    "href": "MUSA500_Stats/MUSA500_HW3.html#assumptions-of-logistic-regression",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Assumptions of Logistic Regression",
    "text": "Assumptions of Logistic Regression\nWith OLS regression, we need to make sure the linear relationship between dependent variable and each predictor, the normality of residuals, and homoscedasticity. But in Logistic regression, there’re no such assumptions. However, the dependent variable in logistic regression must be binary. Also, there should be independence of observations and no severe multicollinearity. Besides that, larger samples are also needed than for OLS because Maximum Likelihood Estimation is used to estimate regression coefficients. For example, at least 50 observations per predictor are needed, compared to about 10 pre predictor in OLS regression."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis-before-logistic-regression",
    "href": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis-before-logistic-regression",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Exploratory Analysis Before Logistic Regression",
    "text": "Exploratory Analysis Before Logistic Regression\nPrior to executing logistic regression, we will conduct exploratory analyses to gain a thorough understanding of the dataset and the interrelations among variables. Initially, we will go through the process of cross-tabulations. This technique examines the connections between the dependent binary variable and other binary predictors. Utilizing R, we’ll construct contingency tables to display the variables’ frequency distributions. Additionally, we will apply the Chi-Square (\\(\\chi^2\\)) test in R to determine if the distribution of one categorical variable depends on another. For example, a cross-tabulation of the DRINKING_D and FATAL_OR_M variables will test the Null Hypothesis (H0): There is no discrepancy in fatality rates between crashes with alcohol-impaired drivers and those without. Conversely, the Alternative Hypothesis (Ha) states that such a discrepancy exists. A significant \\(\\chi^2\\) statistic, combined with a p-value under 0.05, would suggest enough evidence to reject the null hypothesis. In other words, we can conclude there is a significant association between alcohol impairment in drivers and the likelihood of crash fatalities.\nFurther, we will conduct an independent samples t-test to compare the means of a continuous variable across two distinct groups. This test is designed to discern whether any observed mean differences are random or indicative of a genuine disparity in the population. For instance, using PCTBACHMOR as an example, the test will find out if there is a statistically significant mean difference in PCTBACHMOR between crashes with and without alcohol-impaired drivers. Here, the null hypothesis (H0) states that there is no mean difference in PCTBACHMOR values between the two groups, while the alternative hypothesis (Ha) suggests a disparity. Should the t-statistic be significantly high, and the p-value falls beneath 0.05, we would reject H0, concluding a notable difference in average PCTBACHMOR values between crashes involving alcohol-impaired drivers and those that do not."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis",
    "href": "MUSA500_Stats/MUSA500_HW3.html#exploratory-analysis",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\n\n        CRN DRINKING_D COLLISION_ FATAL_OR_M OVERTURNED CELL_PHONE SPEEDING\n1 200806719          0          7          0          0          0        0\n2 200807695          0          7          1          0          0        0\n3 200808809          0          8          0          0          0        0\n4 200809857          0          5          0          0          0        0\n5 200812736          0          1          0          0          0        0\n6 200907381          1          7          1          0          0        0\n  AGGRESSIVE DRIVER1617 DRIVER65PLUS      AREAKEY PCTBACHMOR MEDHHINC\n1          0          0            0 421010001001    64.4737    49107\n2          1          0            0 421010001001    64.4737    49107\n3          0          0            0 421010001001    64.4737    49107\n4          1          0            0 421010001001    64.4737    49107\n5          0          0            0 421010001001    64.4737    49107\n6          0          0            1 421010001001    64.4737    49107\n\n\n\nTabulation of the Dependent Variable & Predictors\nLet’s look at the tabulation of our binary dependent variable, ‘DRINKING_D’.\n\n\n      CRN              DRINKING_D        COLLISION_      FATAL_OR_M     \n Min.   :200800863   Min.   :0.00000   Min.   :0.000   Min.   :0.00000  \n 1st Qu.:200905452   1st Qu.:0.00000   1st Qu.:2.000   1st Qu.:0.00000  \n Median :201008468   Median :0.00000   Median :4.000   Median :0.00000  \n Mean   :201021771   Mean   :0.05731   Mean   :4.433   Mean   :0.03157  \n 3rd Qu.:201111811   3rd Qu.:0.00000   3rd Qu.:7.000   3rd Qu.:0.00000  \n Max.   :201303171   Max.   :1.00000   Max.   :9.000   Max.   :1.00000  \n   OVERTURNED        CELL_PHONE         SPEEDING         AGGRESSIVE    \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.0000  \n Mean   :0.01665   Mean   :0.01047   Mean   :0.03508   Mean   :0.4483  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  \n   DRIVER1617       DRIVER65PLUS       AREAKEY               PCTBACHMOR    \n Min.   :0.00000   Min.   :0.0000   Min.   :421010001001   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:421010104004   1st Qu.: 5.176  \n Median :0.00000   Median :0.0000   Median :421010194001   Median :10.015  \n Mean   :0.01582   Mean   :0.1005   Mean   :421010195889   Mean   :16.572  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:421010292004   3rd Qu.:20.758  \n Max.   :1.00000   Max.   :1.0000   Max.   :421010366001   Max.   :92.987  \n    MEDHHINC     \n Min.   :  2499  \n 1st Qu.: 21277  \n Median : 29464  \n Mean   : 31513  \n 3rd Qu.: 38068  \n Max.   :200001  \n\n\n\n    0     1 \n40879  2485 \n\n\n\n        0         1 \n0.9426944 0.0573056 \n\n\nWe see that there are 94.3% drivers who drink while driving, and only 5.7% who don’t drink. The probability of drivers who drink while driving can be calculated using the formula\n\\[Probability(DRINKING) = \\frac{Number \\; of \\; Drinking \\; Driver}{Total \\; Number \\; of \\; Drivers} = \\frac{40879}{43364} = .94. \\]\nSimilarly, the odds of drinking drivers can be calculated using the formula\n\\[Odds(DRINKING) = \\frac{Number \\; of \\; Drinking \\; Drivers}{Number \\; of \\; Non-drinking \\; Drivers} = \\frac{40879}{2485} = 16.45. \\]\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$FATAL_OR_M \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     39698 |      1181 |     40879 | \n                  |     0.945 |     0.863 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2297 |       188 |      2485 | \n                  |     0.055 |     0.137 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     41995 |      1369 |     43364 | \n                  |     0.968 |     0.032 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  167.5615     d.f. =  1     p =  0.00000000000000000000000000000000000002522202 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  166.0354     d.f. =  1     p =  0.00000000000000000000000000000000000005434077 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$OVERTURNED \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     40267 |       612 |     40879 | \n                  |     0.944 |     0.848 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2375 |       110 |      2485 | \n                  |     0.056 |     0.152 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     42642 |       722 |     43364 | \n                  |     0.983 |     0.017 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  122.788     d.f. =  1     p =  0.0000000000000000000000000001551762 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  121.0052     d.f. =  1     p =  0.000000000000000000000000000381124 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$CELL_PHONE \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     40453 |       426 |     40879 | \n                  |     0.943 |     0.938 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2457 |        28 |      2485 | \n                  |     0.057 |     0.062 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     42910 |       454 |     43364 | \n                  |     0.990 |     0.010 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  0.162071     d.f. =  1     p =  0.6872569 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  0.09065262     d.f. =  1     p =  0.7633491 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$SPEEDING \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     39618 |      1261 |     40879 | \n                  |     0.947 |     0.829 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2225 |       260 |      2485 | \n                  |     0.053 |     0.171 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     41843 |      1521 |     43364 | \n                  |     0.965 |     0.035 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  376.7808     d.f. =  1     p =  0.000000000000000000000000000000000000000000000000000000000000000000000000000000000006249562 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  374.6039     d.f. =  1     p =  0.00000000000000000000000000000000000000000000000000000000000000000000000000000000001861184 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$AGGRESSIVE \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     22357 |     18522 |     40879 | \n                  |     0.934 |     0.953 |           | \n------------------|-----------|-----------|-----------|\n                1 |      1569 |       916 |      2485 | \n                  |     0.066 |     0.047 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     23926 |     19438 |     43364 | \n                  |     0.552 |     0.448 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  67.60186     d.f. =  1     p =  0.000000000000000200079 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  67.2607     d.f. =  1     p =  0.0000000000000002378758 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$DRIVER1617 \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     40205 |       674 |     40879 | \n                  |     0.942 |     0.983 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2473 |        12 |      2485 | \n                  |     0.058 |     0.017 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     42678 |       686 |     43364 | \n                  |     0.984 |     0.016 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  20.45167     d.f. =  1     p =  0.000006115619 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  19.7097     d.f. =  1     p =  0.000009014275 \n\n \n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n                  | mydata$DRIVER65PLUS \nmydata$DRINKING_D |         0 |         1 | Row Total | \n------------------|-----------|-----------|-----------|\n                0 |     36642 |      4237 |     40879 | \n                  |     0.939 |     0.973 |           | \n------------------|-----------|-----------|-----------|\n                1 |      2366 |       119 |      2485 | \n                  |     0.061 |     0.027 |           | \n------------------|-----------|-----------|-----------|\n     Column Total |     39008 |      4356 |     43364 | \n                  |     0.900 |     0.100 |           | \n------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  80.6047     d.f. =  1     p =  0.000000000000000000275703 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  79.9888     d.f. =  1     p =  0.0000000000000000003765375 \n\n \n\n\nPrior to the predictive modeling, we use the Chi-Square test to determine whether the distribution of the categorical variable varies with respect to the drunk driving. From the table below, we can see that the majority of the variables except cell phone have a P-value lower than 0.05. Therefore, we can reject the null hypothesis and confirm that there are associations between drunk driving and overturned vehicle, speeding car, aggressive driving,young driver, old drivers, crash fatalities.\n\n\n\n\n\n\n  \n    \n      Traffic Accident Statistics - Categorical Variables\n    \n    \n    \n      Category\n      Drinking_N\n      Drinking_Perc\n      Non_Drinking_N\n      Non_Drinking_Perc\n      Total_N\n      p_value\n    \n  \n  \n    FATAL_OR_M\n188\n0.43354\n1,181\n2.72346\n1,369\n0\n    OVERTURNED\n110\n0.25367\n612\n1.41131\n722\n0\n    CELL_PHONE\n28\n0.06457\n426\n0.98238\n454\n0.68726\n    SPEEDING\n260\n0.59958\n1,261\n2.90794\n1,521\n0\n    AGGRESSIVE\n916\n2.11235\n18,522\n42.71285\n19,438\n0\n    DRIVER1617\n12\n0.02767\n674\n1.55428\n686\n0.00001\n    DRIVER65PLUS\n119\n0.27442\n4,237\n9.77078\n4,356\n0\n  \n  \n  \n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  mydata$PCTBACHMOR by mydata$DRINKING_D\nt = -0.10842, df = 2777.5, p-value = 0.9137\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.7991398  0.7153982\nsample estimates:\nmean in group 0 mean in group 1 \n       16.56986        16.61173 \n\n\n\n    Welch Two Sample t-test\n\ndata:  mydata$MEDHHINC by mydata$DRINKING_D\nt = -1.4053, df = 2763.9, p-value = 0.16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1235.2508   203.8544\nsample estimates:\nmean in group 0 mean in group 1 \n       31483.05        31998.75 \n\n\nFor the continuous variables PCTBACHMOR,MEDHHINC, we further examine whether their means differ for drunk driving or non-drunk driving. By using the independent samples t-test, the P-value for both variables are not statistically significantly different for crashes that involve drunk drivers and crashes that don’t. Therefore, it can be concluded that the average values of the variables PCTBACHMOR and MEDHHINC are the same for accidents involving drunk drivers and those not involving them.\n\n\n\n\n\n\n  \n    \n      Traffic Accident Statistics - Continuous Variables\n    \n    \n    \n      Category\n      Drinking_mean\n      Drinking_sd\n      Non_Drinking_mean\n      Non_Drinking_sd\n      P_value\n    \n  \n  \n    PCTBACHMOR\n16.61173\n18.72091\n16.56986\n18.21426\n0.9137\n    MEDHHINC\n31,998.75\n17,810.5\n31,483.05\n16,930.1\n0.16\n  \n  \n  \n\n\n\n\nAmong the independent variables, drunk drivers tend to have higher fatality rate, faster car speed, and higher percentage of overturning cars from the primary exploratory analysis.\n\n\nAssociation"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-assumptions",
    "href": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-assumptions",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Logistic Regression Assumptions",
    "text": "Logistic Regression Assumptions\n\nPairwise Pearson Correlations Matrix\nThe correlation matrix in the graph and table is shown below. It is used to test multicollinearity between the predictors. The table shows that the greatest correlation coefficients between predictors are 0.50 and 0.47, which are not strongly correlated. Therefore, no severe multicollinearity between the predictors has been observed.\n\n\n\n\n\n\n\n               FATAL_OR_M    OVERTURNED    CELL_PHONE      SPEEDING  AGGRESSIVE\nFATAL_OR_M    1.000000000  0.0331959240  0.0021603225  0.0817126678 -0.01104729\nOVERTURNED    0.033195924  1.0000000000 -0.0009897786  0.0594402861  0.01643894\nCELL_PHONE    0.002160322 -0.0009897786  1.0000000000 -0.0036011640 -0.02574299\nSPEEDING      0.081712668  0.0594402861 -0.0036011640  1.0000000000  0.21152537\nAGGRESSIVE   -0.011047295  0.0164389397 -0.0257429929  0.2115253684  1.00000000\nDRIVER1617   -0.002808379  0.0037239674  0.0014851333  0.0160115997  0.02842895\nDRIVER65PLUS -0.012512349 -0.0195009743 -0.0027172590 -0.0328541108  0.01502693\nPCTBACHMOR   -0.014652265  0.0093321352 -0.0012458540 -0.0007390853  0.02712211\nMEDHHINC     -0.018212431  0.0279213029  0.0020998852  0.0117866805  0.04344045\n               DRIVER1617 DRIVER65PLUS    PCTBACHMOR     MEDHHINC\nFATAL_OR_M   -0.002808379 -0.012512349 -0.0146522648 -0.018212431\nOVERTURNED    0.003723967 -0.019500974  0.0093321352  0.027921303\nCELL_PHONE    0.001485133 -0.002717259 -0.0012458540  0.002099885\nSPEEDING      0.016011600 -0.032854111 -0.0007390853  0.011786681\nAGGRESSIVE    0.028428953  0.015026930  0.0271221096  0.043440451\nDRIVER1617    1.000000000 -0.020848417 -0.0026359662  0.022877425\nDRIVER65PLUS -0.020848417  1.000000000  0.0261903901  0.050337711\nPCTBACHMOR   -0.002635966  0.026190390  1.0000000000  0.477869537\nMEDHHINC      0.022877425  0.050337711  0.4778695368  1.000000000\n\n\n\n\nAssumptions Check\nLogistic regression has several assumptions, for example the dependent variable must be binary, independence of observations, no severe multicollinearity, and large sample size needed (at least 50 observations per predictor). Our dataset confirms the binary nature of our dependent variable, DRINKING_D, which is limited to values of 0 or 1. The absence of geographical data in our dataset leads us to assume independence of observations without spatial autocorrelations. We assessed multicollinearity among predictors using a Pearson correlation matrix. Here, we define the multicollinearity as the situation where two or more predictors are very strongly correlated with each other, with r&gt;0.9 or r&lt;-0.9. From the matrix table, it indicates generally low or no significant correlations between the predictors, with a notable, yet understandable, relationship between MEDHHINC (Median House Income) and PCTBACHMOR (Percentage of individuals having bachelor’s degree), reflecting the typical correlation between higher education and income.\nWhile a correlation matrix effectively displays pairwise correlations between variables, it’s important to note that the Pearson correlation assumes variables are continuous, not categorical. Given that most of our predictors are binary, with only a couple being continuous, neither Pearson nor Spearman correlation is ideal for assessing correlation and multicollinearity in our case. Instead, we can use the T-test to analyze correlations between a binary and a continuous variable, and the Chi-Squared test for correlations between two binary variables."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-results",
    "href": "MUSA500_Stats/MUSA500_HW3.html#logistic-regression-results",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Logistic Regression Results",
    "text": "Logistic Regression Results\n\nLogistic Regression With All Predictors\n\n\n\nCall:\nglm(formula = DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + \n    SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS + PCTBACHMOR + \n    MEDHHINC, family = \"binomial\", data = mydata)\n\nCoefficients:\n                 Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)  -2.732506616  0.045875659 -59.563 &lt; 0.0000000000000002 ***\nFATAL_OR_M    0.814013802  0.083806924   9.713 &lt; 0.0000000000000002 ***\nOVERTURNED    0.928921376  0.109166324   8.509 &lt; 0.0000000000000002 ***\nCELL_PHONE    0.029550085  0.197777821   0.149               0.8812    \nSPEEDING      1.538975665  0.080545894  19.107 &lt; 0.0000000000000002 ***\nAGGRESSIVE   -0.596915946  0.047779238 -12.493 &lt; 0.0000000000000002 ***\nDRIVER1617   -1.280295964  0.293147168  -4.367 0.000012572447127933 ***\nDRIVER65PLUS -0.774664640  0.095858315  -8.081 0.000000000000000641 ***\nPCTBACHMOR   -0.000370634  0.001296387  -0.286               0.7750    \nMEDHHINC      0.000002804  0.000001341   2.091               0.0365 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19036  on 43363  degrees of freedom\nResidual deviance: 18340  on 43354  degrees of freedom\nAIC: 18360\n\nNumber of Fisher Scoring iterations: 6\n\n\nBased on the regression results, among all studied predictors, except for CELL_PHONE and PCTBACHMOR with a p value greater than 0.05, the rest of them are all significant. The odd ratio provides some insights. The OR of FATAL_OR_M is 2.26 (95% CI, 1.9 – 2.65), suggesting that in crashes where there was a fatality or major injury (FATAL_OR_M = 1), the odds of the crash involving drunk driving are 2.26 times higher compared to crashes where there was no fatality or major injury (FATAL_OR_M = 0).In other words, crashes that result in fatalities or major injuries are more than twice as likely to involve drunk driving as those that do not result in such severe outcomes. The OR of OVERTURNED is 2.53(95% CI, 2.03 – 3.12), suggesting that in involved an overturned vehicle, the odds of the crash involving drunk driving are 2.53 times higher compared to crashes where no vehicle was overturned. The OR of CELL_PHONE is 1.03(95% CI, 0.68 – 1.49), suggesting that in crashes where driver was using cellphones, the odds of the crash involving drunk driving are 1.03 times higher compared to crashes where driver wasn’t using cellphones. The Odds Ratio (OR) associated with SPEEDING is 4.66(95% CI, 3.97 – 5.45). This indicates that in crashes involving speeding cars, the likelihood of drunk driving being a factor is 4.66 times greater than in crashes where speeding is not a factor. This substantial increase highlights the strong association between speeding and the likelihood of drunk driving in accidents. Also, the OR for AGGRESSIVE driving is 0.55(95% CI, 0.5 – 0.6). This suggests that in incidents involving aggressive driving, the probability of drunk driving being involved is only 55% of that in crashes where aggressive driving is not present. The OR of DRIVER1617 is 0.28(95% CI, 0.15 – 0.47), suggesting that in crashes involved at least one driver who was 16 or 17 years old, the odd of the crash involving drunk driving is 0.28 times as much as crashes where no driver was 16 or 17 years old. The OR of DRIVER65PLUS is 0.46(95% CI, 0.38 – 0.55), suggesting that in crashes involved at least one driver who was at least 65 years old, the odd of the crash involving drunk driving is 0.46 times as much as crashes where no driver was at least 65 years old.\n\n\n (Intercept)   FATAL_OR_M   OVERTURNED   CELL_PHONE     SPEEDING   AGGRESSIVE \n  0.06505601   2.25694878   2.53177687   1.02999102   4.65981462   0.55050681 \n  DRIVER1617 DRIVER65PLUS   PCTBACHMOR     MEDHHINC \n  0.27795502   0.46085831   0.99962944   1.00000280 \n\n\n\n\nWaiting for profiling to be done...\n\n\n                     OR      2.5 %     97.5 %\n(Intercept)  0.06505601 0.05947628 0.07119524\nFATAL_OR_M   2.25694878 1.90991409 2.65313350\nOVERTURNED   2.53177687 2.03462326 3.12242730\nCELL_PHONE   1.02999102 0.68354737 1.48846840\nSPEEDING     4.65981462 3.97413085 5.45020642\nAGGRESSIVE   0.55050681 0.50101688 0.60423487\nDRIVER1617   0.27795502 0.14774429 0.47109277\nDRIVER65PLUS 0.46085831 0.37998364 0.55347851\nPCTBACHMOR   0.99962944 0.99707035 1.00215087\nMEDHHINC     1.00000280 1.00000013 1.00000539\n\n\n\n\nWaiting for profiling to be done...\n\n\n                Estimate Std. Error     z value   Pr(&gt;|z|)         OR\n(Intercept)  -2.73250662 0.04587566 -59.5633209 0.00000000 0.06505601\nFATAL_OR_M    0.81401380 0.08380692   9.7129660 0.00000000 2.25694878\nOVERTURNED    0.92892138 0.10916632   8.5092302 0.00000000 2.53177687\nCELL_PHONE    0.02955008 0.19777782   0.1494105 0.88122972 1.02999102\nSPEEDING      1.53897567 0.08054589  19.1068171 0.00000000 4.65981462\nAGGRESSIVE   -0.59691595 0.04777924 -12.4932078 0.00000000 0.55050681\nDRIVER1617   -1.28029596 0.29314717  -4.3674171 0.00001257 0.27795502\nDRIVER65PLUS -0.77466464 0.09585832  -8.0813505 0.00000000 0.46085831\nPCTBACHMOR   -0.00037063 0.00129639  -0.2858974 0.77495667 0.99962944\nMEDHHINC      0.00000280 0.00000134   2.0913870 0.03649338 1.00000280\n                  2.5 %     97.5 %\n(Intercept)  0.05947628 0.07119524\nFATAL_OR_M   1.90991409 2.65313350\nOVERTURNED   2.03462326 3.12242730\nCELL_PHONE   0.68354737 1.48846840\nSPEEDING     3.97413085 5.45020642\nAGGRESSIVE   0.50101688 0.60423487\nDRIVER1617   0.14774429 0.47109277\nDRIVER65PLUS 0.37998364 0.55347851\nPCTBACHMOR   0.99707035 1.00215087\nMEDHHINC     1.00000013 1.00000539\n\n\n\n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n  fit.binary |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |      2374 |        41 |      2415 | \n             |     0.058 |     0.016 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |     38505 |      2444 |     40949 | \n             |     0.942 |     0.984 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary2 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |      2613 |        48 |      2661 | \n             |     0.064 |     0.019 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |     38266 |      2437 |     40703 | \n             |     0.936 |     0.981 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary5 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     19176 |       659 |     19835 | \n             |     0.469 |     0.265 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |     21703 |      1826 |     23529 | \n             |     0.531 |     0.735 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary7 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     37356 |      1935 |     39291 | \n             |     0.914 |     0.779 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      3523 |       550 |      4073 | \n             |     0.086 |     0.221 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary8 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     38370 |      2026 |     40396 | \n             |     0.939 |     0.815 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      2509 |       459 |      2968 | \n             |     0.061 |     0.185 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \n fit.binary9 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     38670 |      2067 |     40737 | \n             |     0.946 |     0.832 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      2209 |       418 |      2627 | \n             |     0.054 |     0.168 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary10 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     38762 |      2077 |     40839 | \n             |     0.948 |     0.836 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      2117 |       408 |      2525 | \n             |     0.052 |     0.164 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary15 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     39743 |      2226 |     41969 | \n             |     0.972 |     0.896 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |      1136 |       259 |      1395 | \n             |     0.028 |     0.104 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary20 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     40690 |      2428 |     43118 | \n             |     0.995 |     0.977 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |       189 |        57 |       246 | \n             |     0.005 |     0.023 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|           N / Col Total |\n|-------------------------|\n\n \nTotal Observations in Table:  43364 \n\n \n             | mydata$DRINKING_D \nfit.binary50 |         0 |         1 | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |     40875 |      2481 |     43356 | \n             |     1.000 |     0.998 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |         4 |         4 |         8 | \n             |     0.000 |     0.002 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |     40879 |      2485 |     43364 | \n             |     0.943 |     0.057 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nWe also computed the specificity, sensitivity, and misclassification rates for various probability cut-offs. The results, presented in the cutoff value table, reveal that a cutoff value of 0.02 corresponds to the highest misclassification rate at 0.89. In contrast, cutoff values of 0.2 and 0.5 demonstrate the lowest misclassification rates, both standing at 0.06. This implies that at these cutoff points, only 6% of cases were inaccurately classified, either as false positives or false negatives, indicating a higher accuracy in predictive performance.\n\n\n\n\n\n\n\n  \n    \n      Cut-off Value Table\n    \n    \n    \n      Cut_off_Value\n      Sensitivity\n      Specificity\n      Misclassification_Rate\n    \n  \n  \n    0.02\n0.98\n0.06\n0.89\n    0.03\n0.98\n0.06\n0.88\n    0.05\n0.27\n0.45\n0.52\n    0.07\n0.22\n0.91\n0.13\n    0.08\n0.18\n0.94\n0.1\n    0.09\n0.17\n0.95\n0.1\n    0.10\n0.16\n0.95\n0.1\n    0.15\n0.1\n0.97\n0.1\n    0.2\n0.02\n1\n0.06\n    0.5\n0\n1\n0.06\n  \n  \n  \n\n\n\n\n\n\nROC\nWe also look at the ROC curve to further assess our model. Based on the result, the optimal cut off rate in this scenario, which minimizes both sensitivity and specificity, is 0.06. At this cutoff, the sensitivity is 0.66, meaning the model correctly identifies 66% of alcohol-related crashes. The specificity is 0.55, meaning the model correctly identifies 55% of crashes that are not alcohol related. The result is different from the result shown in the previous section, where the cutoff value 0.2 with the lowest minimum mis-classification rates of 0.06 is considered as the optimal one. The reason is that the former approach focuses more on overall accuracy of the model rather than balancing sensitivity and specificity, while the ROC approach prioritizes a more balanced approach between capturing as many true alcohol-related crashes as possible (sensitivity) and correctly identifying non-alcohol-related crashes (specificity).\nAlso, the AUC (area under the ROC curve), which is usually interpreted as the probability that the model correctly ranks two randomly selected observations where one has 𝑦=1 and the other one has 𝑦=0, provides some insight. To elaborate, the AUC here is 0.64, meaning that there is a 64% chance that the model will be able to distinguish between a crash that was caused by alcohol and one that was not. This AUC is better than random guessing but shows that there is still considerable room for improvement in the model’s ability to discriminate between positive and negative instances.\n\n\n  labels predictions\n1      0  0.06794568\n2      0  0.08305194\n3      0  0.06794568\n4      0  0.03858293\n5      0  0.06794568\n6      1  0.07048039\n\n\n\n\n\n[[1]]\n[1] 0.6398695\n\n\n                  [,1]\nsensitivity 0.66076459\nspecificity 0.54524328\ncutoff      0.06365151\n\n\n\n\nLogistic Regression With Binary Predictors Only\nFor comparison, we also run another logistic regression with the binary predictors only (FATAL_OR_M, OVERTURNED, CELL_PHONE, SPEEDING, AGGRESSIVE, DRIVER1617, DRIVER65PLUS). To elaborate, the OR of FATAL_OR_M is 2.25 (95% CI, 1.9 – 2.64), suggesting that in crashes where there was a fatality or major injury (FATAL_OR_M = 1), the odds of the crash involving drunk driving are 2.25 times higher compared to crashes where there was no fatality or major injury (FATAL_OR_M = 0).In other words, crashes that result in fatalities or major injuries are more than twice as likely to involve drunk driving as those that do not result in such severe outcomes. The OR of OVERTURNED is 2.56(95% CI, 2.06 – 3.16), suggesting that in involved an overturned vehicle, the odds of the crash involving drunk driving are 2.56 times higher compared to crashes where no vehicle was overturned. The OR of CELL_PHONE is 1.03(95% CI, 0.68 – 1.49), suggesting that in crashes where driver was using cellphones, the odds of the crash involving drunk driving are 1.03 times higher compared to crashes where driver wasn’t using cellphones. The OR associated with SPEEDING is 4.67(95% CI, 3.98 – 5.46). This indicates that in crashes involving speeding cars, the likelihood of drunk driving being a factor is 4.67 times greater than in crashes where speeding is not a factor. This substantial increase highlights the strong association between speeding and the likelihood of drunk driving in accidents. Also, the OR for AGGRESSIVE driving is 0.55(95% CI, 0.5 – 0.6). This suggests that in incidents involving aggressive driving, the probability of drunk driving being involved is only 55% of that in crashes where aggressive driving is not present. The OR of DRIVER1617 is 0.28(95% CI, 0.15 – 0.48), suggesting that in crashes involved at least one driver who was 16 or 17 years old, the odd of the crash involving drunk driving is 0.28 times as much as crashes where no driver was 16 or 17 years old. The OR of DRIVER65PLUS is 0.46(95% CI, 0.38 – 0.56), suggesting that in crashes involved at least one driver who was at least 65 years old, the odd of the crash involving drunk driving is 0.46 times as much as crashes where no driver was at least 65 years old.\n\n\n\nCall:\nglm(formula = DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE + \n    SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS, family = \"binomial\", \n    data = mydata)\n\nCoefficients:\n             Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)  -2.65190    0.02753 -96.324 &lt; 0.0000000000000002 ***\nFATAL_OR_M    0.80932    0.08376   9.662 &lt; 0.0000000000000002 ***\nOVERTURNED    0.93978    0.10903   8.619 &lt; 0.0000000000000002 ***\nCELL_PHONE    0.03107    0.19777   0.157                0.875    \nSPEEDING      1.54032    0.08053  19.128 &lt; 0.0000000000000002 ***\nAGGRESSIVE   -0.59365    0.04775 -12.433 &lt; 0.0000000000000002 ***\nDRIVER1617   -1.27158    0.29311  -4.338  0.00001436374143265 ***\nDRIVER65PLUS -0.76646    0.09576  -8.004  0.00000000000000121 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 19036  on 43363  degrees of freedom\nResidual deviance: 18344  on 43356  degrees of freedom\nAIC: 18360\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n (Intercept)   FATAL_OR_M   OVERTURNED   CELL_PHONE     SPEEDING   AGGRESSIVE \n  0.07051713   2.24636998   2.55942903   1.03156149   4.66608472   0.55230941 \n  DRIVER1617 DRIVER65PLUS \n  0.28038936   0.46465631 \n\n\n\n\nWaiting for profiling to be done...\n\n\n                    OR2      2.5 %    97.5 %\n(Intercept)  0.07051713 0.06678642 0.0743978\nFATAL_OR_M   2.24636998 1.90112455 2.6404533\nOVERTURNED   2.55942903 2.05736015 3.1556897\nCELL_PHONE   1.03156149 0.68459779 1.4907150\nSPEEDING     4.66608472 3.97961862 5.4573472\nAGGRESSIVE   0.55230941 0.50268818 0.6061758\nDRIVER1617   0.28038936 0.14904734 0.4751771\nDRIVER65PLUS 0.46465631 0.38318289 0.5579332\n\n\n\n\nWaiting for profiling to be done...\n\n\n                Estimate Std. Error     z value   Pr(&gt;|z|)        OR2\n(Intercept)  -2.65189961 0.02753107 -96.3238683 0.00000000 0.07051713\nFATAL_OR_M    0.80931557 0.08376150   9.6621431 0.00000000 2.24636998\nOVERTURNED    0.93978420 0.10903433   8.6191585 0.00000000 2.55942903\nCELL_PHONE    0.03107367 0.19777088   0.1571195 0.87515064 1.03156149\nSPEEDING      1.54032033 0.08052787  19.1277908 0.00000000 4.66608472\nAGGRESSIVE   -0.59364687 0.04774781 -12.4329656 0.00000000 0.55230941\nDRIVER1617   -1.27157607 0.29310969  -4.3382260 0.00001436 0.28038936\nDRIVER65PLUS -0.76645727 0.09576440  -8.0035718 0.00000000 0.46465631\n                  2.5 %    97.5 %\n(Intercept)  0.06678642 0.0743978\nFATAL_OR_M   1.90112455 2.6404533\nOVERTURNED   2.05736015 3.1556897\nCELL_PHONE   0.68459779 1.4907150\nSPEEDING     3.97961862 5.4573472\nAGGRESSIVE   0.50268818 0.6061758\nDRIVER1617   0.14904734 0.4751771\nDRIVER65PLUS 0.38318289 0.5579332\n\n\nAs is shown in the result table, similar with the original model all the perdictors, except for the CELL_PHONE, are significant. Also, when looking at the AIC, the AIC for the original model is 18359.6, while the AIC for the new model is 18360.5, suggesting that the original model is a better model.\n\n\n         df      AIC\nmylogit  10 18359.63\nmylogit2  8 18360.47"
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#summarizing-findings",
    "href": "MUSA500_Stats/MUSA500_HW3.html#summarizing-findings",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Summarizing findings",
    "text": "Summarizing findings\nIn our study, we analyzed a dataset of 43,364 car crash observations to identify factors associated with crashes involving drunk driving. The dependent variable was ‘DRINKING_D’ (Drinking driver indicator), and various predictors like crash severity, vehicle overturning, cell phone usage, speeding, aggressive driving, age of drivers, and socioeconomic factors including median house income and percentage of individuals getting bachelor’s degree or higher were examined.\nBased on the logistic regression result, factors such as crash severity (‘FATAL_OR_M’), vehicle overturning (‘OVERTURNED’), speeding (‘SPEEDING’), age groups of drivers (‘DRIVER1617’ and ‘DRIVER65PLUS’), and median house income (‘MEDHHINC’) significantly predicted drunk driving incidents, although the ‘MEDHHINC’ is not that significant which has a p-value of 0.036 and beta coefficient of 0.0000028. On the contrary, cell phone usage (‘CELL_PHONE’) and education level (‘PCTBACHMOR’) were not significantly associated with the drunk driver crashes. Overall, the findings align closely with our initial expectations. Most variables exhibited behavior that was anticipated, particularly the strong associations observed between speeding and vehicle overturning with instances of drunk driving. It is also reasonable that the cell phone usage, socioeconomic factor median house income and education level do not necessarily have a significant impact on the likelihood of drunk driving incidents. However, it is a bit surprising that aggressive driving, are associated with ORs less than 1, which is suggesting a lower likelihood of drunk driving but is not following our initial expectation.\n\n\nCounts:\n\n\n\n    0     1 \n40879  2485 \n\n\n\nPercentages:\n\n\n\n       0        1 \n94.26944  5.73056 \n\n\nOur dependent variable DRINKING_D has the appropriate count of each category, having 43364 sample size with 2485 events where DRINKING_D equals 1 occurring, which account for 5.73% of the total. From what Paul Allison proposed, it could be problematic when the sample size is small and there is rarity of the events, which does not really apply in our case. Consequently, the application of logistic regression in our study is appropriate, given the substantial sample size and the proportion of events within it."
  },
  {
    "objectID": "MUSA500_Stats/MUSA500_HW3.html#limitations",
    "href": "MUSA500_Stats/MUSA500_HW3.html#limitations",
    "title": "MUSA500 Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "Limitations",
    "text": "Limitations\nOne limitation of the model may be failing to include all possible confounding variables that could influence the likelihood of drunk driving accidents. For instance, factors like road conditions, weather, or specific traffic patterns, which might have a significant impact, are not considered. In addition, in reality the traffic crash data will have some extent of spatial autocorrelation as certain areas might have higher accident rates due to specific local conditions. However, in our case we did not take those into considerations.\nThe primary limitation of our analysis is reflected in the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) curve, a crucial metric for assessing the model’s predictive accuracy. In our model, the AUC is approximately 0.64, which, according to standard guidelines, is categorized as poor (grade D). This rating indicates a relatively low effectiveness of the model in distinguishing between the binary outcomes, specifically in correctly predicting ‘1’ responses as ‘1s’ and ‘0’ responses as ‘0s’."
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html",
    "href": "MUSA550_Python/assignment-6.html",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Due date: Wednesday, 12/6 by the end of the day\nLectures 12B and 13A will cover predictive modeling of housing prices in Philadelphia. We’ll extend that analysis in this section by:"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#musa550-geospatial-data-science-in-python",
    "href": "MUSA550_Python/assignment-6.html#musa550-geospatial-data-science-in-python",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "MUSA550 Geospatial Data Science in Python",
    "text": "MUSA550 Geospatial Data Science in Python\n\nHang Zhao"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "MUSA550_Python/assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\nimport geopandas as gpd\nimport requests\n\n\n# the CARTO API url\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\n\n# Only pull 2022 sales for single family residential properties\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\"}\nresponse = requests.get(carto_url, params=params)\n\n\nresponse\n\n&lt;Response [200]&gt;\n\n\n\n# Make the GeoDataFrame\nsalesRaw = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\n# Optional: put it a reproducible order for test/training splits later\nsalesRaw = salesRaw.sort_values(\"parcel_number\")\n\n\nsalesRaw.head(n=5)\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\n...\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\n\n\n\n\n919\nPOINT (-75.14860 39.93145)\n18102\n2022-05-24T00:00:00Z\n0\n36'6\" E OF AMERICAN\n54131081\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1960\nY\n19147\nRSA5\n1001563093\n26\nROW RIVER ROW\n401690820\n\n\n12802\nPOINT (-75.14817 39.93101)\n32924\n2022-05-24T00:00:00Z\nA\n50' W SIDE OF 2ND ST\n54063610\nO50\nROW 3 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n2009\nNone\n19147\nRSA5\n1001190209\n22\nROW TYPICAL\n401704018\n\n\n9020\nPOINT (-75.14781 39.93010)\n28224\n2022-05-24T00:00:00Z\nA\n33.333 S OF REED\n54085418\nP51\nROW W/GAR 3 STY MAS+OTHER\n1\nSINGLE FAMILY\n...\nNone\nI\n2014\nNone\n19147\nICMX\n1001442221\n25\nROW MODERN\n401699421\n\n\n1344\nPOINT (-75.14887 39.93026)\n18517\n2022-05-24T00:00:00Z\nD\n68 FT W PHILIP ST\n54127951\nO50\nROW 3 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1920\nY\n19147\nRSA5\n1001442236\n22\nROW TYPICAL\n401691319\n\n\n12767\nPOINT (-75.14881 39.93012)\n32882\n2022-05-24T00:00:00Z\nD\n42 FT W PHILIP\n54063384\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n...\nNone\nI\n1920\nY\n19147\nRSA5\n1001238775\n22\nROW TYPICAL\n401704228\n\n\n\n\n5 rows × 80 columns\n\n\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\n# Get the data\nurl = \"https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\"\nCensus_tract = gpd.read_file(url)\nurl2 = \"https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\"\nNeighbor = gpd.read_file(url2)\n\n\nCensus_tract.head(n=5)\n\n\n\n\n\n\n\n\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\ngeometry\n\n\n\n\n0\n1\n42\n101\n009400\n42101009400\n94\nCensus Tract 94\nG5020\nS\n366717\n0\n+39.9632709\n-075.2322437\n10429\nPOLYGON ((-75.22927 39.96054, -75.22865 39.960...\n\n\n1\n2\n42\n101\n009500\n42101009500\n95\nCensus Tract 95\nG5020\nS\n319070\n0\n+39.9658709\n-075.2379140\n10430\nPOLYGON ((-75.23536 39.96852, -75.23545 39.969...\n\n\n2\n3\n42\n101\n009600\n42101009600\n96\nCensus Tract 96\nG5020\nS\n405273\n0\n+39.9655396\n-075.2435075\n10431\nPOLYGON ((-75.24343 39.96230, -75.24339 39.962...\n\n\n3\n4\n42\n101\n013800\n42101013800\n138\nCensus Tract 138\nG5020\nS\n341256\n0\n+39.9764504\n-075.1771771\n10468\nPOLYGON ((-75.17341 39.97779, -75.17386 39.977...\n\n\n4\n5\n42\n101\n013900\n42101013900\n139\nCensus Tract 139\nG5020\nS\n562934\n0\n+39.9750563\n-075.1711846\n10469\nPOLYGON ((-75.17313 39.97776, -75.17321 39.977...\n\n\n\n\n\n\n\n\nNeighbor.head(n=5)\n\n\n\n\n\n\n\n\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id\ncreated_at\nupdated_at\ngeometry\n\n\n\n\n0\nPENNYPACK_PARK\nPennypack Park\nPennypack Park\n87084.285589\n6.014076e+07\n9\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\nMULTIPOLYGON (((-75.05645 40.08743, -75.05667 ...\n\n\n1\nOVERBROOK\nOverbrook\nOverbrook\n57004.924607\n7.692499e+07\n138\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\nMULTIPOLYGON (((-75.22719 39.97740, -75.22984 ...\n\n\n2\nGERMANTOWN_SOUTHWEST\nGermantown, Southwest\nSouthwest Germantown\n14880.743608\n1.441867e+07\n59\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\nMULTIPOLYGON (((-75.16208 40.02829, -75.16145 ...\n\n\n3\nEAST_PARKSIDE\nEast Parkside\nEast Parkside\n10885.781535\n4.231000e+06\n129\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\nMULTIPOLYGON (((-75.19931 39.97462, -75.19869 ...\n\n\n4\nGERMANY_HILL\nGermany Hill\nGermany Hill\n13041.939087\n6.949968e+06\n49\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\nMULTIPOLYGON (((-75.22722 40.03523, -75.22865 ...\n\n\n\n\n\n\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\nSjoin1 = gpd.sjoin(\n    salesRaw,  \n    Neighbor.to_crs(salesRaw.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\nSjoin1=Sjoin1.drop(columns='index_right')\n\n\nSjoin = gpd.sjoin(\n    Sjoin1,  # The point data for 311 tickets\n    Census_tract.to_crs(Sjoin1.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\nSjoin.head()\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id_left\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\n...\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\n\n\n\n\n919\nPOINT (-75.14860 39.93145)\n18102\n2022-05-24T00:00:00Z\n0\n36'6\" E OF AMERICAN\n54131081\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n...\n42101002702\n27.02\nCensus Tract 27.02\nG5020\nS\n367673.0\n0.0\n+39.9280114\n-075.1495606\n10368\n\n\n12802\nPOINT (-75.14817 39.93101)\n32924\n2022-05-24T00:00:00Z\nA\n50' W SIDE OF 2ND ST\n54063610\nO50\nROW 3 STY MASONRY\n1\nSINGLE FAMILY\n...\n42101002702\n27.02\nCensus Tract 27.02\nG5020\nS\n367673.0\n0.0\n+39.9280114\n-075.1495606\n10368\n\n\n9020\nPOINT (-75.14781 39.93010)\n28224\n2022-05-24T00:00:00Z\nA\n33.333 S OF REED\n54085418\nP51\nROW W/GAR 3 STY MAS+OTHER\n1\nSINGLE FAMILY\n...\n42101002702\n27.02\nCensus Tract 27.02\nG5020\nS\n367673.0\n0.0\n+39.9280114\n-075.1495606\n10368\n\n\n1344\nPOINT (-75.14887 39.93026)\n18517\n2022-05-24T00:00:00Z\nD\n68 FT W PHILIP ST\n54127951\nO50\nROW 3 STY MASONRY\n1\nSINGLE FAMILY\n...\n42101002702\n27.02\nCensus Tract 27.02\nG5020\nS\n367673.0\n0.0\n+39.9280114\n-075.1495606\n10368\n\n\n12767\nPOINT (-75.14881 39.93012)\n32882\n2022-05-24T00:00:00Z\nD\n42 FT W PHILIP\n54063384\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n...\n42101002702\n27.02\nCensus Tract 27.02\nG5020\nS\n367673.0\n0.0\n+39.9280114\n-075.1495606\n10368\n\n\n\n\n5 rows × 103 columns\n\n\n\n\n#Sjoin.columns.values\n\narray(['geometry', 'cartodb_id_left', 'assessment_date', 'basements',\n       'beginning_point', 'book_and_page', 'building_code',\n       'building_code_description', 'category_code',\n       'category_code_description', 'census_tract', 'central_air',\n       'cross_reference', 'date_exterior_condition', 'depth',\n       'exempt_building', 'exempt_land', 'exterior_condition',\n       'fireplaces', 'frontage', 'fuel', 'garage_spaces', 'garage_type',\n       'general_construction', 'geographic_ward', 'homestead_exemption',\n       'house_extension', 'house_number', 'interior_condition',\n       'location', 'mailing_address_1', 'mailing_address_2',\n       'mailing_care_of', 'mailing_city_state', 'mailing_street',\n       'mailing_zip', 'market_value', 'market_value_date',\n       'number_of_bathrooms', 'number_of_bedrooms', 'number_of_rooms',\n       'number_stories', 'off_street_open', 'other_building', 'owner_1',\n       'owner_2', 'parcel_number', 'parcel_shape', 'quality_grade',\n       'recording_date', 'registry_number', 'sale_date', 'sale_price',\n       'separate_utilities', 'sewer', 'site_type', 'state_code',\n       'street_code', 'street_designation', 'street_direction',\n       'street_name', 'suffix', 'taxable_building', 'taxable_land',\n       'topography', 'total_area', 'total_livable_area', 'type_heater',\n       'unfinished', 'unit', 'utility', 'view_type', 'year_built',\n       'year_built_estimate', 'zip_code', 'zoning', 'pin',\n       'building_code_new', 'building_code_description_new', 'objectid',\n       'name', 'listname', 'mapname', 'shape_leng', 'shape_area',\n       'cartodb_id_right', 'created_at', 'updated_at', 'index_right',\n       'OBJECTID', 'STATEFP10', 'COUNTYFP10', 'TRACTCE10', 'GEOID10',\n       'NAME10', 'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10',\n       'AWATER10', 'INTPTLAT10', 'INTPTLON10', 'LOGRECNO'], dtype=object)\n\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements - Trim the sales data to those sales with prices between $3,000 and $1 million - Set up a pipeline that includes both numerical columns and categorical columns - Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n\nnp.random.seed(42)\n\n\n# Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\n\n# The feature columns we want to use\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"zip_code\",\n    \"name\",\n]\n\n# Trim to these columns and remove NaNs\nsales = Sjoin[cols + [\"geometry\"]].dropna()\n\n# Trim zip code to only the first five digits\nsales[\"zip_code\"] = sales[\"zip_code\"].astype(str).str.slice(0, 5)\n\n\n# Trim very low and very high sales\ntrim = (sales['sale_price'] &gt; 3000) & (sales['sale_price'] &lt; 1e6)\nsales = sales.loc[trim]\n\n\nlen(sales)\n\n17670"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#numerical-data",
    "href": "MUSA550_Python/assignment-6.html#numerical-data",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Numerical Data",
    "text": "Numerical Data\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(sales, test_size=0.3, random_state=42)\n\n\n# the target labels: log of sale price\ny_train = np.log(train_set[\"sale_price\"])\ny_test = np.log(test_set[\"sale_price\"])\n\n\n# The features\nfeature_cols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n]\nX_train = train_set[feature_cols].values\nX_test = test_set[feature_cols].values\n\n\n# Make a linear model pipeline\nlinear_pipeline = make_pipeline(StandardScaler(), LinearRegression())\n\n# Fit on the training data\nlinear_pipeline.fit(X_train, y_train)\n\n# What's the test score?\nlinear_pipeline.score(X_test, y_test)\n\n0.18386976167249158\n\n\n\nForest Pipeline\n\n# Make a random forest pipeline\nforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nscores = cross_val_score(\n    forest_pipeline,\n    X_train,\n    y_train,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\nR^2 scores =  [0.3171259  0.30563743 0.3867302  0.37449741 0.32894432 0.30532346\n 0.30151491 0.29504504 0.29748476 0.23596602]\nScores mean =  0.3148269433933536\nScore std dev =  0.040247139415635486\n\n\n\n# Fit on the training data\nforest_pipeline.fit(X_train, y_train)\n\n# What's the test score?\nforest_pipeline.score(X_test, y_test)\n\n0.33764330237438755"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#include-the-categorical-data",
    "href": "MUSA550_Python/assignment-6.html#include-the-categorical-data",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Include the Categorical Data",
    "text": "Include the Categorical Data\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Numerical columns\nnum_cols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n]\n\n# Categorical columns\ncat_cols = [\"exterior_condition\", \"name\"]\n\n\n# Set up the column transformer with two transformers\n# ----&gt; Scale the numerical columns\n# ----&gt; One-hot encode the categorical columns\n\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n\n# Initialize the pipeline\n# NOTE: only use 10 estimators here so it will run in a reasonable time\npipe = make_pipeline(\n    transformer, RandomForestRegressor(n_estimators=10, \n                                       random_state=42)\n)\n\n\n# Fit the training set\npipe.fit(train_set, y_train);\n\n\n# What's the test score?\npipe.score(test_set, y_test)\n\n0.5313982409697242"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#graphing-the-importance-of-neighbor-info",
    "href": "MUSA550_Python/assignment-6.html#graphing-the-importance-of-neighbor-info",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Graphing the Importance of “Neighbor” Info",
    "text": "Graphing the Importance of “Neighbor” Info\n\ntransformer\n\n\n# The steps in the column transformer\ntransformer.named_transformers_\n\n\n# The one-hot step\nohe = transformer.named_transformers_['cat']\n\nohe\n\n\n# One column for each category type!\nohe_cols = ohe.get_feature_names_out()\n\nohe_cols\n\n\n# Full list of columns is numerical + one-hot \nfeatures = num_cols + list(ohe_cols)\n\nfeatures\n\n\nrandom_forest = pipe[\"randomforestregressor\"]\n\n# Create the dataframe with importances\nimportance = pd.DataFrame(\n    {\"Feature\": features, \"Importance\": random_forest.feature_importances_}\n)\n\n\nimportance.head(n=20)\n\n\nimport hvplot.pandas\n# Sort by importance and get the top 30\n# SORT IN DESCENDING ORDER\nimportance = importance.sort_values(\"Importance\", ascending=False).iloc[:30]\n\n# Plot\nimportance.hvplot.barh(x=\"Feature\", y=\"Importance\", height=700, flip_yaxis=True)"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#k-fold-cross-validation",
    "href": "MUSA550_Python/assignment-6.html#k-fold-cross-validation",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "K-fold Cross Validation",
    "text": "K-fold Cross Validation\n\npipe.named_steps\n\n{'columntransformer': ColumnTransformer(transformers=[('num', StandardScaler(),\n                                  ['total_livable_area', 'total_area',\n                                   'garage_spaces', 'fireplaces',\n                                   'number_of_bathrooms', 'number_of_bedrooms',\n                                   'number_stories']),\n                                 ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                  ['exterior_condition', 'name'])]),\n 'randomforestregressor': RandomForestRegressor(n_estimators=10, random_state=42)}\n\n\n\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\nparam_grid\n\n{'randomforestregressor__n_estimators': [5, 10, 15, 20, 30, 50, 100, 200],\n 'randomforestregressor__max_depth': [2, 5, 7, 9, 13, 21, 33, 51]}\n\n\n\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    forest_pipeline,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\nR^2 scores =  [0.31247274 0.30460143 0.29103723]\nScores mean =  0.30270379808763737\nScore std dev =  0.008853286714194464\n\n\n\n# Create the grid and use 3-fold CV\ngrid_pipe = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid_pipe.fit(train_set, y_train)\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'name'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(n_estimators=10,\n                                                              random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         StandardScaler(),\n                                                                         ['total_livable_area',\n                                                                          'total_area',\n                                                                          'garage_spaces',\n                                                                          'fireplaces',\n                                                                          'number_of_bathrooms',\n                                                                          'number_of_bedrooms',\n                                                                          'number_stories']),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['exterior_condition',\n                                                                          'name'])])),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(n_estimators=10,\n                                                              random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['total_livable_area',\n                                                   'total_area',\n                                                   'garage_spaces',\n                                                   'fireplaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'name'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(n_estimators=10, random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'name'])])num['total_livable_area', 'total_area', 'garage_spaces', 'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories']StandardScalerStandardScaler()cat['exterior_condition', 'name']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(n_estimators=10, random_state=42)\n\n\n\ngrid_pipe.best_estimator_\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['total_livable_area',\n                                                   'total_area',\n                                                   'garage_spaces',\n                                                   'fireplaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'name'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=33, n_estimators=200,\n                                       random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['total_livable_area',\n                                                   'total_area',\n                                                   'garage_spaces',\n                                                   'fireplaces',\n                                                   'number_of_bathrooms',\n                                                   'number_of_bedrooms',\n                                                   'number_stories']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['exterior_condition',\n                                                   'name'])])),\n                ('randomforestregressor',\n                 RandomForestRegressor(max_depth=33, n_estimators=200,\n                                       random_state=42))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num', StandardScaler(),\n                                 ['total_livable_area', 'total_area',\n                                  'garage_spaces', 'fireplaces',\n                                  'number_of_bathrooms', 'number_of_bedrooms',\n                                  'number_stories']),\n                                ('cat', OneHotEncoder(handle_unknown='ignore'),\n                                 ['exterior_condition', 'name'])])num['total_livable_area', 'total_area', 'garage_spaces', 'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms', 'number_stories']StandardScalerStandardScaler()cat['exterior_condition', 'name']OneHotEncoderOneHotEncoder(handle_unknown='ignore')RandomForestRegressorRandomForestRegressor(max_depth=33, n_estimators=200, random_state=42)\n\n\n\ngrid_pipe.best_params_\n\n{'randomforestregressor__max_depth': 33,\n 'randomforestregressor__n_estimators': 200}\n\n\n\ngrid_pipe.score(test_set, y_test)\n\n0.5527144321712075\n\n\n\n# Create the grid and use 3-fold CV for the random forest - no categorical data\ngrid = GridSearchCV(forest_pipeline, param_grid, cv=3, verbose=1)\n\n# Run the search\ngrid.fit(X_train, y_train)\n\n\n# The best estimator\ngrid.best_estimator_\n\n\n# The best hyper parameters\ngrid.best_params_\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\ndef evaluate_mape(model, X_test, y_test):\n    \"\"\"\n    Given a model and test features/targets, print out the \n    mean absolute error and accuracy\n    \"\"\"\n    # Make the predictions\n    predictions = model.predict(X_test)\n\n    # Absolute error\n    #errors = abs(predictions - (y_test))\n    errors = abs(np.exp(predictions) - (np.exp(y_test)))\n    avg_error = np.mean(errors)\n\n    # Mean absolute percentage error\n    mape = 100 * np.mean(errors / np.exp(y_test))\n\n    # Accuracy\n    accuracy = 100 - mape\n\n    print(\"Model Performance\")\n    print(f\"Average Absolute Error: {avg_error:0.4f}\")\n    print(f\"Accuracy = {accuracy:0.2f}%.\")\n\n    return accuracy"
  },
  {
    "objectID": "MUSA550_Python/assignment-6.html#random-forest",
    "href": "MUSA550_Python/assignment-6.html#random-forest",
    "title": "MUSA550 - Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Random Forest",
    "text": "Random Forest\n\n# Evaluate on the test set\nbase_accuracy = evaluate_mape(forest_pipeline, X_test, y_test)\n\nModel Performance\nAverage Absolute Error: 88735.0743\nAccuracy = 35.91%.\n\n\n\n# Evaluate the best random forest model\nbest_random = grid_pipe.best_estimator_\nrandom_accuracy = evaluate_mape(best_random, test_set, y_test)\n\n# What's the improvement?\nimprovement = 100 * (random_accuracy - base_accuracy) / base_accuracy\nprint(f'Improvement of {improvement:0.4f}%.')\n\nModel Performance\nAverage Absolute Error: 69164.6411\nAccuracy = 50.68%.\nImprovement of 41.1356%.\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\nsliced_data = Sjoin.loc[test_set.index]\n\n\npd.options.display.max_columns = 999\n\n\nsliced_data.head(n=5)\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id_left\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id_right\ncreated_at\nupdated_at\nindex_right\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\n\n\n\n\n5300\nPOINT (-75.25152 40.04542)\n23579\n2022-05-24T00:00:00Z\nA\n429'5 1/2\"NW SHAWMONT\n54098598\nA21\nDET 1.5 STY MASONRY+OTHER\n1\nSINGLE FAMILY\n219\nY\nNone\nNone\n98.0\n80000.0\n0.0\n3\n1.0\n78.0\nNone\n0.0\nNone\nE\n21\n0\nNone\n7848\n3\n7848 NIXON ST\nMCDAY JOHN B\nNone\nNone\nVILLANOVA PA\n2058 MATSONS CIR\n19085\n294100\nNone\n2.0\n3.0\nNaN\n2.0\n6204.0\nNone\nMCDAY JOHN B\nMCDAY JANE A\n212412910\nE\nC+\n2022-09-16T00:00:00Z\n150N210094\n2022-09-07T00:00:00Z\n450000\nNone\nNone\nNone\nPA\n59920\nST\nNone\nNIXON\nNone\n155300.0\n58800.0\nF\n7594.0\n1595.0\nB\nNone\nNone\nNone\nB\n1975\nY\n19128\nRSA3\n1001390664\n05\nNone\n401696756\nUPPER_ROXBOROUGH\nUpper Roxborough\nUpper Roxborough\n50882.081408\n6.869143e+07\n46.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n250.0\n251.0\n42\n101\n021900\n42101021900\n219\nCensus Tract 219\nG5020\nS\n1410069.0\n51091.0\n+40.0505829\n-075.2470033\n10545\n\n\n12908\nPOINT (-75.12923 40.03141)\n33061\n2022-05-24T00:00:00Z\nF\n122'9\" W OF 3RD ST\n54061847\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n286\nN\nNone\nNone\n75.0\n0.0\n0.0\n4\n0.0\n15.0\nNone\n1.0\nNone\nA\n42\n0\nNone\n316\n4\n316 W FISHER AVE\nELIAS MUNEER\nNone\nNone\nPHILADELPHIA PA\n454 DELMAR\n19128\n107800\nNone\n1.0\n3.0\nNaN\n2.0\n1942.0\nNone\nELIAS MUNEER\nNone\n422248000\nE\nC\n2022-06-27T00:00:00Z\n123N12 229\n2022-06-24T00:00:00Z\n83139\nNone\nNone\nNone\nPA\n34140\nAVE\nW\nFISHER\nNone\n86240.0\n21560.0\nF\n1144.0\n1290.0\nH\nNone\nNone\nNone\nI\n1930\nY\n19120\nRSA5\n1001214868\n24\nROW PORCH FRONT\n401704506\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n108.0\n109.0\n42\n101\n028600\n42101028600\n286\nCensus Tract 286\nG5020\nS\n717845.0\n0.0\n+40.0285757\n-075.1276448\n10600\n\n\n20713\nPOINT (-75.20568 40.02309)\n43246\n2022-05-24T00:00:00Z\nE\n126'N OF SALAIGNAC ST\n54006879\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n211\nY\nNone\nNone\n100.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n1.0\nNone\nE\n21\n0\nNone\n5455\n4\n5455 VICARIS ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n5455 VICARIS ST\n19128-2826\n245400\nNone\n1.0\n3.0\nNaN\n2.0\n2941.0\nNone\nHOOKS SHEENA\nNone\n213221174\nE\nC\n2022-03-24T00:00:00Z\n96N12 85\n2022-03-04T00:00:00Z\n250000\nNone\nNone\nNone\nPA\n80000\nST\nNone\nVICARIS\nNone\n196320.0\n49080.0\nF\n1600.0\n1110.0\nA\nNone\nNone\nNone\nI\n1965\nY\n19128\nRSA5\n1001540422\n23\nROW POST WAR\n401715911\nWISSAHICKON\nWissahickon\nWissahickon\n17714.185972\n9.064239e+06\n53.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n85.0\n86.0\n42\n101\n021100\n42101021100\n211\nCensus Tract 211\nG5020\nS\n540313.0\n0.0\n+40.0264564\n-075.2065293\n10537\n\n\n2310\nPOINT (-75.17142 39.93675)\n19863\n2022-05-24T00:00:00Z\nNone\n261'6\" W 16TH ST\n54118070\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n22\nNone\nNone\nNone\n61.0\n0.0\n0.0\n3\n0.0\n15.0\nNone\n0.0\nNone\nA\n36\n0\nNone\n1634\n4\n1634 ANNIN ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n1634 ANNIN ST\n19146-3006\n215000\nNone\n1.0\n3.0\nNaN\n2.0\n634.0\nNone\nEXPERIENCE PROPERTY LLC\nNone\n365284200\nE\nC\n2022-11-07T00:00:00Z\n008S030119\n2022-10-19T00:00:00Z\n280000\nNone\nNone\nNone\nPA\n12740\nST\nNone\nANNIN\nNone\n172000.0\n43000.0\nF\n915.0\n1322.0\nNone\nNone\nNone\nNone\nI\n1925\nNone\n19146\nRSA5\n1001067242\n22\nROW TYPICAL\n401693197\nPOINT_BREEZE\nPoint Breeze\nPoint Breeze\n17529.283714\n1.693772e+07\n103.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n39.0\n40.0\n42\n101\n002200\n42101002200\n22\nCensus Tract 22\nG5020\nS\n228696.0\n0.0\n+39.9364111\n-075.1703923\n10363\n\n\n16088\nPOINT (-75.17645 39.92744)\n37061\n2022-05-24T00:00:00Z\nD\n198' W OF 18TH ST\n54044658\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n37\nN\nNone\nNone\n47.0\n0.0\n0.0\n4\n0.0\n14.0\nNone\n0.0\nNone\nA\n48\n0\nNone\n1829\n4\n1829 HOFFMAN ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n1829 HOFFMAN ST\n19145-2925\n159400\nNone\n1.0\n3.0\nNaN\n2.0\n1977.0\nNone\nWAXMAN ERIN\nNone\n481073400\nE\nC\n2022-05-20T00:00:00Z\n015S130058\n2022-05-20T00:00:00Z\n200000\nNone\nNone\nNone\nPA\n42960\nST\nNone\nHOFFMAN\nNone\n127600.0\n31800.0\nF\n658.0\n896.0\nH\nNone\nNone\nNone\nI\n1920\nY\n19145\nRM1\n1001277147\n22\nROW TYPICAL\n401708174\nWEST_PASSYUNK\nWest Passyunk\nWest Passyunk\n10499.291848\n6.494799e+06\n157.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n198.0\n199.0\n42\n101\n003701\n42101003701\n37.01\nCensus Tract 37.01\nG5020\nS\n355619.0\n0.0\n+39.9276728\n-075.1806654\n10378\n\n\n\n\n\n\n\n\n# Make the predictions\npredictions = pipe.predict(test_set)\n\n# Absolute error\n#errors = abs(predictions - (y_test))\nerrors = (np.exp(predictions)) - (np.exp(y_test))\npercent_error = 100 * (errors / (np.exp(y_test)))\npercent_error = percent_error.rename(\"percent error\")\n\n\nsliced_data[\"percent error\"] = percent_error\n\n\nsliced_data.max()\n\n/var/folders/8y/y89rxkzj5bv2c24kw9zjzy9c0000gn/T/ipykernel_2403/1633908358.py:1: FutureWarning: The default value of numeric_only in GeoDataFrame.max is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  sliced_data.max()\n\n\ncartodb_id_left                             55958\nassessment_date              2023-12-08T00:00:00Z\nbook_and_page                            54230133\ncategory_code                                  1 \ncategory_code_description           SINGLE FAMILY\n                                     ...         \nAWATER10                                1936838.0\nINTPTLAT10                            +40.1290877\nINTPTLON10                           -075.2679037\nLOGRECNO                                    10710\npercent error                           6274.1765\nLength: 71, dtype: object\n\n\n\nmerged = pd.concat([sliced_data, percent_error], axis=1)\nmerged.head(n=5)\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id_left\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id_right\ncreated_at\nupdated_at\nindex_right\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\npercent error\n\n\n\n\n5300\nPOINT (-75.25152 40.04542)\n23579\n2022-05-24T00:00:00Z\nA\n429'5 1/2\"NW SHAWMONT\n54098598\nA21\nDET 1.5 STY MASONRY+OTHER\n1\nSINGLE FAMILY\n219\nY\nNone\nNone\n98.0\n80000.0\n0.0\n3\n1.0\n78.0\nNone\n0.0\nNone\nE\n21\n0\nNone\n7848\n3\n7848 NIXON ST\nMCDAY JOHN B\nNone\nNone\nVILLANOVA PA\n2058 MATSONS CIR\n19085\n294100\nNone\n2.0\n3.0\nNaN\n2.0\n6204.0\nNone\nMCDAY JOHN B\nMCDAY JANE A\n212412910\nE\nC+\n2022-09-16T00:00:00Z\n150N210094\n2022-09-07T00:00:00Z\n450000\nNone\nNone\nNone\nPA\n59920\nST\nNone\nNIXON\nNone\n155300.0\n58800.0\nF\n7594.0\n1595.0\nB\nNone\nNone\nNone\nB\n1975\nY\n19128\nRSA3\n1001390664\n05\nNone\n401696756\nUPPER_ROXBOROUGH\nUpper Roxborough\nUpper Roxborough\n50882.081408\n6.869143e+07\n46.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n250.0\n251.0\n42\n101\n021900\n42101021900\n219\nCensus Tract 219\nG5020\nS\n1410069.0\n51091.0\n+40.0505829\n-075.2470033\n10545\n-13.025614\n\n\n12908\nPOINT (-75.12923 40.03141)\n33061\n2022-05-24T00:00:00Z\nF\n122'9\" W OF 3RD ST\n54061847\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n286\nN\nNone\nNone\n75.0\n0.0\n0.0\n4\n0.0\n15.0\nNone\n1.0\nNone\nA\n42\n0\nNone\n316\n4\n316 W FISHER AVE\nELIAS MUNEER\nNone\nNone\nPHILADELPHIA PA\n454 DELMAR\n19128\n107800\nNone\n1.0\n3.0\nNaN\n2.0\n1942.0\nNone\nELIAS MUNEER\nNone\n422248000\nE\nC\n2022-06-27T00:00:00Z\n123N12 229\n2022-06-24T00:00:00Z\n83139\nNone\nNone\nNone\nPA\n34140\nAVE\nW\nFISHER\nNone\n86240.0\n21560.0\nF\n1144.0\n1290.0\nH\nNone\nNone\nNone\nI\n1930\nY\n19120\nRSA5\n1001214868\n24\nROW PORCH FRONT\n401704506\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n108.0\n109.0\n42\n101\n028600\n42101028600\n286\nCensus Tract 286\nG5020\nS\n717845.0\n0.0\n+40.0285757\n-075.1276448\n10600\n27.456476\n\n\n20713\nPOINT (-75.20568 40.02309)\n43246\n2022-05-24T00:00:00Z\nE\n126'N OF SALAIGNAC ST\n54006879\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n211\nY\nNone\nNone\n100.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n1.0\nNone\nE\n21\n0\nNone\n5455\n4\n5455 VICARIS ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n5455 VICARIS ST\n19128-2826\n245400\nNone\n1.0\n3.0\nNaN\n2.0\n2941.0\nNone\nHOOKS SHEENA\nNone\n213221174\nE\nC\n2022-03-24T00:00:00Z\n96N12 85\n2022-03-04T00:00:00Z\n250000\nNone\nNone\nNone\nPA\n80000\nST\nNone\nVICARIS\nNone\n196320.0\n49080.0\nF\n1600.0\n1110.0\nA\nNone\nNone\nNone\nI\n1965\nY\n19128\nRSA5\n1001540422\n23\nROW POST WAR\n401715911\nWISSAHICKON\nWissahickon\nWissahickon\n17714.185972\n9.064239e+06\n53.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n85.0\n86.0\n42\n101\n021100\n42101021100\n211\nCensus Tract 211\nG5020\nS\n540313.0\n0.0\n+40.0264564\n-075.2065293\n10537\n-7.974018\n\n\n2310\nPOINT (-75.17142 39.93675)\n19863\n2022-05-24T00:00:00Z\nNone\n261'6\" W 16TH ST\n54118070\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n22\nNone\nNone\nNone\n61.0\n0.0\n0.0\n3\n0.0\n15.0\nNone\n0.0\nNone\nA\n36\n0\nNone\n1634\n4\n1634 ANNIN ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n1634 ANNIN ST\n19146-3006\n215000\nNone\n1.0\n3.0\nNaN\n2.0\n634.0\nNone\nEXPERIENCE PROPERTY LLC\nNone\n365284200\nE\nC\n2022-11-07T00:00:00Z\n008S030119\n2022-10-19T00:00:00Z\n280000\nNone\nNone\nNone\nPA\n12740\nST\nNone\nANNIN\nNone\n172000.0\n43000.0\nF\n915.0\n1322.0\nNone\nNone\nNone\nNone\nI\n1925\nNone\n19146\nRSA5\n1001067242\n22\nROW TYPICAL\n401693197\nPOINT_BREEZE\nPoint Breeze\nPoint Breeze\n17529.283714\n1.693772e+07\n103.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n39.0\n40.0\n42\n101\n002200\n42101002200\n22\nCensus Tract 22\nG5020\nS\n228696.0\n0.0\n+39.9364111\n-075.1703923\n10363\n28.642106\n\n\n16088\nPOINT (-75.17645 39.92744)\n37061\n2022-05-24T00:00:00Z\nD\n198' W OF 18TH ST\n54044658\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n37\nN\nNone\nNone\n47.0\n0.0\n0.0\n4\n0.0\n14.0\nNone\n0.0\nNone\nA\n48\n0\nNone\n1829\n4\n1829 HOFFMAN ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n1829 HOFFMAN ST\n19145-2925\n159400\nNone\n1.0\n3.0\nNaN\n2.0\n1977.0\nNone\nWAXMAN ERIN\nNone\n481073400\nE\nC\n2022-05-20T00:00:00Z\n015S130058\n2022-05-20T00:00:00Z\n200000\nNone\nNone\nNone\nPA\n42960\nST\nNone\nHOFFMAN\nNone\n127600.0\n31800.0\nF\n658.0\n896.0\nH\nNone\nNone\nNone\nI\n1920\nY\n19145\nRM1\n1001277147\n22\nROW TYPICAL\n401708174\nWEST_PASSYUNK\nWest Passyunk\nWest Passyunk\n10499.291848\n6.494799e+06\n157.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n198.0\n199.0\n42\n101\n003701\n42101003701\n37.01\nCensus Tract 37.01\nG5020\nS\n355619.0\n0.0\n+39.9276728\n-075.1806654\n10378\n-32.858445\n\n\n\n\n\n\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\ngrouped = merged.groupby(\"GEOID10\", as_index=False)[\"percent error\"].median()\n\n\ngrouped.median()\n\nGEOID10          4.210102e+10\npercent error   -2.154517e+00\ndtype: float64\n\n\n\ntype(grouped)\n\npandas.core.frame.DataFrame\n\n\n\nmap_data = pd.merge(Census_tract[[\"GEOID10\", \"geometry\"]], grouped,  on=\"GEOID10\")\nmap_data.head(n=5)\n\n\n\n\n\n\n\n\nGEOID10\ngeometry\npercent error\n\n\n\n\n0\n42101009400\nPOLYGON ((-75.22927 39.96054, -75.22865 39.960...\n29.722841\n\n\n1\n42101009500\nPOLYGON ((-75.23536 39.96852, -75.23545 39.969...\n17.925935\n\n\n2\n42101009600\nPOLYGON ((-75.24343 39.96230, -75.24339 39.962...\n64.579176\n\n\n3\n42101013800\nPOLYGON ((-75.17341 39.97779, -75.17386 39.977...\n9.536258\n\n\n4\n42101013900\nPOLYGON ((-75.17313 39.97776, -75.17321 39.977...\n-23.318433\n\n\n\n\n\n\n\n\nmap_data.plot(\n    column=\"percent error\", \n    cmap=\"viridis\",\n    legend=True)\n    \n\n&lt;Axes: &gt;\n\n\n\n\n\n\n#import hvplot.pandas\nchoro = map_data.hvplot(\n    c=\"percent error\", \n    frame_width=600,\n    frame_height=600,\n    alpha=0.5,\n    cmap=\"plasma\",\n    geo=True, \n    tiles='CartoLight')\n\nchoro\n\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/set_operations.py:133: RuntimeWarning: invalid value encountered in intersection\n  return lib.intersection(a, b, **kwargs)\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n\n\n\n\n\n\n  \n\n\n\n\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\nmerged['qct'] = merged['NAME10'].isin(qct)\nmerged.head(n=5)\n\n\n\n\n\n\n\n\ngeometry\ncartodb_id_left\nassessment_date\nbasements\nbeginning_point\nbook_and_page\nbuilding_code\nbuilding_code_description\ncategory_code\ncategory_code_description\ncensus_tract\ncentral_air\ncross_reference\ndate_exterior_condition\ndepth\nexempt_building\nexempt_land\nexterior_condition\nfireplaces\nfrontage\nfuel\ngarage_spaces\ngarage_type\ngeneral_construction\ngeographic_ward\nhomestead_exemption\nhouse_extension\nhouse_number\ninterior_condition\nlocation\nmailing_address_1\nmailing_address_2\nmailing_care_of\nmailing_city_state\nmailing_street\nmailing_zip\nmarket_value\nmarket_value_date\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_of_rooms\nnumber_stories\noff_street_open\nother_building\nowner_1\nowner_2\nparcel_number\nparcel_shape\nquality_grade\nrecording_date\nregistry_number\nsale_date\nsale_price\nseparate_utilities\nsewer\nsite_type\nstate_code\nstreet_code\nstreet_designation\nstreet_direction\nstreet_name\nsuffix\ntaxable_building\ntaxable_land\ntopography\ntotal_area\ntotal_livable_area\ntype_heater\nunfinished\nunit\nutility\nview_type\nyear_built\nyear_built_estimate\nzip_code\nzoning\npin\nbuilding_code_new\nbuilding_code_description_new\nobjectid\nname\nlistname\nmapname\nshape_leng\nshape_area\ncartodb_id_right\ncreated_at\nupdated_at\nindex_right\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\npercent error\nqct\n\n\n\n\n5300\nPOINT (-75.25152 40.04542)\n23579\n2022-05-24T00:00:00Z\nA\n429'5 1/2\"NW SHAWMONT\n54098598\nA21\nDET 1.5 STY MASONRY+OTHER\n1\nSINGLE FAMILY\n219\nY\nNone\nNone\n98.0\n80000.0\n0.0\n3\n1.0\n78.0\nNone\n0.0\nNone\nE\n21\n0\nNone\n7848\n3\n7848 NIXON ST\nMCDAY JOHN B\nNone\nNone\nVILLANOVA PA\n2058 MATSONS CIR\n19085\n294100\nNone\n2.0\n3.0\nNaN\n2.0\n6204.0\nNone\nMCDAY JOHN B\nMCDAY JANE A\n212412910\nE\nC+\n2022-09-16T00:00:00Z\n150N210094\n2022-09-07T00:00:00Z\n450000\nNone\nNone\nNone\nPA\n59920\nST\nNone\nNIXON\nNone\n155300.0\n58800.0\nF\n7594.0\n1595.0\nB\nNone\nNone\nNone\nB\n1975\nY\n19128\nRSA3\n1001390664\n05\nNone\n401696756\nUPPER_ROXBOROUGH\nUpper Roxborough\nUpper Roxborough\n50882.081408\n6.869143e+07\n46.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n250.0\n251.0\n42\n101\n021900\n42101021900\n219\nCensus Tract 219\nG5020\nS\n1410069.0\n51091.0\n+40.0505829\n-075.2470033\n10545\n-13.025614\nFalse\n\n\n12908\nPOINT (-75.12923 40.03141)\n33061\n2022-05-24T00:00:00Z\nF\n122'9\" W OF 3RD ST\n54061847\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n286\nN\nNone\nNone\n75.0\n0.0\n0.0\n4\n0.0\n15.0\nNone\n1.0\nNone\nA\n42\n0\nNone\n316\n4\n316 W FISHER AVE\nELIAS MUNEER\nNone\nNone\nPHILADELPHIA PA\n454 DELMAR\n19128\n107800\nNone\n1.0\n3.0\nNaN\n2.0\n1942.0\nNone\nELIAS MUNEER\nNone\n422248000\nE\nC\n2022-06-27T00:00:00Z\n123N12 229\n2022-06-24T00:00:00Z\n83139\nNone\nNone\nNone\nPA\n34140\nAVE\nW\nFISHER\nNone\n86240.0\n21560.0\nF\n1144.0\n1290.0\nH\nNone\nNone\nNone\nI\n1930\nY\n19120\nRSA5\n1001214868\n24\nROW PORCH FRONT\n401704506\nOLNEY\nOlney\nOlney\n32197.205271\n5.030840e+07\n8.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n108.0\n109.0\n42\n101\n028600\n42101028600\n286\nCensus Tract 286\nG5020\nS\n717845.0\n0.0\n+40.0285757\n-075.1276448\n10600\n27.456476\nTrue\n\n\n20713\nPOINT (-75.20568 40.02309)\n43246\n2022-05-24T00:00:00Z\nE\n126'N OF SALAIGNAC ST\n54006879\nR30\nROW B/GAR 2 STY MASONRY\n1\nSINGLE FAMILY\n211\nY\nNone\nNone\n100.0\n0.0\n0.0\n4\n0.0\n16.0\nNone\n1.0\nNone\nE\n21\n0\nNone\n5455\n4\n5455 VICARIS ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n5455 VICARIS ST\n19128-2826\n245400\nNone\n1.0\n3.0\nNaN\n2.0\n2941.0\nNone\nHOOKS SHEENA\nNone\n213221174\nE\nC\n2022-03-24T00:00:00Z\n96N12 85\n2022-03-04T00:00:00Z\n250000\nNone\nNone\nNone\nPA\n80000\nST\nNone\nVICARIS\nNone\n196320.0\n49080.0\nF\n1600.0\n1110.0\nA\nNone\nNone\nNone\nI\n1965\nY\n19128\nRSA5\n1001540422\n23\nROW POST WAR\n401715911\nWISSAHICKON\nWissahickon\nWissahickon\n17714.185972\n9.064239e+06\n53.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n85.0\n86.0\n42\n101\n021100\n42101021100\n211\nCensus Tract 211\nG5020\nS\n540313.0\n0.0\n+40.0264564\n-075.2065293\n10537\n-7.974018\nFalse\n\n\n2310\nPOINT (-75.17142 39.93675)\n19863\n2022-05-24T00:00:00Z\nNone\n261'6\" W 16TH ST\n54118070\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n22\nNone\nNone\nNone\n61.0\n0.0\n0.0\n3\n0.0\n15.0\nNone\n0.0\nNone\nA\n36\n0\nNone\n1634\n4\n1634 ANNIN ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n1634 ANNIN ST\n19146-3006\n215000\nNone\n1.0\n3.0\nNaN\n2.0\n634.0\nNone\nEXPERIENCE PROPERTY LLC\nNone\n365284200\nE\nC\n2022-11-07T00:00:00Z\n008S030119\n2022-10-19T00:00:00Z\n280000\nNone\nNone\nNone\nPA\n12740\nST\nNone\nANNIN\nNone\n172000.0\n43000.0\nF\n915.0\n1322.0\nNone\nNone\nNone\nNone\nI\n1925\nNone\n19146\nRSA5\n1001067242\n22\nROW TYPICAL\n401693197\nPOINT_BREEZE\nPoint Breeze\nPoint Breeze\n17529.283714\n1.693772e+07\n103.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n39.0\n40.0\n42\n101\n002200\n42101002200\n22\nCensus Tract 22\nG5020\nS\n228696.0\n0.0\n+39.9364111\n-075.1703923\n10363\n28.642106\nTrue\n\n\n16088\nPOINT (-75.17645 39.92744)\n37061\n2022-05-24T00:00:00Z\nD\n198' W OF 18TH ST\n54044658\nO30\nROW 2 STY MASONRY\n1\nSINGLE FAMILY\n37\nN\nNone\nNone\n47.0\n0.0\n0.0\n4\n0.0\n14.0\nNone\n0.0\nNone\nA\n48\n0\nNone\n1829\n4\n1829 HOFFMAN ST\nSIMPLIFILE LC E-RECORDING\nNone\nNone\nPHILADELPHIA PA\n1829 HOFFMAN ST\n19145-2925\n159400\nNone\n1.0\n3.0\nNaN\n2.0\n1977.0\nNone\nWAXMAN ERIN\nNone\n481073400\nE\nC\n2022-05-20T00:00:00Z\n015S130058\n2022-05-20T00:00:00Z\n200000\nNone\nNone\nNone\nPA\n42960\nST\nNone\nHOFFMAN\nNone\n127600.0\n31800.0\nF\n658.0\n896.0\nH\nNone\nNone\nNone\nI\n1920\nY\n19145\nRM1\n1001277147\n22\nROW TYPICAL\n401708174\nWEST_PASSYUNK\nWest Passyunk\nWest Passyunk\n10499.291848\n6.494799e+06\n157.0\n2013-03-19 17:41:50.508000+00:00\n2013-03-19 17:41:50.743000+00:00\n198.0\n199.0\n42\n101\n003701\n42101003701\n37.01\nCensus Tract 37.01\nG5020\nS\n355619.0\n0.0\n+39.9276728\n-075.1806654\n10378\n-32.858445\nTrue\n\n\n\n\n\n\n\n\nnew_data = merged.groupby(\"qct\", as_index=False)[\"percent error\"].median()\n\n\nnew_data\n\n\n\n\n\n\n\n\nqct\npercent error\n\n\n\n\n0\nFalse\n-6.104082\n\n\n1\nTrue\n2.834477\n\n\n\n\n\n\n\n\nPercent error in True areas is positive, which is obviously higher than the False areas. It is true that algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts"
  },
  {
    "objectID": "MUSA550_Python/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "href": "MUSA550_Python/assignment-4.html#part-1-visualizing-crash-data-in-philadelphia",
    "title": "MUSA550 - Assignment 4: Street Networks & Web Scraping",
    "section": "Part 1: Visualizing crash data in Philadelphia",
    "text": "Part 1: Visualizing crash data in Philadelphia\n\n1.1 Load the geometry for the region being analyzed\nWe’ll analyze crashes in the “Central” planning district in Philadelphia, a rough approximation for Center City. Planning districts can be loaded from Open Data Philly. Read the data into a GeoDataFrame using the following link:\nhttp://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\nSelect the “Central” district and extract the geometry polygon for only this district. After this part, you should have a polygon variable of type shapely.geometry.polygon.Polygon.\n\nimport osmnx as ox\nimport geopandas as gpd\n\n\nplanning_district = gpd.read_file(\"http://data.phl.opendata.arcgis.com/datasets/0960ea0f38f44146bb562f2b212075aa_0.geojson\")\n\n\nplanning_district.head()\n\n\n\n\n\n\n\n\nOBJECTID_1\nOBJECTID\nDIST_NAME\nABBREV\nShape__Area\nShape__Length\nPlanningDist\nDaytimePop\ngeometry\n\n\n\n\n0\n1\n14\nRiver Wards\nRW\n2.107270e+08\n66931.595020\nNaN\nNaN\nPOLYGON ((-75.09798 40.00496, -75.09687 40.005...\n\n\n1\n2\n3\nNorth Delaware\nNDEL\n2.700915e+08\n89213.074378\nNaN\nNaN\nPOLYGON ((-74.98159 40.05363, -74.98139 40.053...\n\n\n2\n3\n0\nLower Far Northeast\nLFNE\n3.068529e+08\n92703.285159\nNaN\nNaN\nPOLYGON ((-74.96443 40.11728, -74.96434 40.117...\n\n\n3\n4\n9\nCentral\nCTR\n1.782880e+08\n71405.143450\nNaN\nNaN\nPOLYGON ((-75.14791 39.96733, -75.14715 39.967...\n\n\n4\n5\n10\nUniversity Southwest\nUSW\n1.296468e+08\n65267.676141\nNaN\nNaN\nPOLYGON ((-75.18742 39.96338, -75.18644 39.963...\n\n\n\n\n\n\n\n\ncentral_district = planning_district.query(\"DIST_NAME == 'Central'\")\n\n\nax = ox.project_gdf(central_district).plot(fc=\"lightblue\", ec=\"gray\")\nax.set_axis_off()\n\n\n\n\n\ncenter_city_outline = central_district.squeeze().geometry\n\ncenter_city_outline\n\n\n\n\n\ntype(center_city_outline)\n\nshapely.geometry.polygon.Polygon\n\n\n\n\n1.2 Get the street network graph\nUse OSMnx to create a network graph (of type ‘drive’) from your polygon boundary in 1.1.\n\n# Get the graph\nG_cc = ox.graph_from_polygon(center_city_outline, network_type=\"drive\")\n\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/constructive.py:181: RuntimeWarning: invalid value encountered in buffer\n  return lib.buffer(\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n  return lib.intersects(a, b, **kwargs)\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/set_operations.py:340: RuntimeWarning: invalid value encountered in union\n  return lib.union(a, b, **kwargs)\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/predicates.py:798: RuntimeWarning: invalid value encountered in intersects\n  return lib.intersects(a, b, **kwargs)\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/set_operations.py:340: RuntimeWarning: invalid value encountered in union\n  return lib.union(a, b, **kwargs)\n\n\n\n# Viola!\nox.plot_graph(ox.project_graph(G_cc), node_size=0);\n\n\n\n\n\n\n1.3 Convert your network graph edges to a GeoDataFrame\nUse OSMnx to create a GeoDataFrame of the network edges in the graph object from part 1.2. The GeoDataFrame should contain the edges but not the nodes from the network.\n\ntype(G_cc)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\n# only get the edges\ncc_edges = ox.graph_to_gdfs(G_cc, edges=True, nodes=False)\n\n\ntype(cc_edges)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\n1.4 Load PennDOT crash data\nData for crashes (of all types) for 2020, 2021, and 2022 in Philadelphia County is available at the following path:\n./data/CRASH_PHILADELPHIA_XXXX.csv\nYou should see three separate files in the data/ folder. Use pandas to read each of the CSV files, and combine them into a single dataframe using pd.concat().\nThe data was downloaded for Philadelphia County from here.\n\nimport pandas as pd\n\n\ncrash_2020 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2020.csv\")\ncrash_2021 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2021.csv\")\ncrash_2022 = pd.read_csv(\"./data/CRASH_PHILADELPHIA_2022.csv\")\ncrash_all = pd.concat([crash_2020, crash_2021, crash_2022])\n\n\nlen(crash_all)\n\n29463\n\n\n\n\n1.5 Convert the crash data to a GeoDataFrame\nYou will need to use the DEC_LAT and DEC_LONG columns for latitude and longitude.\nThe full data dictionary for the data is available here\n\ncrash_all = crash_all.dropna(subset=[\"DEC_LAT\", \"DEC_LONG\"])\n\n\ng_crash = gpd.GeoDataFrame(\n    crash_all,  # The pandas DataFrame\n    geometry=gpd.points_from_xy(crash_all[\"DEC_LONG\"], crash_all[\"DEC_LAT\"]), # The geometry!\n    crs=\"EPSG:4326\", # The CRS \n)\ntype(g_crash)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\n1.6 Trim the crash data to Center City\n\nGet the boundary of the edges data frame (from part 1.3). Accessing the .geometry.unary_union.convex_hull property will give you a nice outer boundary region.\nTrim the crashes using the within() function of the crash GeoDataFrame to find which crashes are within the boundary.\n\nThere should be about 3,750 crashes within the Central district.\n\nboundary = cc_edges.geometry.unary_union.convex_hull\n\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/shapely/set_operations.py:426: RuntimeWarning: invalid value encountered in unary_union\n  return lib.unary_union(collections, **kwargs)\n\n\n\nwithin = g_crash[g_crash.within(boundary) == True]\n\n\nlen(within)\n\n3751\n\n\n\n\n1.7 Re-project our data into an approriate CRS\nWe’ll need to find the nearest edge (street) in our graph for each crash. To do this, osmnx will calculate the distance from each crash to the graph edges. For this calculation to be accurate, we need to convert from latitude/longitude\nWe’ll convert the local state plane CRS for Philadelphia, EPSG=2272\n\nTwo steps:\n\nProject the graph object (G) using the ox.project_graph. Run ox.project_graph? to see the documentation for how to convert to a specific CRS.\nProject the crash data using the .to_crs() function.\n\n\nreproject = ox.project_graph(G_cc, to_crs=2272)\n\n\nox.project_graph?\n\n\nSignature: ox.project_graph(G, to_crs=None)\nDocstring:\nReproject a graph from its current CRS to another.\nIf `to_crs` is None, project the graph to the UTM CRS for the UTM zone in\nwhich the graph's centroid lies. Otherwise, project the graph to the CRS\ndefined by `to_crs`.\nParameters\n----------\nG : networkx.MultiDiGraph\n    the graph to be projected\nto_crs : string or pyproj.CRS\n    if None, project graph to UTM zone in which graph centroid lies,\n    otherwise project graph to this CRS\nReturns\n-------\nG_proj : networkx.MultiDiGraph\n    the projected graph\nFile:      ~/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/osmnx/projection.py\nType:      function\n\n\n\n\n\n\n1.8 Find the nearest edge for each crash\nSee: ox.distance.nearest_edges(). It takes three arguments:\n\nthe network graph\nthe longitude of your crash data (the x attribute of the geometry column)\nthe latitude of your crash data (the y attribute of the geometry column)\n\nYou will get a numpy array with 3 columns that represent (u, v, key) where each u and v are the node IDs that the edge links together. We will ignore the key value for our analysis.\n\nwithin_x = within.geometry.x\nwithin_y = within.geometry.y\n\n\nnode = ox.distance.nearest_edges(G_cc, within_x, within_y) \n\n\nox.distance.nearest_edges?\n\n\nSignature: ox.distance.nearest_edges(G, X, Y, interpolate=None, return_dist=False)\nDocstring:\nFind the nearest edge to a point or to each of several points.\nIf `X` and `Y` are single coordinate values, this will return the nearest\nedge to that point. If `X` and `Y` are lists of coordinate values, this\nwill return the nearest edge to each point. This function uses an R-tree\nspatial index and minimizes the euclidean distance from each point to the\npossible matches. For accurate results, use a projected graph and points.\nParameters\n----------\nG : networkx.MultiDiGraph\n    graph in which to find nearest edges\nX : float or list\n    points' x (longitude) coordinates, in same CRS/units as graph and\n    containing no nulls\nY : float or list\n    points' y (latitude) coordinates, in same CRS/units as graph and\n    containing no nulls\ninterpolate : float\n    deprecated, do not use\nreturn_dist : bool\n    optionally also return distance between points and nearest edges\nReturns\n-------\nne or (ne, dist) : tuple or list\n    nearest edges as (u, v, key) or optionally a tuple where `dist`\n    contains distances between the points and their nearest edges\nFile:      ~/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/osmnx/distance.py\nType:      function\n\n\n\n\n\n1.9 Calculate the total number of crashes per street\n\nMake a DataFrame from your data from part 1.7 with three columns, u, v, and key (we will only use the u and v columns)\nGroup by u and v and calculate the size\nReset the index and name your size() column as crash_count\n\nAfter this step you should have a DataFrame with three columns: u, v, and crash_count.\n\n# convert graph to geodataframe\nreproject_gdf = ox.graph_to_gdfs(reproject, edges=True, nodes=False)\n\n\nreproject_groupby = reproject_gdf.groupby([\"u\", \"v\"], as_index=False).size().rename(columns={\"size\": \"crash_count\"})\nlen(reproject_groupby)\n\n3883\n\n\n\n\n1.10 Merge your edges GeoDataFrame and crash count DataFrame\nYou can use pandas to merge them on the u and v columns. This will associate the total crash count with each edge in the street network.\nTips: - Use a left merge where the first argument of the merge is the edges GeoDataFrame. This ensures no edges are removed during the merge. - Use the fillna(0) function to fill in missing crash count values with zero.\n\nmerged_data = pd.merge(cc_edges, reproject_groupby, how=\"left\", on=[\"u\", \"v\"]).fillna(0)\nlen(merged_data)\n\n3896\n\n\n\n\n1.11 Calculate a “Crash Index”\nLet’s calculate a “crash index” that provides a normalized measure of the crash frequency per street. To do this, we’ll need to:\n\nCalculate the total crash count divided by the street length, using the length column\nPerform a log transformation of the crash/length variable — use numpy’s log10() function\nNormalize the index from 0 to 1 (see the lecture notes for an example of this transformation)\n\nNote: since the crash index involves a log transformation, you should only calculate the index for streets where the crash count is greater than zero.\nAfter this step, you should have a new column in the data frame from 1.9 that includes a column called part 1.9.\n\nimport numpy as np\n\n\ncrash_by_length = merged_data['crash_count'].count()/merged_data['length']\nlogged = np.log10(crash_by_length)\n# normalized = log_index/crash_index.max()\n# merged_data[\"part 1.9\"] = normalized\n# normalized\nmerged_data[\"part 1.9\"] = (logged-np.min(logged))/(np.max(logged)-np.min(logged))\nmerged_data[\"part 1.9\"].mean()\n\n0.464853601936429\n\n\n\n\n1.12 Plot a histogram of the crash index values\nUse matplotlib’s hist() function to plot the crash index values from the previous step.\nYou should see that the index values are Gaussian-distributed, providing justification for why we log-transformed!\n\nimport matplotlib.pyplot as plt\n\n\nmerged_data[\"part 1.9\"]\n\n0       0.526520\n1       0.398106\n2       0.423697\n3       0.500100\n4       0.487254\n          ...   \n3891    0.488001\n3892    0.551123\n3893    0.573787\n3894    0.681563\n3895    0.488001\nName: part 1.9, Length: 3896, dtype: float64\n\n\n\nplt.hist(merged_data[\"part 1.9\"])\n\n(array([   4.,   27.,   88., 1099., 1366.,  906.,  238.,  144.,   19.,\n           5.]),\n array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n1.13 Plot an interactive map of the street networks, colored by the crash index\nYou can use GeoPandas to make an interactive Folium map, coloring the streets by the crash index column.\nTip: if you use the viridis color map, try using a “dark” tile set for better constrast of the colors.\n\nimport folium\n\n\ninteractive_map = merged_data.explore(column=\"part 1.9\", tiles=\"CartoDB dark_matter\", scheme=\"Quantiles\", k=5, cmap=\"plasma\")\n\ninteractive_map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "MUSA550_Python/assignment-4.html#part-2-scraping-craigslist",
    "href": "MUSA550_Python/assignment-4.html#part-2-scraping-craigslist",
    "title": "MUSA550 - Assignment 4: Street Networks & Web Scraping",
    "section": "Part 2: Scraping Craigslist",
    "text": "Part 2: Scraping Craigslist\nIn this part, we’ll be extracting information on apartments from Craigslist search results. You’ll be using Selenium and BeautifulSoup to extract the relevant information from the HTML text.\nFor reference on CSS selectors, please see the notes from Week 6.\n\nPrimer: the Craigslist website URL\nWe’ll start with the Philadelphia region. First we need to figure out how to submit a query to Craigslist. As with many websites, one way you can do this is simply by constructing the proper URL and sending it to Craigslist.\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThere are three components to this URL.\n\nThe base URL: http://philadelphia.craigslist.org/search/apa\nThe user’s search parameters: ?min_price=1&min_bedrooms=1&minSqft=1\n\n\nWe will send nonzero defaults for some parameters (bedrooms, size, price) in order to exclude results that have empty values for these parameters.\n\n\nThe URL hash: #search=1~gallery~0~0\n\n\nAs we will see later, this part will be important because it contains the search page result number.\n\nThe Craigslist website requires Javascript, so we’ll need to use Selenium to load the page, and then use BeautifulSoup to extract the information we want.\n\n\n2.1 Initialize a selenium driver and open Craigslist\nAs discussed in lecture, you can use Chrome, Firefox, or Edge as your selenium driver. In this part, you should do two things:\n\nInitialize the selenium driver\nUse the driver.get() function to open the following URL:\n\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\nThis will give you the search results for 1-bedroom apartments in Philadelphia.\n\n# Import the webdriver from selenium\nfrom selenium import webdriver\n\n\n# UNCOMMENT BELOW TO USE CHROME\ndriver = webdriver.Chrome()\n\n\nurl = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=1gallery0~0\"\ndriver.get(url)\n\n\n\n2.2 Initialize your “soup”\nOnce selenium has the page open, we can get the page source from the driver and use BeautifulSoup to parse it. In this part, initialize a BeautifulSoup object with the driver’s page source\n\n# Start with the usual imports\n# We'll use these throughout\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport requests\n\n\n# Initialize the soup for this page\n\nsoup1 = BeautifulSoup(driver.page_source, \"html.parser\")\n\n\n\n2.3 Parsing the HTML\nNow that we have our “soup” object, we can use BeautifulSoup to extract out the elements we need:\n\nUse the Web Inspector to identify the HTML element that holds the information on each apartment listing.\nUse BeautifulSoup to extract these elements from the HTML.\n\nAt the end of this part, you should have a list of 120 elements, where each element is the listing for a specific apartment on the search page.\n\n# Use the # for ID selector\nselector = \".cl-search-result\"\n\nrows = soup1.select(selector)\nlen(rows)\n\n120\n\n\n\n\n2.4 Find the relevant pieces of information\nWe will now focus on the first element in the list of 120 apartments. Use the prettify() function to print out the HTML for this first element.\nFrom this HTML, identify the HTML elements that hold:\n\nThe apartment price\nThe number of bedrooms\nThe square footage\nThe apartment title\n\nFor the first apartment, print out each of these pieces of information, using BeautifulSoup to select the proper elements.\n\nrow1 = rows[0]\n\n\nprint(row1.prettify())\n\n&lt;li class=\"cl-search-result cl-search-view-mode-gallery\" data-pid=\"7678404936\" title=\"Concierge, Stainless steel appliances, Yoga Studio\"&gt;\n &lt;div class=\"gallery-card\"&gt;\n  &lt;div class=\"cl-gallery\"&gt;\n   &lt;div class=\"gallery-inner\"&gt;\n    &lt;a class=\"main\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-concierge-stainless-steel/7678404936.html\"&gt;\n     &lt;div class=\"swipe\" style=\"visibility: visible;\"&gt;\n      &lt;div class=\"swipe-wrap\" style=\"width: 5688px;\"&gt;\n       &lt;div data-index=\"0\" style=\"width: 316px; left: 0px; transition-duration: 0ms; transform: translateX(0px);\"&gt;\n        &lt;span class=\"loading icom-\"&gt;\n        &lt;/span&gt;\n        &lt;img alt=\"Concierge, Stainless steel appliances, Yoga Studio 1\" src=\"https://images.craigslist.org/00i0i_hntUEmw3Am3_09i05T_300x300.jpg\"/&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"1\" style=\"width: 316px; left: -316px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"2\" style=\"width: 316px; left: -632px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"3\" style=\"width: 316px; left: -948px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"4\" style=\"width: 316px; left: -1264px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"5\" style=\"width: 316px; left: -1580px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"6\" style=\"width: 316px; left: -1896px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"7\" style=\"width: 316px; left: -2212px; transition-duration: 0ms; transform: translateX(316px);\"&gt;\n       &lt;/div&gt;\n       &lt;div data-index=\"8\" style=\"width: 316px; left: -2528px; transition-duration: 0ms; transform: translateX(-316px);\"&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-back-arrow icom-\"&gt;\n     &lt;/div&gt;\n     &lt;div class=\"slider-forward-arrow icom-\"&gt;\n     &lt;/div&gt;\n    &lt;/a&gt;\n   &lt;/div&gt;\n   &lt;div class=\"dots\"&gt;\n    &lt;span class=\"dot selected\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n    &lt;span class=\"dot\"&gt;\n     •\n    &lt;/span&gt;\n   &lt;/div&gt;\n  &lt;/div&gt;\n  &lt;a class=\"cl-app-anchor text-only posting-title\" href=\"https://philadelphia.craigslist.org/apa/d/philadelphia-concierge-stainless-steel/7678404936.html\" tabindex=\"0\"&gt;\n   &lt;span class=\"label\"&gt;\n    Concierge, Stainless steel appliances, Yoga Studio\n   &lt;/span&gt;\n  &lt;/a&gt;\n  &lt;div class=\"meta\"&gt;\n   11 mins ago\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   &lt;span class=\"housing-meta\"&gt;\n    &lt;span class=\"post-bedrooms\"&gt;\n     3br\n    &lt;/span&gt;\n    &lt;span class=\"post-sqft\"&gt;\n     1883ft\n     &lt;span class=\"exponent\"&gt;\n      2\n     &lt;/span&gt;\n    &lt;/span&gt;\n   &lt;/span&gt;\n   &lt;span class=\"separator\"&gt;\n    ·\n   &lt;/span&gt;\n   Steps from Rittenhouse Square\n  &lt;/div&gt;\n  &lt;span class=\"priceinfo\"&gt;\n   $5,074\n  &lt;/span&gt;\n  &lt;button class=\"bd-button cl-favorite-button icon-only\" tabindex=\"0\" title=\"add to favorites list\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n   &lt;/span&gt;\n  &lt;/button&gt;\n  &lt;button class=\"bd-button cl-banish-button icon-only\" tabindex=\"0\" title=\"hide posting\" type=\"button\"&gt;\n   &lt;span class=\"icon icom-\"&gt;\n   &lt;/span&gt;\n   &lt;span class=\"label\"&gt;\n    hide\n   &lt;/span&gt;\n  &lt;/button&gt;\n &lt;/div&gt;\n&lt;/li&gt;\n\n\n\n\n# Use the . to specify class name\nprice = row1.select_one(\".priceinfo\").text\nprint(\"the apartment price is\", price)\n\n# The number of bedroom\nnbed = row1.select_one(\".post-bedrooms\").text\nprint(\"the number of bedroom is\", nbed)\n\n# The square footage\nsquare = row1.select_one(\".post-sqft\").text\nprint(\"the square footage is\", square)\n\n# apartment title\ntitle = row1.select_one(\".label\").text\nprint(\"the apartment title is\", title)\n\nthe apartment price is $5,074\nthe number of bedroom is 3br\nthe square footage is 1883ft2\nthe apartment title is Concierge, Stainless steel appliances, Yoga Studio\n\n\n\n\n2.5 Functions to format the results\nIn this section, you’ll create functions that take in the raw string elements for price, size, and number of bedrooms and returns them formatted as numbers.\nI’ve started the functions to format the values. You should finish theses functions in this section.\nHints - You can use string formatting functions like string.replace() and string.strip() - The int() and float() functions can convert strings to numbers\n\ndef format_bedrooms(bedrooms_string):\n    # Format the bedrooms string and return an int\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    x = bedrooms_string.replace(\"br\", \"\")\n    \n    return int(x)\n\n\ndef format_size(size_string):\n    # Format the size string and return a float\n    # \n    # This will involve using the string.replace() function to \n    # remove unwanted characters\n    y = size_string.replace(\"ft2\", \"\")\n    \n    return float(y)\n\n\ndef format_price(price_string):\n    # Format the price string and return a float\n    # \n    # This will involve using the string.strip() function to \n    # remove unwanted characters\n    z = price_string.strip(\"$\").replace(\",\",\"\")\n    \n    return float(z)\n\n\n\n2.6 Putting it all together\nIn this part, you’ll complete the code block below using results from previous parts. The code will loop over 5 pages of search results and scrape data for 600 apartments.\nWe can get a specific page by changing the search=PAGE part of the URL hash. For example, to get page 2 instead of page 1, we will navigate to:\nhttps://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1#search=2gallery0~0\nIn the code below, the outer for loop will loop over 5 pages of search results. The inner for loop will loop over the 120 apartments listed on each search page.\nFill in the missing pieces of the inner loop using the code from the previous section. We will be able to extract out the relevant pieces of info for each apartment.\nAfter filling in the missing pieces and executing the code cell, you should have a Data Frame called results that holds the data for 600 apartment listings.\n\nNotes\nBe careful if you try to scrape more listings. Craigslist will temporarily ban your IP address (for a very short time) if you scrape too much at once. I’ve added a sleep() function to the for loop to wait 30 seconds between scraping requests.\nIf the for loop gets stuck at the “Processing page X…” step for more than a minute or so, your IP address is probably banned temporarily, and you’ll have to wait a few minutes before trying again.\n\nfrom time import sleep\n\n\nresults = []\n\n# search in batches of 120 for 5 pages\n# NOTE: you will get temporarily banned if running more than ~5 pages or so\n# the API limits are more leninient during off-peak times, and you can try\n# experimenting with more pages\nmax_pages = 5\n\n# The base URL we will be using\nbase_url = \"https://philadelphia.craigslist.org/search/apa?min_price=1&min_bedrooms=1&minSqft=1\"\n\n# loop over each page of search results\nfor page_num in range(0, max_pages):\n    print(f\"Processing page {page_num}...\")\n\n    # Update the URL hash for this page number and make the combined URL\n    url_hash = f\"#search=1~gallery~{page_num}~0\"\n    url = base_url + url_hash\n\n    # Go to the driver and wait for 5 seconds\n    driver.get(url)\n    sleep(5)\n\n    # YOUR CODE: get the list of all apartments\n    # This is the same code from Part 1.2 and 1.3\n    # It should be a list of 120 apartments\n    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n    selector = \".cl-search-result\"\n    apts = soup.select(selector)\n    print(\"Number of apartments = \", len(apts))\n\n    # loop over each apartment in the list\n    page_results = []\n    for apt in apts:\n\n        # YOUR CODE: the bedrooms string\n        bedrooms = apt.select_one(\".post-bedrooms\").text\n\n        # YOUR CODE: the size string\n        size = apt.select_one(\".post-sqft\").text\n\n        # YOUR CODE: the title string\n        title = apt.select_one(\".label\").text\n\n        # YOUR CODE: the price string\n        price = apt.select_one(\".priceinfo\").text\n\n\n        # Format using functions from Part 1.5\n        bedrooms = format_bedrooms(bedrooms)\n        size = format_size(size)\n        price = format_price(price)\n\n        # Save the result\n        page_results.append([price, size, bedrooms, title])\n\n    # Create a dataframe and save\n    col_names = [\"price\", \"size\", \"bedrooms\", \"title\"]\n    df = pd.DataFrame(page_results, columns=col_names)\n    results.append(df)\n\n    print(\"sleeping for 10 seconds between calls\")\n    sleep(10)\n\n# Finally, concatenate all the results\nresults = pd.concat(results, axis=0).reset_index(drop=True)\n\nProcessing page 0...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 1...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 2...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 3...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\nProcessing page 4...\nNumber of apartments =  120\nsleeping for 10 seconds between calls\n\n\n\nresults.tail()\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\n\n\n\n\n595\n1400.0\n880.0\n1\nSpacious apt in the heart of Fishtown! With Ou...\n\n\n596\n3775.0\n675.0\n1\nHandyman and maintenance service, 24/7 onsite ...\n\n\n597\n1840.0\n930.0\n1\nPet-friendly living, BBQ and deck area\n\n\n598\n1365.0\n728.0\n1\nWelcome to The Grande in Conshohocken.\n\n\n599\n1350.0\n1392.0\n3\nAdorable 3 bedroom 1 & 1/2 bath bath rowhome l...\n\n\n\n\n\n\n\n\nresults[0:10]\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\n\n\n\n\n0\n5074.0\n1883.0\n3\nConcierge, Stainless steel appliances, Yoga St...\n\n\n1\n1495.0\n1200.0\n2\nYour own So Philly HOUSE! *Great Location, Ren...\n\n\n2\n1899.0\n692.0\n1\nHeated 3 Season Pool, Cyber Café, Zen Courtyard\n\n\n3\n900.0\n800.0\n1\n- 1 Bed and 1 Bath - Port Chester\n\n\n4\n2145.0\n1157.0\n2\nGreat community for great people! Visit today!\n\n\n5\n2410.0\n1428.0\n2\n2/bd, Pet-friendly living, in Philadelphia PA\n\n\n6\n1100.0\n1000.0\n2\nA Very Nice 2Bed 1Bath for Rent.\n\n\n7\n1650.0\n612.0\n2\n5261 Larchwood Ave #1- Beautiful 2BD apartment\n\n\n8\n1700.0\n605.0\n1\n1715 Wallace Street #301 - Updated 1BD apartme...\n\n\n9\n1300.0\n750.0\n2\n2123 Tasker St #2 – 2BR 2nd floor apartment in...\n\n\n\n\n\n\n\n\nresults[120:130]\n\n\n\n\n\n\n\n\nprice\nsize\nbedrooms\ntitle\n\n\n\n\n120\n2690.0\n1500.0\n3\nFairmount Unit-206\n\n\n121\n1690.0\n950.0\n2\nQueen Village Unit-11\n\n\n122\n1500.0\n700.0\n1\n1BR 1 Bath 1010 Race St\n\n\n123\n2286.0\n1401.0\n1\nLocated in Philadelphia, 1BD 1BA, Online Payme...\n\n\n124\n1500.0\n1179.0\n1\nLARGE BEAUTIFUL LOFT-STYLE 1 BEDROOM(Philadelp...\n\n\n125\n3995.0\n1159.0\n2\nOn-demand house cleaning services, Laundry and...\n\n\n126\n1508.0\n518.0\n1\n1B/1B, Package Receiving, Large Closets\n\n\n127\n2199.0\n691.0\n1\nBusiness Meeting Room, Cyber Café, Yoga Studio\n\n\n128\n850.0\n1500.0\n3\nRoom Available in West Philly Grad Student House!\n\n\n129\n2034.0\n637.0\n1\nFree Weights, Cable Ready, Elevator\n\n\n\n\n\n\n\n\n\n\n2.7 Plotting the distribution of prices\nUse matplotlib’s hist() function to make two histograms for:\n\nApartment prices\nApartment prices per square foot (price / size)\n\nMake sure to add labels to the respective axes and a title describing the plot.\n\nimport matplotlib.pyplot as plt\n\n\nplt.hist(results[\"price\"], bins=45, edgecolor='black', linewidth = 0.5, color='orange')\nplt.title(\"apartment prices distribution in Philadelphia\")\nplt.xlabel(\"price per month\")\nplt.ylabel(\"number of apartments\")\n\nText(0, 0.5, 'number of apartments')\n\n\n\n\n\n\nresults.duplicated()\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n595    False\n596    False\n597    False\n598    False\n599    False\nLength: 600, dtype: bool\n\n\n\nprice_size = results[\"price\"]/results[\"size\"]\n\n\nprice_size\n\n0      2.694636\n1      1.245833\n2      2.744220\n3      1.125000\n4      1.853933\n         ...   \n595    1.590909\n596    5.592593\n597    1.978495\n598    1.875000\n599    0.969828\nLength: 600, dtype: float64\n\n\n\n#price_size= [x if x &lt; 200 else np.median(price_size.to_list()) for x in price_size.to_list()]\n\n\nplt.hist(price_size, bins = 40, edgecolor='black', linewidth = 0.5, color='gray', range = (0,10))\nplt.title(\"apartment prices per square foot in Philadelphia\")\nplt.xlabel(\"price per square foot/month\")\nplt.ylabel(\"number of apartments\")\n\nText(0, 0.5, 'number of apartments')\n\n\n\n\n\n\nSide note: rental prices per sq. ft. from Craigslist\nThe histogram of price per sq ft should be centered around ~1.5. Here is a plot of how Philadelphia’s rents compare to the other most populous cities:\n\nSource\n\n\n\n2.8 Comparing prices for different sizes\nUse altair to explore the relationship between price, size, and number of bedrooms. Make an interactive scatter plot of price (x-axis) vs. size (y-axis), with the points colored by the number of bedrooms.\nMake sure the plot is interactive (zoom-able and pan-able) and add a tooltip with all of the columns in our scraped data frame.\nWith this sort of plot, you can quickly see the outlier apartments in terms of size and price.\n\nimport altair as alt  \n\n\n# Step 1: Initialize the chart with the data\nchart = alt.Chart(results)\n\n# Step 2: Define what kind of marks to use\nchart = chart.mark_circle(size=60)\n\n# Step 3: Encode the visual channels\nchart = chart.encode(\n    x=\"price\",\n    y=\"size\",\n    color=\"bedrooms\", \n    tooltip=[\"price\", \"size\", \"bedrooms\", \"title\"],\n)\n\n# Optional: Make the chart interactive\nchart.interactive()"
  },
  {
    "objectID": "MUSA550_Python/MUSA550_Assignment2_HangZhao.html",
    "href": "MUSA550_Python/MUSA550_Assignment2_HangZhao.html",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 2",
    "section": "",
    "text": "Hang Zhao, Collaborated with George Chen\n\n\nPart 1: Load the datasets and prepare for plotting\n\n# Let's setup the imports we'll need first\n\nimport altair as alt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\n# import air quality index data\naqi_days = pd.read_csv(\"./MUSA_550/data/air_quality_index_days.csv\")\n\n# import philly shooting data\nshooting = pd.read_csv(\"./MUSA_550/data/shootings.csv\")\n\n# import heat vulnerability index data\nheat_index = pd.read_csv(\"./MUSA_550/data/HEAT_EXPOSURE_CENSUS_TRACT.csv\")\n\n\n# adjust the shooting data to 5000 random samples for altair specifically\nshooting_5000 = shooting.sample(n=5000)\n\n\nresult = shooting_5000.groupby('wound')['dist'].aggregate(['mean','median'])\n\n\nheat_index.head(n=10)\n\n\n\n\n\n\n\n\nOBJECTID\nGEOID10\nNAME10\nN_VERYHIGH\nYEAR\nHSI_SCORE\nHEI_SCORE\nHVI_SCORE\nShape__Area\nShape__Length\n\n\n\n\n0\n1\n42101019000\n190.0\n0\n2023\n0.428964\n0.130289\n0.811801\n1.350244e+06\n5483.874734\n\n\n1\n2\n42101019100\n191.0\n0\n2023\n1.449914\n2.208449\n1.447200\n2.491485e+06\n7728.470750\n\n\n2\n3\n42101019200\n192.0\n0\n2023\n3.699012\n1.705167\n3.060558\n1.120759e+06\n5638.148182\n\n\n3\n4\n42101019700\n197.0\n1\n2023\n6.630249\n2.763526\n5.068283\n7.688302e+05\n3791.744189\n\n\n4\n5\n42101019800\n198.0\n1\n2023\n8.357598\n2.549294\n6.149583\n9.234407e+05\n4189.517838\n\n\n5\n6\n42101019900\n199.0\n1\n2023\n7.477713\n2.807744\n5.447631\n1.129508e+06\n4314.962567\n\n\n6\n7\n42101020000\n200.0\n1\n2023\n9.654336\n-0.354854\n6.836606\n5.403059e+05\n4048.132611\n\n\n7\n8\n42101020200\n202.0\n1\n2023\n9.290132\n0.701139\n6.267128\n1.291572e+06\n5104.433910\n\n\n8\n9\n42101020300\n203.0\n1\n2023\n9.015131\n1.363072\n6.315538\n8.672223e+05\n5262.760049\n\n\n9\n10\n42101020400\n204.0\n0\n2023\n6.183471\n1.296948\n4.387879\n6.954559e+05\n3715.326123\n\n\n\n\n\n\n\n\n\nPart 2: One Matplotlib Plot\n\n# plot three lines in the same graph\nfig, ax = plt.subplots(figsize=(10, 6))\naqi_days.plot(x='year', y='good', color='#3EBCB8', ax=ax)\naqi_days.plot(x='year', y='moderate', color='#FC8653', ax=ax)\naqi_days.plot(x='year', y='unhealthy', color='#C94256', ax=ax)\n\n# creata a label for y-axis\nplt.ylabel('Days')\n\n# create a verticla line for reference of information\nax.axvline(x=1996, c='k', linewidth=0.5)\nax.text(1996, 1, \"Autobody refinishing regulations adopted\", ha='left', fontsize=11);\n\n\n\n\nMatplotlib is basically widely used for most of the datasets and cases, as it can plot several types of data in different graph types, including line graph, bar chart, histogram, scatter, pie chart, and so on.\nHere I use matplotlib because it is easy to use and can interpret this ‘air quality index’ dataset quite well, as it is a continuous dataset with years and three levels of AQI in each year, which means I can use year as the x-axis and days as y-axis to compare the three AQI levels and read the message directly from this line graph.\n\n\nOne Seaborn Plot\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {1: \"#EB5234\", 0: \"#72DBD7\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the seaborn scatterplot() function\nsns.scatterplot(\n    x=\"HVI_SCORE\", # the x column\n    y=\"HEI_SCORE\", # the y column\n    hue=\"N_VERYHIGH\", # the third dimension (color)\n    size=\"YEAR\",\n    sizes=[50],\n    data=heat_index, # pass in the data\n    ax=ax, # plot on the axes object we made\n    **style # add our style keywords\n)\n\n\n# Format with matplotlib commands\nax.set_xlabel(\"Heat Vulnerability Index\" )\nax.set_ylabel(\"Heat Exposure Index\")\nax.grid(True)\nax.legend(loc='best')\n\n&lt;matplotlib.legend.Legend at 0x291673af0&gt;\n\n\n\n\n\nSeaborn is a great tool when you are trying to do visualization for statistical analysis, which can create colorful and informative graphs that can be used for showing statistical results, with short and easy code. For this one, the better way to do it is to use sns.lmplot() or sns.regplot(), which could make scatter with a fitted line, and that only requires one line of code.\n\n\nThree Altair Plots\n\n# Create the selection box\nbrush = alt.selection_interval()\n\n\n(\n    alt.Chart(heat_index)  # Create the chart\n    .mark_point()  # Use point markers\n    .encode(  # Encode\n        x=alt.X(\"HVI_SCORE\", scale=alt.Scale(zero=False)),  # X\n        y=alt.Y(\"HEI_SCORE\", scale=alt.Scale(zero=False)),  # Y\n        # NEW: Use a conditional color based on brush\n        color=alt.condition(brush, \"N_VERYHIGH\", alt.value(\"lightgray\")),  # Color\n        tooltip=[\"N_VERYHIGH\", \"HVI_SCORE\", \"HEI_SCORE\"],  # Tooltip\n    )\n    .add_params(brush)  # NEW: Add brush parameter\n    .properties(width=200, height=200)  # Set width/height\n    .facet(column=\"N_VERYHIGH\") # Facet\n)\n\n\n\n\n\n\n\nThese are the separate scatter plots of HEI versus HVI scores, separated by “Number of Very High Scores” category\n\n# Setup the selection brush\nbrush = alt.selection_interval()\n\n# Setup the chart\n(\n    alt.Chart(heat_index)\n    .mark_circle()\n    .encode(\n        x=alt.X(alt.repeat(\"column\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        y=alt.Y(alt.repeat(\"row\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        color=alt.condition(\n            brush, \"N_VERYHIGH:N\", alt.value(\"lightgray\")\n        ),  # conditional color\n    )\n    .properties(\n        width=200,\n        height=200,\n    )\n    .add_params(brush)\n    .repeat(  # repeat variables across rows and columns\n        row=[\"HSI_SCORE\", \"HEI_SCORE\", \"HVI_SCORE\"],\n        column=[\"HVI_SCORE\", \"HEI_SCORE\", \"HSI_SCORE\"],\n    )\n)\n\n\n\n\n\n\n\nThese are the correlation graphs for HVI, HEI, and HSI, which stands for heat vulnerability index, heat exposure index, and heat sensitivity index respectively.\n\nshooting dataset\n\n# plot the bar chart showing victim's sex from 2015-2023\n(\n    alt.Chart(shooting_5000)\n    .mark_bar()\n    .encode(\n        # X should show the (normalized) count of each group\n        x=alt.X(\"*:Q\", aggregate=\"count\", stack=\"normalize\"), # The * is a placeholder here\n        y=\"year:N\",\n        color=\"sex:N\",\n        tooltip=[\"year\", \"sex\", \"count(*):Q\"],\n    )\n)\n\n\n\n\n\n\n\nClearly, Male victims are much more than female victims\n\n# plot the barchart showing shooting victims race and counts from 2015-2023\nalt.Chart(shooting_5000).mark_bar(\n    cornerRadiusTopLeft=3,\n    cornerRadiusTopRight=3\n).encode(\n    x='year:O',\n    y='count():Q',\n    color='race:N'\n)\n\n\n\n\n\n\n\nBlack ethnic group are the greatest victim, and shooting cases are clearly higher during the Covid years.\n\n# groupby the dataset and sum up fatal shooting case numbers\nshooting_groupby = shooting_5000.groupby([\"year\", \"wound\"], as_index=False)[\"fatal\"].sum()\n# drop the 0 values\nshooting_group = shooting_groupby[shooting_groupby.loc[:]!=0].dropna()\n\n\n# plot the circle graph with color intensity of fatal cases with different shooting wounds during 2015-2023\nalt.Chart(shooting_group).mark_circle().encode(\n    alt.X('year').bin(maxbins=35),\n    alt.Y('wound'),#.bin(),\n    size='fatal',\n    color='fatal:Q'\n).properties(width=600, height=500)\n\n\n\n\n\n\n\nvictim’s wound information and counts of these wounds lead to fatal shooting from year 2015-2023.\n\n# Setup a brush selection\nbrush = alt.selection_interval()\n\n# Plot the point plot first, set X Y and size, color\npoints = alt.Chart().mark_point().encode(\n    alt.X('year:Q').title('year').bin(),\n    alt.Y('age_diff:Q').title('age difference to mean age').bin(),\n    color=alt.condition(brush, 'year:N', alt.value('lightgray')),\n    size=alt.Size('age:Q')#.scale(range=[1, 60])\n).transform_joinaggregate(\n    age_max=\"max(age)\", age_min=\"min(age)\", age_mean=\"mean(age)\", age_median=\"median(age)\"\n).transform_calculate(\n    age_range=\"datum.age_max-datum.age_min\",\n    age_diff=\"datum.age-datum.age_mean\"\n).properties(\n    width=400,\n    height=400\n).add_params(\n    brush\n)\n\n# Then plot the bar chart\nbars = alt.Chart().mark_bar().encode(\n    x='count()',\n    y='dist:N',\n    color=alt.Color('dist:N'),#.scale(scale),\n# ).transform_calculate(\n#     \"count\", \"datum.inside+datum.outside\"\n).transform_filter(\n    brush\n).properties(\n    width=600\n)\n\n\nalt.vconcat(points, bars, data=shooting_5000)\n\n\n\n\n\n\n\nPoint chart shows the age difference (age minus mean age) in different years;\nBar chart shows the count of records of the distance of shooting (how far away being shoot)\n\n# Plot a line graph\n(\n    alt.Chart(shooting_5000.dropna())\n    .mark_line()\n    .encode(\n        x=alt.X(\"year:Q\", bin=alt.Bin(maxbins=10)), # Bin the data!\n        y=alt.Y(\n            \"mean(age):Q\", scale=alt.Scale(zero=False) # Mean of flipper length\n        ),  \n        color=\"race:N\",\n        tooltip=[\"mean(age):Q\", \"count():Q\"],\n    )\n    .properties(height=300, width=500)\n)\n\n\n\n\n\n\n\nThis line graph demonstrates the victim’s mean age and race, from year of 2015-2023.\nAsian victims have higher average age\n\n# Setup a brush selection\nbrush = alt.selection_interval()\n\n# The top scatterplot: flipper length vs bill length\npoints = (\n    alt.Chart()\n    .mark_point()\n    .encode(\n        x=alt.X(\"point_x:Q\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"point_y:Q\", scale=alt.Scale(zero=False)),\n        color=alt.condition(brush, \"race:N\", alt.value(\"lightgray\")),\n    )\n    .properties(width=800)\n    .add_params(brush)\n)\n\n# The bottom bar plot\nbars = (\n    alt.Chart()\n    .mark_bar()\n    .encode(\n        x=\"count(race):Q\",\n        y=\"race:N\",\n        color=\"race:N\",\n    )\n    .transform_filter(\n        brush  # NEW: the filter transform uses the selection to filter the input data to this chart\n    )\n    .properties(width=800)\n)\n\n# Final chart is a vertical stack\nchart = alt.vconcat(points, bars, data=shooting_5000)\n\n# Output the chart\nchart\n\n\n\n\n\n\n\nscatter plot shows coordinate x y and the race of victims. Some regions with really dense dots are the locations that frequently occuring shooting events.\nThe bar chart shows races of the victims by counts"
  },
  {
    "objectID": "MUSA550_Python/assignment-5.html",
    "href": "MUSA550_Python/assignment-5.html",
    "title": "MUSA550 - Assignment 5: Exploring Yelp Reviews in Philadelphia",
    "section": "",
    "text": "In this assignment, we’ll explore restaurant review data available through the Yelp Dataset Challenge. The dataset includes Yelp data for user reviews and business information for many metropolitan areas. I’ve already downloaded this dataset (8 GB total!) and extracted out the data files for reviews and restaurants in Philadelphia. I’ve placed these data files into the data directory in this repository.\nThis assignment is broken into two parts:\nPart 1: Analyzing correlations between restaurant reviews and census data\nWe’ll explore the relationship between restaurant reviews and the income levels of the restaurant’s surrounding area.\nPart 2: Exploring the impact of fast food restaurants\nWe’ll run a sentiment analysis on reviews of fast food restaurants and estimate income levels in neighborhoods with fast food restaurants. We’ll test how well our sentiment analysis works by comparing the number of stars to the sentiment of reviews.\nBackground readings - Does sentiment analysis work? - The Geography of Taste: Using Yelp to Study Urban Culture"
  },
  {
    "objectID": "MUSA550_Python/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "href": "MUSA550_Python/assignment-5.html#correlating-restaurant-ratings-and-income-levels",
    "title": "MUSA550 - Assignment 5: Exploring Yelp Reviews in Philadelphia",
    "section": "1. Correlating restaurant ratings and income levels",
    "text": "1. Correlating restaurant ratings and income levels\nIn this part, we’ll use the census API to download household income data and explore how it correlates with restaurant review data.\n\n1.1 Query the Census API\nUse the cenpy package to download median household income in the past 12 months by census tract from the 2021 ACS 5-year data set for your county of interest.\nYou have two options to find the correct variable names: - Search through: https://api.census.gov/data/2021/acs/acs5/variables.html - Initialize an API connection and use the .varslike() function to search for the proper keywords\nAt the end of this step, you should have a pandas DataFrame holding the income data for all census tracts within the county being analyzed. Feel free to rename your variable from the ACS so it has a more meaningful name!\n\n\n\n\n\n\nCaution\n\n\n\nSome census tracts won’t have any value because there are not enough households in that tract. The census will use a negative number as a default value for those tracts. You can safely remove those tracts from the analysis!\n\n\n\nimport cenpy\n\n\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nacs\n\nConnection to American Community Survey: 5-Year Estimates: Detailed Tables 5-Year (ID: https://api.census.gov/data/id/ACSDT5Y2021)\n\n\n\nacs.variables.head(n=10)\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nattributes\nrequired\n\n\n\n\nfor\nCensus API FIPS 'for' clause\nCensus API Geography Specification\nfips-for\nN/A\n0\nTrue\nNaN\nNaN\nNaN\n\n\nin\nCensus API FIPS 'in' clause\nCensus API Geography Specification\nfips-in\nN/A\n0\nTrue\nNaN\nNaN\nNaN\n\n\nucgid\nUniform Census Geography Identifier clause\nCensus API Geography Specification\nucgid\nN/A\n0\nTrue\nTrue\nNaN\nNaN\n\n\nB24022_060E\nEstimate!!Total:!!Female:!!Service occupations...\nSEX BY OCCUPATION AND MEDIAN EARNINGS IN THE P...\nint\nB24022\n0\nNaN\nNaN\nB24022_060EA,B24022_060M,B24022_060MA\nNaN\n\n\nB19001B_014E\nEstimate!!Total:!!$100,000 to $124,999\nHOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 202...\nint\nB19001B\n0\nNaN\nNaN\nB19001B_014EA,B19001B_014M,B19001B_014MA\nNaN\n\n\nB07007PR_019E\nEstimate!!Total:!!Moved from different municip...\nGEOGRAPHICAL MOBILITY IN THE PAST YEAR BY CITI...\nint\nB07007PR\n0\nNaN\nNaN\nB07007PR_019EA,B07007PR_019M,B07007PR_019MA\nNaN\n\n\nB19101A_004E\nEstimate!!Total:!!$15,000 to $19,999\nFAMILY INCOME IN THE PAST 12 MONTHS (IN 2021 I...\nint\nB19101A\n0\nNaN\nNaN\nB19101A_004EA,B19101A_004M,B19101A_004MA\nNaN\n\n\nB24022_061E\nEstimate!!Total:!!Female:!!Service occupations...\nSEX BY OCCUPATION AND MEDIAN EARNINGS IN THE P...\nint\nB24022\n0\nNaN\nNaN\nB24022_061EA,B24022_061M,B24022_061MA\nNaN\n\n\nB19001B_013E\nEstimate!!Total:!!$75,000 to $99,999\nHOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 202...\nint\nB19001B\n0\nNaN\nNaN\nB19001B_013EA,B19001B_013M,B19001B_013MA\nNaN\n\n\nB07007PR_018E\nEstimate!!Total:!!Moved from different municip...\nGEOGRAPHICAL MOBILITY IN THE PAST YEAR BY CITI...\nint\nB07007PR\n0\nNaN\nNaN\nB07007PR_018EA,B07007PR_018M,B07007PR_018MA\nNaN\n\n\n\n\n\n\n\n\nmedian_income = acs.varslike(\n    pattern=\"MEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS\",\n    by=\"concept\",  # searches along concept column\n).sort_index()\nmedian_income.sample(n=10)\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nattributes\nrequired\n\n\n\n\nB19019_004E\nEstimate!!Total:!!3-person households\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19019\n0\nNaN\nNaN\nB19019_004EA,B19019_004M,B19019_004MA\nNaN\n\n\nB22008_003E\nEstimate!!Median household income in the past ...\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB22008\n0\nNaN\nNaN\nB22008_003EA,B22008_003M,B22008_003MA\nNaN\n\n\nB19019_008E\nEstimate!!Total:!!7-or-more-person households\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19019\n0\nNaN\nNaN\nB19019_008EA,B19019_008M,B19019_008MA\nNaN\n\n\nB19019_003E\nEstimate!!Total:!!2-person households\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19019\n0\nNaN\nNaN\nB19019_003EA,B19019_003M,B19019_003MA\nNaN\n\n\nB22008_001E\nEstimate!!Median household income in the past ...\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB22008\n0\nNaN\nNaN\nB22008_001EA,B22008_001M,B22008_001MA\nNaN\n\n\nB19013_001E\nEstimate!!Median household income in the past ...\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19013\n0\nNaN\nNaN\nB19013_001EA,B19013_001M,B19013_001MA\nNaN\n\n\nB19049_004E\nEstimate!!Median household income in the past ...\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19049\n0\nNaN\nNaN\nB19049_004EA,B19049_004M,B19049_004MA\nNaN\n\n\nB19013D_001E\nEstimate!!Median household income in the past ...\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19013D\n0\nNaN\nNaN\nB19013D_001EA,B19013D_001M,B19013D_001MA\nNaN\n\n\nB19013E_001E\nEstimate!!Median household income in the past ...\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19013E\n0\nNaN\nNaN\nB19013E_001EA,B19013E_001M,B19013E_001MA\nNaN\n\n\nB19019_007E\nEstimate!!Total:!!6-person households\nMEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS ...\nint\nB19019\n0\nNaN\nNaN\nB19019_007EA,B19019_007M,B19019_007MA\nNaN\n\n\n\n\n\n\n\n\nacs.geographies['fips']\n\n\n\n\n\n\n\n\nname\ngeoLevelDisplay\nreferenceDate\nrequires\nwildcard\noptionalWithWCFor\n\n\n\n\n0\nus\n010\n2021-01-01\nNaN\nNaN\nNaN\n\n\n1\nregion\n020\n2021-01-01\nNaN\nNaN\nNaN\n\n\n2\ndivision\n030\n2021-01-01\nNaN\nNaN\nNaN\n\n\n3\nstate\n040\n2021-01-01\nNaN\nNaN\nNaN\n\n\n4\ncounty\n050\n2021-01-01\n[state]\n[state]\nstate\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n82\npublic use microdata area\n795\n2021-01-01\n[state]\n[state]\nstate\n\n\n83\nzip code tabulation area\n860\n2021-01-01\nNaN\nNaN\nNaN\n\n\n84\nschool district (elementary)\n950\n2021-01-01\n[state]\n[state]\nstate\n\n\n85\nschool district (secondary)\n960\n2021-01-01\n[state]\n[state]\nstate\n\n\n86\nschool district (unified)\n970\n2021-01-01\n[state]\n[state]\nstate\n\n\n\n\n87 rows × 6 columns\n\n\n\n\ncounties = cenpy.explorer.fips_table(\"COUNTY\")\ncounties.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nAL\n1\n1\nAutauga County\nH1\n\n\n1\nAL\n1\n3\nBaldwin County\nH1\n\n\n2\nAL\n1\n5\nBarbour County\nH1\n\n\n3\nAL\n1\n7\nBibb County\nH1\n\n\n4\nAL\n1\n9\nBlount County\nH1\n\n\n\n\n\n\n\n\n# Trim to just Philadelphia\n# Search for rows where name contains \"Philadelphia\"\ncounties.loc[ counties[3].str.contains(\"Philadelphia\") ]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n2294\nPA\n42\n101\nPhiladelphia County\nH6\n\n\n\n\n\n\n\n\nphilly_county_code = \"101\"\npa_state_code = \"42\"\n\n\nvariables = [\n    \"NAME\",\n    \"B19013_001E\" # Total\n]\n\n\nphilly_data = acs.query(\n    cols=variables,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": pa_state_code, \"county\": philly_county_code, \"tract\": \"*\"},\n)\n\n\nphilly_data.head()\n\n\n\n\n\n\n\n\nNAME\nB19013_001E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nBlock Group 1, Census Tract 1.01, Philadelphia...\n-666666666\n42\n101\n000101\n1\n\n\n1\nBlock Group 2, Census Tract 1.01, Philadelphia...\n-666666666\n42\n101\n000101\n2\n\n\n2\nBlock Group 3, Census Tract 1.01, Philadelphia...\n-666666666\n42\n101\n000101\n3\n\n\n3\nBlock Group 4, Census Tract 1.01, Philadelphia...\n97210\n42\n101\n000101\n4\n\n\n4\nBlock Group 5, Census Tract 1.01, Philadelphia...\n109269\n42\n101\n000101\n5\n\n\n\n\n\n\n\n\nfor variable in variables:\n    # Convert all variables EXCEPT for NAME\n    if variable != \"NAME\":\n        philly_data[variable] = philly_data[variable].astype(float)\n\n\nphilly_filtered = philly_data[philly_data['B19013_001E'] &gt;= 0]\n\n\n\n1.2 Download census tracts from the Census and merge the data from part 1.1\n\nDownload census tracts for the desired geography using the pygris package\nMerge the downloaded census tracts with the household income DataFrame\n\n\nimport pygris\n\n\nphilly_block_groups = pygris.block_groups(\n    state=pa_state_code, county=philly_county_code, year=2021\n)\n\n\nphilly_block_groups.head(n=10)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nBLKGRPCE\nGEOID\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n124\n42\n101\n989100\n2\n421019891002\nBlock Group 2\nG5030\nS\n373653\n7060\n+40.0373207\n-075.0177378\nPOLYGON ((-75.02195 40.03435, -75.02191 40.034...\n\n\n125\n42\n101\n989100\n4\n421019891004\nBlock Group 4\nG5030\nS\n1399991\n948653\n+40.0407424\n-074.9975532\nPOLYGON ((-75.01254 40.03803, -75.01195 40.038...\n\n\n126\n42\n101\n989100\n3\n421019891003\nBlock Group 3\nG5030\nS\n820212\n566882\n+40.0297287\n-075.0119000\nPOLYGON ((-75.02251 40.03083, -75.02176 40.031...\n\n\n156\n42\n101\n028300\n1\n421010283001\nBlock Group 1\nG5030\nS\n69892\n0\n+40.0299231\n-075.1408226\nPOLYGON ((-75.14202 40.02998, -75.14200 40.030...\n\n\n158\n42\n101\n028400\n1\n421010284001\nBlock Group 1\nG5030\nS\n94792\n0\n+40.0293965\n-075.1380180\nPOLYGON ((-75.13966 40.02974, -75.13930 40.031...\n\n\n192\n42\n101\n025700\n3\n421010257003\nBlock Group 3\nG5030\nS\n89392\n0\n+40.0717286\n-075.1919479\nPOLYGON ((-75.19437 40.07059, -75.19419 40.070...\n\n\n193\n42\n101\n001302\n4\n421010013024\nBlock Group 4\nG5030\nS\n396274\n60583\n+39.9436029\n-075.1858337\nPOLYGON ((-75.19371 39.94226, -75.19268 39.942...\n\n\n194\n42\n101\n020800\n1\n421010208001\nBlock Group 1\nG5030\nS\n512635\n0\n+40.0184109\n-075.1977311\nPOLYGON ((-75.20606 40.01483, -75.20601 40.014...\n\n\n195\n42\n101\n023600\n2\n421010236002\nBlock Group 2\nG5030\nS\n282169\n0\n+40.0470905\n-075.1953636\nPOLYGON ((-75.19941 40.04647, -75.19881 40.047...\n\n\n196\n42\n101\n010500\n1\n421010105001\nBlock Group 1\nG5030\nS\n277095\n0\n+39.9702909\n-075.2143967\nPOLYGON ((-75.21777 39.97263, -75.21633 39.972...\n\n\n\n\n\n\n\n\nphilly_final = philly_block_groups.merge(\n    philly_filtered,\n    left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\"],\n    right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n)\n\n\nphilly_final.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nBLKGRPCE\nGEOID\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\nNAME\nB19013_001E\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\n42\n101\n025700\n3\n421010257003\nBlock Group 3\nG5030\nS\n89392\n0\n+40.0717286\n-075.1919479\nPOLYGON ((-75.19437 40.07059, -75.19419 40.070...\nBlock Group 3, Census Tract 257, Philadelphia ...\n53171.0\n42\n101\n025700\n3\n\n\n1\n42\n101\n001302\n4\n421010013024\nBlock Group 4\nG5030\nS\n396274\n60583\n+39.9436029\n-075.1858337\nPOLYGON ((-75.19371 39.94226, -75.19268 39.942...\nBlock Group 4, Census Tract 13.02, Philadelphi...\n136813.0\n42\n101\n001302\n4\n\n\n2\n42\n101\n020800\n1\n421010208001\nBlock Group 1\nG5030\nS\n512635\n0\n+40.0184109\n-075.1977311\nPOLYGON ((-75.20606 40.01483, -75.20601 40.014...\nBlock Group 1, Census Tract 208, Philadelphia ...\n85333.0\n42\n101\n020800\n1\n\n\n3\n42\n101\n023600\n2\n421010236002\nBlock Group 2\nG5030\nS\n282169\n0\n+40.0470905\n-075.1953636\nPOLYGON ((-75.19941 40.04647, -75.19881 40.047...\nBlock Group 2, Census Tract 236, Philadelphia ...\n89167.0\n42\n101\n023600\n2\n\n\n4\n42\n101\n010500\n1\n421010105001\nBlock Group 1\nG5030\nS\n277095\n0\n+39.9702909\n-075.2143967\nPOLYGON ((-75.21777 39.97263, -75.21633 39.972...\nBlock Group 1, Census Tract 105, Philadelphia ...\n22705.0\n42\n101\n010500\n1\n\n\n\n\n\n\n\n\n# Rename columns\nphilly_final = philly_final.rename(\n    columns={\n        \"B19013_001E\": \"Total\"  }# Total\n)\n\n\nphilly_final.head()\n\n\n\n1.3 Load the restaurants data\nThe Yelp dataset includes data for 7,350 restaurants across the city. Load the data from the data/ folder and use the latitude and longitude columns to create a GeoDataFrame after loading the JSON data. Be sure to set the right CRS on when initializing the GeoDataFrame!\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\nimport pandas as pd\nimport geopandas as gpd\n\n\nrestaurant_data=pd.read_json(\"data/restaurants_philly.json.gz\", orient='record', lines=True)\n\n\nrestaurant_data.head()\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, B...\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Resta...\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\n\n\n\n\n\n\n\n\ngdata = gpd.GeoDataFrame(\n    restaurant_data,  # The pandas DataFrame\n    geometry=gpd.points_from_xy(restaurant_data[\"longitude\"], restaurant_data[\"latitude\"]), # The geometry!\n    crs=\"EPSG:4326\", # The CRS \n)\n\n\n\n1.4 Add tract info for each restaurant\nDo a spatial join to identify which census tract each restaurant is within. Make sure each dataframe has the same CRS!\nAt the end of this step, you should have a new dataframe with a column identifying the tract number for each restaurant.\n\njoined = gpd.sjoin(\n    gdata,  # The point data for 311 tickets\n    philly_block_groups.to_crs(gdata.crs),  # The neighborhoods (in the same CRS)\n    predicate=\"within\",\n    how=\"left\",\n)\njoined.head()\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry\nindex_right\nSTATEFP\n...\nTRACTCE\nBLKGRPCE\nGEOID\nNAMELSAD\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, B...\nPOINT (-75.15556 39.95551)\n6286.0\n42\n...\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923.0\n0.0\n+39.9553611\n-075.1560966\n\n\n1\nMUTTqe8uqyMdBl186RmNeA\n39.953949\n-75.143226\nTuna Bar\n245\n4.0\nSushi Bars, Restaurants, Japanese\nPOINT (-75.14323 39.95395)\n3404.0\n42\n...\n000102\n3\n421010001023\nBlock Group 3\nG5030\nS\n48303.0\n0.0\n+39.9533424\n-075.1439968\n\n\n2\nROeacJQwBeh05Rqg7F6TCg\n39.943223\n-75.162568\nBAP\n205\n4.5\nKorean, Restaurants\nPOINT (-75.16257 39.94322)\n3692.0\n42\n...\n001500\n2\n421010015002\nBlock Group 2\nG5030\nS\n166161.0\n0.0\n+39.9421477\n-075.1611428\n\n\n3\nQdN72BWoyFypdGJhhI5r7g\n39.939825\n-75.157447\nBar One\n65\n4.0\nCocktail Bars, Bars, Italian, Nightlife, Resta...\nPOINT (-75.15745 39.93982)\n5492.0\n42\n...\n001800\n1\n421010018001\nBlock Group 1\nG5030\nS\n147883.0\n0.0\n+39.9396209\n-075.1566638\n\n\n4\nMjboz24M9NlBeiOJKLEd_Q\n40.022466\n-75.218314\nDeSandro on Main\n41\n3.0\nPizza, Restaurants, Salad, Soup\nPOINT (-75.21831 40.02247)\n4526.0\n42\n...\n021000\n4\n421010210004\nBlock Group 4\nG5030\nS\n299302.0\n55718.0\n+40.0229216\n-075.2183537\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n1.5 Add income data to your restaurant data\nAdd the income data to your dataframe from the previous step, merging the census data based on the tract that each restaurant is within.\n\n# Do GeoDataFrame.merge(DataFrame) here...\nmerged_data = joined.merge(philly_final, left_on=\"TRACTCE\", right_on=\"tract\")\nmerged_data\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry_x\nindex_right\nSTATEFP_x\n...\nAWATER_y\nINTPTLAT_y\nINTPTLON_y\ngeometry_y\nNAME\nTotal\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, B...\nPOINT (-75.15556 39.95551)\n6286.0\n42\n...\n0\n+39.9549929\n-075.1527921\nPOLYGON ((-75.15451 39.95428, -75.15441 39.954...\nBlock Group 2, Census Tract 2, Philadelphia Co...\n250001.0\n42\n101\n000200\n2\n\n\n1\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, B...\nPOINT (-75.15556 39.95551)\n6286.0\n42\n...\n0\n+39.9553611\n-075.1560966\nPOLYGON ((-75.15811 39.95626, -75.15793 39.957...\nBlock Group 1, Census Tract 2, Philadelphia Co...\n42308.0\n42\n101\n000200\n1\n\n\n2\nL_sXNadtVHjxMw7Yhvkj9Q\n39.955454\n-75.154900\nNaoki Ramen\n92\n4.0\nRamen, Restaurants, Japanese\nPOINT (-75.15490 39.95545)\n6286.0\n42\n...\n0\n+39.9549929\n-075.1527921\nPOLYGON ((-75.15451 39.95428, -75.15441 39.954...\nBlock Group 2, Census Tract 2, Philadelphia Co...\n250001.0\n42\n101\n000200\n2\n\n\n3\nL_sXNadtVHjxMw7Yhvkj9Q\n39.955454\n-75.154900\nNaoki Ramen\n92\n4.0\nRamen, Restaurants, Japanese\nPOINT (-75.15490 39.95545)\n6286.0\n42\n...\n0\n+39.9553611\n-075.1560966\nPOLYGON ((-75.15811 39.95626, -75.15793 39.957...\nBlock Group 1, Census Tract 2, Philadelphia Co...\n42308.0\n42\n101\n000200\n1\n\n\n4\nicp_IKE9zIkAqAucyS1vTA\n39.955495\n-75.155256\nHakka Beef House\n33\n4.5\nRestaurants, Chinese\nPOINT (-75.15526 39.95549)\n6286.0\n42\n...\n0\n+39.9549929\n-075.1527921\nPOLYGON ((-75.15451 39.95428, -75.15441 39.954...\nBlock Group 2, Census Tract 2, Philadelphia Co...\n250001.0\n42\n101\n000200\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19370\ne0KzISSDK4b4unzirf3cYQ\n40.004687\n-75.178820\nMimmo's Pizza\n47\n2.0\nPizza, Restaurants\nPOINT (-75.17882 40.00469)\n1209.0\n42\n...\n0\n+40.0057567\n-075.1710769\nPOLYGON ((-75.17863 40.00505, -75.17843 40.005...\nBlock Group 3, Census Tract 171, Philadelphia ...\n22045.0\n42\n101\n017100\n3\n\n\n19371\ne0KzISSDK4b4unzirf3cYQ\n40.004687\n-75.178820\nMimmo's Pizza\n47\n2.0\nPizza, Restaurants\nPOINT (-75.17882 40.00469)\n1209.0\n42\n...\n0\n+40.0022881\n-075.1705064\nPOLYGON ((-75.17159 40.00238, -75.17125 40.003...\nBlock Group 2, Census Tract 171, Philadelphia ...\n26786.0\n42\n101\n017100\n2\n\n\n19372\nCgweZJAp5Fz-2XzjPE4Htw\n40.018455\n-75.097339\nWawa\n6\n2.0\nRestaurants, Food, Sandwiches, Delis, Convenie...\nPOINT (-75.09734 40.01845)\n10144.0\n42\n...\n0\n+40.0279627\n-075.0900097\nPOLYGON ((-75.09399 40.02704, -75.09351 40.027...\nBlock Group 3, Census Tract 292, Philadelphia ...\n63558.0\n42\n101\n029200\n3\n\n\n19373\nCgweZJAp5Fz-2XzjPE4Htw\n40.018455\n-75.097339\nWawa\n6\n2.0\nRestaurants, Food, Sandwiches, Delis, Convenie...\nPOINT (-75.09734 40.01845)\n10144.0\n42\n...\n3740\n+40.0266525\n-075.1091614\nPOLYGON ((-75.11288 40.02746, -75.11280 40.027...\nBlock Group 1, Census Tract 292, Philadelphia ...\n43894.0\n42\n101\n029200\n1\n\n\n19374\nCgweZJAp5Fz-2XzjPE4Htw\n40.018455\n-75.097339\nWawa\n6\n2.0\nRestaurants, Food, Sandwiches, Delis, Convenie...\nPOINT (-75.09734 40.01845)\n10144.0\n42\n...\n24704\n+40.0249035\n-075.1017831\nPOLYGON ((-75.11114 40.02330, -75.11066 40.023...\nBlock Group 2, Census Tract 292, Philadelphia ...\n113194.0\n42\n101\n029200\n2\n\n\n\n\n19375 rows × 40 columns\n\n\n\n\n\n1.6 Make a plot of median household income vs. Yelp stars\nOur dataset has the number of stars for each restaurant, rounded to the nearest 0.5 star. In this step, create a line plot that shows the average income value for each stars category (e.g., all restaurants with 1 star, 1.5 stars, 2 stars, etc.)\nWhile their are multiple ways to do this, the seaborn.lineplot() is a great option. This can show the average value in each category as well as 95% uncertainty intervals. Use this function to plot the stars (“x”) vs. average income (“y”) for all of our restaurants, using the dataframe from last step. Be sure to format your figure to make it look nice!\nQuestion: Is there a correlation between a restaurant’s ratings and the income levels of its surrounding neighborhood?\n\nimport seaborn as sns\n\n\n# Reset options to default\npd.reset_option('display.max_columns')\npd.reset_option('display.max_rows')\npd.reset_option('display.max_colwidth')\n\n\npd.set_option('display.max_columns', None)\n\n\nmerged_data.head()\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry_x\nindex_right\nSTATEFP_x\nCOUNTYFP_x\nTRACTCE_x\nBLKGRPCE_x\nGEOID_x\nNAMELSAD_x\nMTFCC_x\nFUNCSTAT_x\nALAND_x\nAWATER_x\nINTPTLAT_x\nINTPTLON_x\nSTATEFP_y\nCOUNTYFP_y\nTRACTCE_y\nBLKGRPCE_y\nGEOID_y\nNAMELSAD_y\nMTFCC_y\nFUNCSTAT_y\nALAND_y\nAWATER_y\nINTPTLAT_y\nINTPTLON_y\ngeometry_y\nNAME\nTotal\nstate\ncounty\ntract\nblock group\n\n\n\n\n0\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, B...\nPOINT (-75.15556 39.95551)\n6286.0\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923.0\n0.0\n+39.9553611\n-075.1560966\n42\n101\n000200\n2\n421010002002\nBlock Group 2\nG5030\nS\n108369\n0\n+39.9549929\n-075.1527921\nPOLYGON ((-75.15451 39.95428, -75.15441 39.954...\nBlock Group 2, Census Tract 2, Philadelphia Co...\n250001.0\n42\n101\n000200\n2\n\n\n1\nMTSW4McQd7CbVtyjqoe9mw\n39.955505\n-75.155564\nSt Honore Pastries\n80\n4.0\nRestaurants, Food, Bubble Tea, Coffee & Tea, B...\nPOINT (-75.15556 39.95551)\n6286.0\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923.0\n0.0\n+39.9553611\n-075.1560966\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923\n0\n+39.9553611\n-075.1560966\nPOLYGON ((-75.15811 39.95626, -75.15793 39.957...\nBlock Group 1, Census Tract 2, Philadelphia Co...\n42308.0\n42\n101\n000200\n1\n\n\n2\nL_sXNadtVHjxMw7Yhvkj9Q\n39.955454\n-75.154900\nNaoki Ramen\n92\n4.0\nRamen, Restaurants, Japanese\nPOINT (-75.15490 39.95545)\n6286.0\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923.0\n0.0\n+39.9553611\n-075.1560966\n42\n101\n000200\n2\n421010002002\nBlock Group 2\nG5030\nS\n108369\n0\n+39.9549929\n-075.1527921\nPOLYGON ((-75.15451 39.95428, -75.15441 39.954...\nBlock Group 2, Census Tract 2, Philadelphia Co...\n250001.0\n42\n101\n000200\n2\n\n\n3\nL_sXNadtVHjxMw7Yhvkj9Q\n39.955454\n-75.154900\nNaoki Ramen\n92\n4.0\nRamen, Restaurants, Japanese\nPOINT (-75.15490 39.95545)\n6286.0\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923.0\n0.0\n+39.9553611\n-075.1560966\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923\n0\n+39.9553611\n-075.1560966\nPOLYGON ((-75.15811 39.95626, -75.15793 39.957...\nBlock Group 1, Census Tract 2, Philadelphia Co...\n42308.0\n42\n101\n000200\n1\n\n\n4\nicp_IKE9zIkAqAucyS1vTA\n39.955495\n-75.155256\nHakka Beef House\n33\n4.5\nRestaurants, Chinese\nPOINT (-75.15526 39.95549)\n6286.0\n42\n101\n000200\n1\n421010002001\nBlock Group 1\nG5030\nS\n115923.0\n0.0\n+39.9553611\n-075.1560966\n42\n101\n000200\n2\n421010002002\nBlock Group 2\nG5030\nS\n108369\n0\n+39.9549929\n-075.1527921\nPOLYGON ((-75.15451 39.95428, -75.15441 39.954...\nBlock Group 2, Census Tract 2, Philadelphia Co...\n250001.0\n42\n101\n000200\n2\n\n\n\n\n\n\n\n\nmerged_data['Total'].mean()\n\n81058.04283870968\n\n\n\nsns.lineplot(\n    data=merged_data,\n    x='stars',\n    y='Total')\n\n&lt;Axes: xlabel='stars', ylabel='Total'&gt;"
  },
  {
    "objectID": "MUSA550_Python/assignment-5.html#fast-food-trends-in-philadelphia",
    "href": "MUSA550_Python/assignment-5.html#fast-food-trends-in-philadelphia",
    "title": "MUSA550 - Assignment 5: Exploring Yelp Reviews in Philadelphia",
    "section": "2. Fast food trends in Philadelphia",
    "text": "2. Fast food trends in Philadelphia\nAt the end of part 1, you should have seen a strong trend where higher income tracts generally had restaurants with better reviews. In this section, we’ll explore the impact of fast food restaurants and how they might be impacting this trend.\nHypothesis\n\nFast food restaurants are predominantly located in areas with lower median income levels.\nFast food restaurants have worse reviews compared to typical restaurants.\n\nIf true, these two hypotheses could help to explain the trend we found in part 1. Let’s dive in and test our hypotheses!\n\n2.1 Identify fast food restaurants\nThe “categories” column in our dataset contains multiple classifications for each restaurant. One such category is “Fast Food”. In this step, add a new column called “is_fast_food” that is True if the “categories” column contains the term “Fast Food” and False otherwise\n\nmerged_data.loc[merged_data['categories'] != 'Fast Food', 'is_fast_food'] = 'False'\nmerged_data.loc[merged_data['categories'].str.contains('Fast Food'), 'is_fast_food'] = 'True'\n\n\nmerged_data.sample(n=10)\n\n\n\n\n\n\n\n\nbusiness_id\nlatitude\nlongitude\nname\nreview_count\nstars\ncategories\ngeometry_x\nindex_right\nSTATEFP_x\nCOUNTYFP_x\nTRACTCE_x\nBLKGRPCE_x\nGEOID_x\nNAMELSAD_x\nMTFCC_x\nFUNCSTAT_x\nALAND_x\nAWATER_x\nINTPTLAT_x\nINTPTLON_x\nSTATEFP_y\nCOUNTYFP_y\nTRACTCE_y\nBLKGRPCE_y\nGEOID_y\nNAMELSAD_y\nMTFCC_y\nFUNCSTAT_y\nALAND_y\nAWATER_y\nINTPTLAT_y\nINTPTLON_y\ngeometry_y\nNAME\nTotal\nstate\ncounty\ntract\nblock group\nis_fast_food\n\n\n\n\n9886\nh9JsX6EAJ9TokfmoIbmWhQ\n39.946666\n-75.163993\nMisso\n14\n4.0\nRestaurants, Specialty Food, Sushi Bars, Ethni...\nPOINT (-75.16399 39.94667)\n1611.0\n42\n101\n001101\n1\n421010011011\nBlock Group 1\nG5030\nS\n36067.0\n0.0\n+39.9455460\n-075.1642618\n42\n101\n001101\n3\n421010011013\nBlock Group 3\nG5030\nS\n53345\n0\n+39.9453105\n-075.1609140\nPOLYGON ((-75.16228 39.94459, -75.16223 39.944...\nBlock Group 3, Census Tract 11.01, Philadelphi...\n39085.0\n42\n101\n001101\n3\nFalse\n\n\n2164\n8bXN8rouWRTKxvfuHNDRBw\n39.956272\n-75.168971\nTGI Fridays\n265\n2.0\nAmerican (Traditional), Bars, Restaurants, Nig...\nPOINT (-75.16897 39.95627)\n3410.0\n42\n101\n000300\n4\n421010003004\nBlock Group 4\nG5030\nS\n159485.0\n0.0\n+39.9570141\n-075.1700148\n42\n101\n000300\n4\n421010003004\nBlock Group 4\nG5030\nS\n159485\n0\n+39.9570141\n-075.1700148\nPOLYGON ((-75.17268 39.95611, -75.17258 39.956...\nBlock Group 4, Census Tract 3, Philadelphia Co...\n88194.0\n42\n101\n000300\n4\nFalse\n\n\n2883\n0v5gedzRunqlbgtj3u1QCQ\n39.953187\n-75.202974\nQDOBA Mexican Eats\n65\n3.5\nRestaurants, Fast Food, Event Planning & Servi...\nPOINT (-75.20297 39.95319)\n205.0\n42\n101\n008802\n5\n421010088025\nBlock Group 5\nG5030\nS\n70770.0\n0.0\n+39.9540229\n-075.2033541\n42\n101\n008802\n2\n421010088022\nBlock Group 2\nG5030\nS\n102603\n0\n+39.9559911\n-075.2021804\nPOLYGON ((-75.20477 39.95594, -75.20446 39.955...\nBlock Group 2, Census Tract 88.02, Philadelphi...\n16729.0\n42\n101\n008802\n2\nTrue\n\n\n10326\nQQGyPctWtd9WOZnMKUChUQ\n40.059745\n-75.190854\nMcMenamin's Tavern\n224\n4.0\nRestaurants, Pubs, American (New), Bars, Night...\nPOINT (-75.19085 40.05974)\n9478.0\n42\n101\n038800\n2\n421010388002\nBlock Group 2\nG5030\nS\n205950.0\n0.0\n+40.0572213\n-075.1943895\n42\n101\n038800\n1\n421010388001\nBlock Group 1\nG5030\nS\n790799\n4751\n+40.0602959\n-075.1987496\nPOLYGON ((-75.20477 40.05486, -75.20469 40.054...\nBlock Group 1, Census Tract 388, Philadelphia ...\n64798.0\n42\n101\n038800\n1\nFalse\n\n\n8523\nGVRTpquM8rSPe7x8XkmmrA\n39.949756\n-75.148062\nTaps Fill Station\n22\n4.5\nNightlife, Wine Bars, Cocktail Bars, Bars, Bee...\nPOINT (-75.14806 39.94976)\n397.0\n42\n101\n000101\n2\n421010001012\nBlock Group 2\nG5030\nS\n60664.0\n0.0\n+39.9499842\n-075.1500255\n42\n101\n000101\n4\n421010001014\nBlock Group 4\nG5030\nS\n130577\n0\n+39.9502981\n-075.1456484\nPOLYGON ((-75.14919 39.94926, -75.14911 39.949...\nBlock Group 4, Census Tract 1.01, Philadelphia...\n97210.0\n42\n101\n000101\n4\nFalse\n\n\n4451\nZpaf7JKJ7Z1F7bcpKnBBNg\n39.949858\n-75.160455\nMidtown\n5\n3.0\nRestaurants, American (Traditional), Bars, Nig...\nPOINT (-75.16045 39.94986)\n1997.0\n42\n101\n000600\n1\n421010006001\nBlock Group 1\nG5030\nS\n79423.0\n0.0\n+39.9497770\n-075.1614869\n42\n101\n000600\n2\n421010006002\nBlock Group 2\nG5030\nS\n93229\n0\n+39.9490501\n-075.1556507\nPOLYGON ((-75.15880 39.94944, -75.15863 39.950...\nBlock Group 2, Census Tract 6, Philadelphia Co...\n87697.0\n42\n101\n000600\n2\nFalse\n\n\n13013\nf4QAry-KSSkNT3fZ034NQw\n39.924954\n-75.175654\nDanny's Cafe\n10\n4.0\nFood, Restaurants, Cambodian, Coffee & Tea, Sa...\nPOINT (-75.17565 39.92495)\n1728.0\n42\n101\n003901\n4\n421010039014\nBlock Group 4\nG5030\nS\n112921.0\n0.0\n+39.9256027\n-075.1743917\n42\n101\n003901\n1\n421010039011\nBlock Group 1\nG5030\nS\n114949\n0\n+39.9251855\n-075.1712223\nPOLYGON ((-75.17307 39.92429, -75.17294 39.924...\nBlock Group 1, Census Tract 39.01, Philadelphi...\n52072.0\n42\n101\n003901\n1\nFalse\n\n\n18809\nmx31XU-QJIxObRGH_qvy3w\n39.978348\n-75.130836\nHAPP'S\n23\n4.5\nFood, Restaurants, Coffee & Tea, Sandwiches, B...\nPOINT (-75.13084 39.97835)\n1234.0\n42\n101\n015700\n3\n421010157003\nBlock Group 3\nG5030\nS\n305255.0\n0.0\n+39.9768522\n-075.1353407\n42\n101\n015700\n2\n421010157002\nBlock Group 2\nG5030\nS\n69048\n0\n+39.9788673\n-075.1352374\nPOLYGON ((-75.13666 39.97794, -75.13656 39.978...\nBlock Group 2, Census Tract 157, Philadelphia ...\n67891.0\n42\n101\n015700\n2\nFalse\n\n\n8412\n-rDhWLQJ28L2ttkaftdZNQ\n39.939024\n-75.173208\nMartabak Ok\n75\n4.5\nRestaurants, Bakeries, Food, Comfort Food, Asi...\nPOINT (-75.17321 39.93902)\n6409.0\n42\n101\n001900\n2\n421010019002\nBlock Group 2\nG5030\nS\n138246.0\n0.0\n+39.9401848\n-075.1753318\n42\n101\n001900\n1\n421010019001\nBlock Group 1\nG5030\nS\n65313\n0\n+39.9407635\n-075.1722668\nPOLYGON ((-75.17438 39.94053, -75.17427 39.941...\nBlock Group 1, Census Tract 19, Philadelphia C...\n139906.0\n42\n101\n001900\n1\nFalse\n\n\n5133\nmbglgx3nbRRPKxh0SxDjNQ\n39.959003\n-75.158230\nParada Maimon\n202\n4.5\nLatin American, Caribbean, Dominican, Mexican,...\nPOINT (-75.15823 39.95900)\n3429.0\n42\n101\n037600\n3\n421010376003\nBlock Group 3\nG5030\nS\n209087.0\n0.0\n+39.9589628\n-075.1586003\n42\n101\n037600\n3\n421010376003\nBlock Group 3\nG5030\nS\n209087\n0\n+39.9589628\n-075.1586003\nPOLYGON ((-75.16232 39.95793, -75.16229 39.958...\nBlock Group 3, Census Tract 376, Philadelphia ...\n130217.0\n42\n101\n037600\n3\nFalse\n\n\n\n\n\n\n\n\n\n2.2 Calculate the median income for fast food and otherwise\nGroup by the “is_fast_food” column and calculate the median income for restaurants that are and are not fast food. You should find that income levels are lower in tracts with fast food.\nNote: this is just an estimate, since we are calculating a median of median income values.\n\ngroupby_fast = merged_data.groupby(\"is_fast_food\").median()\n\ngroupby_fast\n\n/var/folders/8y/y89rxkzj5bv2c24kw9zjzy9c0000gn/T/ipykernel_2144/1530561349.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.median is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  groupby_fast = merged_data.groupby(\"is_fast_food\").median()\n\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nreview_count\nstars\nindex_right\nALAND_x\nAWATER_x\nALAND_y\nAWATER_y\nTotal\n\n\nis_fast_food\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\n39.955413\n-75.160578\n32.0\n4.0\n3390.0\n129277.0\n0.0\n115923.0\n0.0\n73279.0\n\n\nTrue\n39.959412\n-75.159020\n19.0\n2.5\n3367.0\n163390.0\n0.0\n128171.0\n0.0\n58047.0\n\n\n\n\n\n\n\n\n\n2.3 Load fast food review data\nIn the rest of part 2, we’re going to run a sentiment analysis on the reviews for fast food restaurants. The review data for all fast food restaurants identified in part 2.1 is already stored in the data/ folder. The data is stored as a JSON file and you can use pandas.read_json to load it.\nNotes\nThe JSON data is in a “records” format. To load it, you’ll need to pass the following keywords:\n\norient='records'\nlines=True\n\n\nreview_data = pd.read_json(\"data/reviews_philly_fast_food.json.gz\",orient='records',lines=True)\n\n\n\n2.4 Trim to the most popular fast food restaurants\nThere’s too many reviews to run a sentiment analysis on all of them in a reasonable time. Let’s trim our reviews dataset to the most popular fast food restaurants, using the list provided below.\nYou will need to get the “business_id” values for each of these restaurants from the restaurants data loaded in part 1.3. Then, trim the reviews data to include reviews only for those business IDs.\n\nreview_data.head(n=10)\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything ...\n\n\n1\nFKrP06TDAKtxNG1vrRQcQQ\n0IpFZoaY_RRNjha8Q_Wz6w\n2\nPerfect place to go if you like waiting 20 mi...\n\n\n2\nw9hS5x1F52Id-G1KTrAOZg\n0KlwfaHZyvao41_3S47dyg\n2\nWas not a fan of their cheesesteak. Their wiz ...\n\n\n3\nfr2qDm_mY1afIGMvqsKUCg\noKSUOq7pCQzyypFDSa1HoA\n3\nOk this is an aberration from my city foodie r...\n\n\n4\nfr2qDm_mY1afIGMvqsKUCg\n6SMUmb7Npwnq6AusxqOXzQ\n5\nMy family has been customers of George's for y...\n\n\n5\nQrIV69RPS4LTpIwPoL22_w\nLO1VbNOfA1FtTPnuNL9XLw\n4\ni think that this was good. i like applebees a...\n\n\n6\nFKrP06TDAKtxNG1vrRQcQQ\nChEe7cTIDhGSjOzQxXGRaw\n2\nThis place is really poor on the service. I do...\n\n\n7\nFKrP06TDAKtxNG1vrRQcQQ\n-02mXFOmtdjKXGYy_okLMw\n5\nOur server, Julliette, was very very nice to t...\n\n\n8\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock ...\n\n\n9\nQrIV69RPS4LTpIwPoL22_w\n6CehSaDzaZHetnnfP3QAPg\n1\nEvery dam time we come here it's an issue!!! E...\n\n\n\n\n\n\n\n\npopular_fast_food = [\n    \"McDonald's\",\n    \"Wendy's\",\n    \"Subway\",\n    \"Popeyes Louisiana Kitchen\",\n    \"Taco Bell\",\n    \"KFC\",\n    \"Burger King\",\n]\n\n\ntrim = restaurant_data.loc[restaurant_data[\"name\"].isin(popular_fast_food)]\nreview_trim = review_data.loc[review_data[\"business_id\"].isin(trim[\"business_id\"])]\nreview_trim.head()\n\n\n\n\n\n\n\n\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything ...\n\n\n8\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock ...\n\n\n13\nkgMEBZG6rjkGeFzPaIM4MQ\nNGaXI03qbtBLshjfJV4pbQ\n3\nDirty bathrooms and very slow service, but I w...\n\n\n17\nLACylKxImNI29DKUQpWuHw\nHHy9yIjW07VHUE6nXVbsVA\n3\nBurger King is an okay alternative to Mcdonald...\n\n\n21\ngq4zw-ru_rkZ2UBIanaZFQ\nyMZTK5B_0SAdUXSrIkXrmA\n1\nive tried going here four times with no succes...\n\n\n\n\n\n\n\n\nlen(review_trim)\n\n2823\n\n\n\n\n2.5 Run the emotions classifier on fast food reviews\nRun a sentiment analysis on the reviews data from the previous step. Use the DistilBERT model that can predict emotion labels (anger, fear, sadness, joy, love, and surprise). Transform the result from the classifier into a DataFrame so that you have a column for each of the emotion labels.\n\nfrom transformers import pipeline\n\n\n# Remove rows with missing descriptions\nstep_0 = review_trim.dropna(subset=[\"text\"])\nstep_05 = step_0.loc[step_0[\"text\"] != \"\"]\n\n# Strip out spaces and convert to a list\ndescriptions = step_05[\"text\"].str.strip().tolist()\n\n\n# The model\nmodel = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n\n# Initialize our sentiment analyzer\nemotion_classifier = pipeline(\n    task=\"text-classification\",  # The task we are doing\n    model=model,  # The specific model name\n    top_k=None,  # Predict all labels, not just top ones\n    tokenizer=model,  # Tokenize inputs using model tokenizer\n    truncation=True,  # Truncate text if we need to\n)\n\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n\n\n\n%%time \n\nemotion_scores = emotion_classifier(descriptions)\n\nCPU times: user 2min 38s, sys: 38.4 s, total: 3min 17s\nWall time: 1min 16s\n\n\n\nemotion_scores[0]\n\n[{'label': 'sadness', 'score': 0.7338715195655823},\n {'label': 'fear', 'score': 0.2506738603115082},\n {'label': 'anger', 'score': 0.011038964614272118},\n {'label': 'joy', 'score': 0.002758037531748414},\n {'label': 'surprise', 'score': 0.0010148948058485985},\n {'label': 'love', 'score': 0.000642782892100513}]\n\n\n\nemotion = pd.DataFrame(\n    [{d[\"label\"]: d[\"score\"] for d in dd} for dd in emotion_scores]\n).assign(text=descriptions)\n\n\nemotion_labels = [\"anger\", \"fear\", \"sadness\", \"joy\", \"surprise\", \"love\"]\n\n\n\n2.6 Identify the predicted emotion for each text\nUse the pandas idxmax() to identify the predicted emotion for each review, and add this value to a new column called “prediction”\nThe predicted emotion has the highest confidence score across all emotion labels for a particular label.\n\nemotion[emotion_labels].idxmax(axis=1)\n\n0       sadness\n1           joy\n2           joy\n3           joy\n4           joy\n         ...   \n2818       love\n2819      anger\n2820      anger\n2821      anger\n2822      anger\nLength: 2823, dtype: object\n\n\n\nemotion['prediction'] = emotion[emotion_labels].idxmax(axis=1)\n\n\nemotion.head()\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\nprediction\n\n\n\n\n0\n0.733872\n0.250674\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything ...\nsadness\n\n\n1\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock ...\njoy\n\n\n2\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I w...\njoy\n\n\n3\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonald...\njoy\n\n\n4\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no succes...\njoy\n\n\n\n\n\n\n\n\n\n2.7 Combine the ratings and sentiment data\nCombine the data from part 2.4 (reviews data) and part 2.6 (emotion data). Use the pd.concat() function and combine along the column axis.\nNote: You’ll need to reset the index of your reviews data frame so it matches the emotion data index (it should run from 0 to the length of the data - 1).\n\nreview_trim.reset_index(drop=True, inplace=True)\n\n\nconcat_data = pd.concat([emotion, review_trim], axis=1)\n\n\nconcat_data.head()\n\n\n\n\n\n\n\n\nsadness\nfear\nanger\njoy\nsurprise\nlove\ntext\nprediction\nbusiness_id\nreview_id\nstars\ntext\n\n\n\n\n0\n0.733872\n0.250674\n0.011039\n0.002758\n0.001015\n0.000643\nI know I shouldn't expect much but everything ...\nsadness\nkgMEBZG6rjkGeFzPaIM4MQ\nE-yGr1OhsUBxNeUVLDVouA\n1\nI know I shouldn't expect much but everything ...\n\n\n1\n0.000230\n0.000126\n0.000165\n0.998759\n0.000246\n0.000475\nI am only giving 5 stars because the Shamrock ...\njoy\nPjknD8uD_0tisZQbomiYoQ\n6TqKBa-HDiq2_W_ip2AItA\n5\nI am only giving 5 stars because the Shamrock ...\n\n\n2\n0.000216\n0.000088\n0.000153\n0.998563\n0.000161\n0.000819\nDirty bathrooms and very slow service, but I w...\njoy\nkgMEBZG6rjkGeFzPaIM4MQ\nNGaXI03qbtBLshjfJV4pbQ\n3\nDirty bathrooms and very slow service, but I w...\n\n\n3\n0.000838\n0.000403\n0.000811\n0.996928\n0.000140\n0.000880\nBurger King is an okay alternative to Mcdonald...\njoy\nLACylKxImNI29DKUQpWuHw\nHHy9yIjW07VHUE6nXVbsVA\n3\nBurger King is an okay alternative to Mcdonald...\n\n\n4\n0.005284\n0.000753\n0.006195\n0.985421\n0.001620\n0.000726\nive tried going here four times with no succes...\njoy\ngq4zw-ru_rkZ2UBIanaZFQ\nyMZTK5B_0SAdUXSrIkXrmA\n1\nive tried going here four times with no succes...\n\n\n\n\n\n\n\n\n\n2.8 Plot sentiment vs. stars\nWe now have a dataframe with the predicted primary emotion for each review and the associated number of stars for each review. Let’s explore two questions:\n\nDoes sentiment analysis work? Do reviews with fewer stars have negative emotions?\nFor our fast food restaurants, are reviews generally positive or negative?\n\nUse seaborn’s histplot() to make a stacked bar chart showing the breakdown of each emotion for each stars category (1 star, 2 stars, etc.). A few notes:\n\nTo stack multiple emotion labels in one bar, use the multiple=\"stack\" keyword\nThe discrete=True can be helpful to tell seaborn our stars values are discrete categories\n\n\nsns.histplot(data=concat_data, x='stars', hue=\"prediction\", multiple=\"stack\", discrete=True, palette=\"Spectral\")\n\n&lt;Axes: xlabel='stars', ylabel='Count'&gt;\n\n\n\n\n\nQuestion: What does your chart indicate for the effectiveness of our sentiment analysis? Does our original hypothesis about fast food restaurants seem plausible?\n\n\nThis chart indicates that our sentiment analysis is effective in terms of\n\n\npredicting the attitude of the reviews, whereas the 1-star reviews desmonstrate\n\n\nbunch of anger and sadness, where as 5-star reviews show greater joy.\n\n\nAs for our hypothesis about fast food restaurants, it is true that reviews are\n\n\ngenerally negative."
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html",
    "href": "MUSA550_Python/assignment-3-HangZhao.html",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\n# See lots of columns\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#assignment-3",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#assignment-3",
    "title": "MUSA550 Geospatial Data Science in Python",
    "section": "",
    "text": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\n# See lots of columns\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#part-1-exploring-evictions-and-code-violations-in-philadelphia",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "Part 1: Exploring Evictions and Code Violations in Philadelphia",
    "text": "Part 1: Exploring Evictions and Code Violations in Philadelphia\nIn this assignment, we’ll explore spatial trends evictions in Philadelphia using data from the Eviction Lab and building code violations using data from OpenDataPhilly.\nWe’ll be exploring the idea that evictions can occur as retaliation against renters for reporting code violations. Spatial correlations between evictions and code violations from the City’s Licenses and Inspections department can offer some insight into this question.\nA couple of interesting background readings: - HuffPost article - PlanPhilly article"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#explore-eviction-lab-data",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#explore-eviction-lab-data",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "1.1 Explore Eviction Lab Data",
    "text": "1.1 Explore Eviction Lab Data\nThe Eviction Lab built the first national database for evictions. If you aren’t familiar with the project, you can explore their website: https://evictionlab.org/\n\n1.1.1 Read data using geopandas\nThe first step is to read the eviction data by census tract using geopandas. The data for all of Pennsylvania by census tract is available in the data/ folder in a GeoJSON format.\nLoad the data file “PA-tracts.geojson” using geopandas\nNote: If you’d like to see all columns in the data frame, you can increase the max number of columns using pandas display options:\n\nPA_Census = gpd.read_file(\"data/PA-tracts.geojson\")\n\n\n\n1.1.2 Explore and trim the data\nWe will need to trim data to Philadelphia only. Take a look at the data dictionary for the descriptions of the various columns in top-level repository folder: eviction_lab_data_dictionary.txt\nNote: the column names are shortened — see the end of the above file for the abbreviations. The numbers at the end of the columns indicate the years. For example, e-16 is the number of evictions in 2016.\nTake a look at the individual columns and trim to census tracts in Philadelphia. (Hint: Philadelphia is both a city and a county).\n\nphilly = PA_Census.loc[PA_Census['pl'] == 'Philadelphia County, Pennsylvania']\n\n\n\n1.1.3 Transform from wide to tidy format\nFor this assignment, we are interested in the number of evictions by census tract for various years. Right now, each year has it’s own column, so it will be easiest to transform to a tidy format.\nUse the pd.melt() function to transform the eviction data into tidy format, using the number of evictions from 2003 to 2016.\nThe tidy data frame should have four columns: GEOID, geometry, a column holding the number of evictions, and a column telling you what the name of the original column was for that value.\nHints: - You’ll want to specify the GEOID and geometry columns as the id_vars. This will keep track of the census tract information. - You should specify the names of the columns holding the number of evictions as the value_vars. - You can generate a list of this column names using Python’s f-string formatting: python     value_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\n\nvalue_vars = [f\"e-{x:02d}\" for x in range(3, 17)]\nphilly = philly.melt(id_vars=['GEOID','geometry'], value_vars=value_vars, var_name='year', value_name='evictions')\n\n\nphilly.head(n=10)\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nyear\nevictions\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-75.14161 39.95549, -75.14163 ...\ne-03\n21.0\n\n\n1\n42101000200\nMULTIPOLYGON (((-75.15122 39.95686, -75.15167 ...\ne-03\n3.0\n\n\n2\n42101000300\nMULTIPOLYGON (((-75.16234 39.95782, -75.16237 ...\ne-03\n17.0\n\n\n3\n42101000801\nMULTIPOLYGON (((-75.17732 39.95096, -75.17784 ...\ne-03\n13.0\n\n\n4\n42101000804\nMULTIPOLYGON (((-75.17118 39.94778, -75.17102 ...\ne-03\n21.0\n\n\n5\n42101001002\nMULTIPOLYGON (((-75.14919 39.94903, -75.14602 ...\ne-03\n16.0\n\n\n6\n42101001400\nMULTIPOLYGON (((-75.16558 39.94366, -75.16567 ...\ne-03\n30.0\n\n\n7\n42101002100\nMULTIPOLYGON (((-75.18062 39.93582, -75.17984 ...\ne-03\n8.0\n\n\n8\n42101018800\nMULTIPOLYGON (((-75.10971 39.99233, -75.11026 ...\ne-03\n75.0\n\n\n9\n42101019000\nMULTIPOLYGON (((-75.09704 40.01602, -75.09656 ...\ne-03\n43.0\n\n\n\n\n\n\n\n\n\n1.1.4 Plot the total number of evictions per year from 2003 to 2016\nUse hvplot to plot the total number of evictions from 2003 to 2016. You will first need to perform a group by operation and sum up the total number of evictions for all census tracts, and then use hvplot() to make your plot.\nYou can use any type of hvplot chart you’d like to show the trend in number of evictions over time.\n\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension(\"bokeh\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nphilly_grouped = philly.groupby(\"year\", as_index=False)[\"evictions\"].sum()\nphilly_grouped\n\n\n\n\n\n\n\n\nyear\nevictions\n\n\n\n\n0\ne-03\n10647.0\n\n\n1\ne-04\n10491.0\n\n\n2\ne-05\n10550.0\n\n\n3\ne-06\n11078.0\n\n\n4\ne-07\n11032.0\n\n\n5\ne-08\n10866.0\n\n\n6\ne-09\n9821.0\n\n\n7\ne-10\n10628.0\n\n\n8\ne-11\n10882.0\n\n\n9\ne-12\n11130.0\n\n\n10\ne-13\n10803.0\n\n\n11\ne-14\n11182.0\n\n\n12\ne-15\n10098.0\n\n\n13\ne-16\n10264.0\n\n\n\n\n\n\n\n\nphilly_grouped.hvplot(x='year', y='evictions', kind=\"line\")\n\n\n\n\n\n  \n\n\n\n\n\n\n1.1.5 The number of evictions across Philadelphia\nOur tidy data frame is still a GeoDataFrame with a geometry column, so we can visualize the number of evictions for all census tracts.\nUse hvplot() to generate a choropleth showing the number of evictions for a specified year, with a widget dropdown to select a given year (or variable name, e.g., e-16, e-15, etc).\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select between them. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\n# Via hvplot\n\nchoro = philly.to_crs(epsg=3857).hvplot(\n    c=\"evictions\",\n    frame_width=600,\n    frame_height=600,\n    groupby='year',\n    alpha=0.7,\n    geo=True,\n    crs=3857,\n    cmap=\"inferno\",\n)\nchoro\n# gvts.EsriImagery * choro"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#code-violations-in-philadelphia",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#code-violations-in-philadelphia",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "1.2 Code Violations in Philadelphia",
    "text": "1.2 Code Violations in Philadelphia\nNext, we’ll explore data for code violations from the Licenses and Inspections Department of Philadelphia to look for potential correlations with the number of evictions.\n\n1.2.1 Load data from 2012 to 2016\nL+I violation data for years including 2012 through 2016 (inclusive) is provided in a CSV format in the “data/” folder.\nLoad the data using pandas and convert to a GeoDataFrame.\n\n# Load the data from a CSV file into a pandas DataFrame\nviolation_df = pd.read_csv(\n    \"data/li_violations.csv\"  # Use the file path relative to the current folder\n)\n\n# Remove rows with missing geometry\nviolation_df = violation_df.dropna(subset=[\"lat\", \"lng\"])\n\n# Create our GeoDataFrame with geometry column created from lon/lat\nviolation = gpd.GeoDataFrame(\n    violation_df,\n    geometry=gpd.points_from_xy(violation_df[\"lng\"], violation_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nviolation_0 = gpd.GeoDataFrame(\n    violation_df, geometry=gpd.points_from_xy(violation_df.lng, violation_df.lat), crs=\"EPSG:4326\"\n)\nviolation_0\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nPOINT (-75.12608 40.05053)\n\n\n1\n40.050593\n-75.126578\nLICENSE-CHANGE OF ADDRESS\nPOINT (-75.12658 40.05059)\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-75.12658 40.05059)\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nPOINT (-75.12889 39.99199)\n\n\n4\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-75.16485 40.02326)\n\n\n...\n...\n...\n...\n...\n\n\n434047\n40.012805\n-75.155963\nSD-REQD EXIST GROUP R\nPOINT (-75.15596 40.01281)\n\n\n434048\n40.009985\n-75.068968\nRUBBISH/GARBAGE EXTERIOR-OWNER\nPOINT (-75.06897 40.00999)\n\n\n434049\n40.009829\n-75.068912\nCLIP VIOLATION NOTICE\nPOINT (-75.06891 40.00983)\n\n\n434050\n40.009776\n-75.068895\nPERSONAL PROPERTY EXT OWNER\nPOINT (-75.06889 40.00978)\n\n\n434051\n40.009776\n-75.068895\nLICENSE - RENTAL PROPERTY\nPOINT (-75.06889 40.00978)\n\n\n\n\n434052 rows × 4 columns\n\n\n\n\n\n1.2.2 Trim to specific violation types\nThere are many different types of code violations (running the nunique() function on the violationdescription column will extract all of the unique ones). More information on different types of violations can be found on the City’s website.\nBelow, I’ve selected 15 types of violations that deal with property maintenance and licensing issues. We’ll focus on these violations. The goal is to see if these kinds of violations are correlated spatially with the number of evictions in a given area.\nUse the list of violations given to trim your data set to only include these types.\n\nviolation['violationdescription'].nunique()\n\n1342\n\n\n\nviolation_types = [\n    \"INT-PLMBG MAINT FIXTURES-RES\",\n    \"INT S-CEILING REPAIR/MAINT SAN\",\n    \"PLUMBING SYSTEMS-GENERAL\",\n    \"CO DETECTOR NEEDED\",\n    \"INTERIOR SURFACES\",\n    \"EXT S-ROOF REPAIR\",\n    \"ELEC-RECEPTABLE DEFECTIVE-RES\",\n    \"INT S-FLOOR REPAIR\",\n    \"DRAINAGE-MAIN DRAIN REPAIR-RES\",\n    \"DRAINAGE-DOWNSPOUT REPR/REPLC\",\n    \"LIGHT FIXTURE DEFECTIVE-RES\",\n    \"LICENSE-RES SFD/2FD\",\n    \"ELECTRICAL -HAZARD\",\n    \"VACANT PROPERTIES-GENERAL\",\n    \"INT-PLMBG FIXTURES-RES\",\n]\nviolation_sel = violation_0['violationdescription'].isin(violation_types)\nviolation_15 = violation_0.loc[violation_sel]\n\n\n\n1.2.3 Make a hex bin map\nThe code violation data is point data. We can get a quick look at the geographic distribution using matplotlib and the hexbin() function. Make a hex bin map of the code violations and overlay the census tract outlines.\nHints: - The eviction data from part 1 was by census tract, so the census tract geometries are available as part of that GeoDataFrame. You can use it to overlay the census tracts on your hex bin map. - Make sure you convert your GeoDataFrame to a CRS that’s better for visualization than plain old 4326.\n\nviolation_3857 = violation_15.to_crs(epsg=3857)\n\n\nfrom matplotlib import pyplot as plt\n\n\n# Create the axes\nfig, ax = plt.subplots(figsize=(12, 12))\n\n# Extract out the x/y coordindates of the Point objects\nxcoords = violation_3857.geometry.x\nycoords = violation_3857.geometry.y\n\n# Plot a hexbin chart\n# NOTE: We are passing the full set of coordinates to matplotlib\nhex_vals = ax.hexbin(xcoords, ycoords, gridsize=50)\n\n\n# Add the philly census tract geometry boundaries\nphilly.to_crs(violation_3857.crs).plot(\n    ax=ax, facecolor=\"none\", edgecolor=\"white\", linewidth=0.25\n)\n\n# Add a colorbar and format\nfig.colorbar(hex_vals, ax=ax)\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n1.2.4 Spatially join data sets\nTo do a census tract comparison to our eviction data, we need to find which census tract each of the code violations falls into. Use the geopandas.sjoin() function to do just that.\nHints - You can re-use your eviction data frame, but you will only need the geometry column (specifying census tract polygons) and the GEOID column (specifying the name of each census tract). - Make sure both data frames have the same CRS before joining them together!\n\nviolation_tract = gpd.sjoin(\n    violation_3857,  \n    philly.to_crs(violation_3857.crs),  \n    predicate=\"intersects\",\n    how=\"inner\",\n).drop(columns=[\"index_right\", \"lat\", \"evictions\", \"lng\", \"year\"])\nviolation_tract.head()\n\n\n\n\n\n\n\n\nviolationdescription\ngeometry\nGEOID\n\n\n\n\n2\nLICENSE-RES SFD/2FD\nPOINT (-8363052.408 4873297.042)\n42101027100\n\n\n16024\nCO DETECTOR NEEDED\nPOINT (-8363202.689 4872597.863)\n42101027100\n\n\n17225\nLICENSE-RES SFD/2FD\nPOINT (-8363040.274 4872575.906)\n42101027100\n\n\n43436\nINT S-CEILING REPAIR/MAINT SAN\nPOINT (-8362971.479 4872591.029)\n42101027100\n\n\n43452\nEXT S-ROOF REPAIR\nPOINT (-8362970.588 4872596.991)\n42101027100\n\n\n\n\n\n\n\n\n\n1.2.5 Calculate the number of violations by type per census tract\nNext, we’ll want to find the number of violations (for each kind) per census tract. You should group the data frame by violation type and census tract name.\nThe result of this step should be a data frame with three columns: violationdescription, GEOID, and N, where N is the number of violations of that kind in the specified census tract.\nOptional: to make prettier plots\nSome census tracts won’t have any violations, and they won’t be included when we do the above calculation. However, there is a trick to set the values for those census tracts to be zero. After you calculate the sizes of each violation/census tract group, you can run:\nN = N.unstack(fill_value=0).stack().reset_index(name='N')\nwhere N gives the total size of each of the groups, specified by violation type and census tract name.\nSee this StackOverflow post for more details.\nThis part is optional, but will make the resulting maps a bit prettier.\n\nNviolation_tract = violation_tract.groupby([\"violationdescription\",\"GEOID\"]).size()\nNviolation_tract = Nviolation_tract.unstack(fill_value=0).stack().reset_index(name='N')\n\n\n#merged[merged['GEOID']=='42101980800']\n\n\n\n1.2.6 Merge with census tracts geometries\nWe now have the number of violations of different types per census tract specified as a regular DataFrame. You can now merge it with the census tract geometries (from your eviction data GeoDataFrame) to create a GeoDataFrame.\nHints - Use pandas.merge() and specify the on keyword to be the column holding census tract names. - Make sure the result of the merge operation is a GeoDataFrame — you will want the GeoDataFrame holding census tract geometries to be the first argument of the pandas.merge() function.\n\n# Do GeoDataFrame.merge(DataFrame) here...\nphilly_projected = philly.to_crs(violation_3857.crs)\nphilly_projected=philly_projected[['GEOID','geometry']].drop_duplicates()\nmerged = philly_projected.merge(Nviolation_tract, on=\"GEOID\")\nmerged\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nviolationdescription\nN\n\n\n\n\n0\n42101000100\nMULTIPOLYGON (((-8364725.429 4859476.459, -836...\nCO DETECTOR NEEDED\n0\n\n\n1\n42101000100\nMULTIPOLYGON (((-8364725.429 4859476.459, -836...\nDRAINAGE-DOWNSPOUT REPR/REPLC\n84\n\n\n2\n42101000100\nMULTIPOLYGON (((-8364725.429 4859476.459, -836...\nDRAINAGE-MAIN DRAIN REPAIR-RES\n0\n\n\n3\n42101000100\nMULTIPOLYGON (((-8364725.429 4859476.459, -836...\nELEC-RECEPTABLE DEFECTIVE-RES\n0\n\n\n4\n42101000100\nMULTIPOLYGON (((-8364725.429 4859476.459, -836...\nELECTRICAL -HAZARD\n14\n\n\n...\n...\n...\n...\n...\n\n\n5530\n42101018400\nMULTIPOLYGON (((-8355531.552 4864854.203, -835...\nINTERIOR SURFACES\n28\n\n\n5531\n42101018400\nMULTIPOLYGON (((-8355531.552 4864854.203, -835...\nLICENSE-RES SFD/2FD\n154\n\n\n5532\n42101018400\nMULTIPOLYGON (((-8355531.552 4864854.203, -835...\nLIGHT FIXTURE DEFECTIVE-RES\n0\n\n\n5533\n42101018400\nMULTIPOLYGON (((-8355531.552 4864854.203, -835...\nPLUMBING SYSTEMS-GENERAL\n28\n\n\n5534\n42101018400\nMULTIPOLYGON (((-8355531.552 4864854.203, -835...\nVACANT PROPERTIES-GENERAL\n0\n\n\n\n\n5535 rows × 4 columns\n\n\n\n\n\n1.2.7 Interactive choropleths for each violation type\nNow, we can use hvplot() to create an interactive choropleth for each violation type and add a widget to specify different violation types.\nHints - You’ll need to use the groupby keyword to tell hvplot to make a series of maps, with a widget to select different violation types. - You will need to specify dynamic=False as a keyword argument to the hvplot() function. - Be sure to specify a width and height that makes your output map (roughly) square to limit distortions\n\n# Via hvplot\n\nchoro2 = merged.to_crs(epsg=3857).hvplot(\n    c=\"N\",\n    frame_width=600,\n    frame_height=600,\n    groupby='violationdescription',\n    alpha=0.7,\n    geo=True,\n    crs=3857,\n    cmap=\"magma\",\n    #hover_cols=[\"GEOID\"]\n)\nchoro2"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#a-side-by-side-comparison",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#a-side-by-side-comparison",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "1.3. A side-by-side comparison",
    "text": "1.3. A side-by-side comparison\nFrom the interactive maps of evictions and violations, you should notice a lot of spatial overlap.\nAs a final step, we’ll make a side-by-side comparison to better show the spatial correlations. This will involve a few steps:\n\nTrim the evictions data frame plotted in section 1.1.5 to only include evictions from 2016.\nTrim the L+I violations data frame plotted in section 1.2.7 to only include a single violation type (pick whichever one you want!).\nUse hvplot() to make two interactive choropleth maps, one for the data from step 1. and one for the data in step 2.\nShow these two plots side by side (one row and 2 columns) using the syntax for combining charts.\n\nNote: since we selected a single year and violation type, you won’t need to use the groupby= keyword here.\n\neviction_e16 = philly[\"year\"].isin([\"e-16\"])\nphilly_trim16 = philly.loc[eviction_e16]\nvacant_properties = merged[\"violationdescription\"].isin([\"INT S-FLOOR REPAIR\"])\nmerged_trim = merged.loc[vacant_properties]\n\n\nphilly_trim16.to_crs(epsg=3857).hvplot(\n    c=\"evictions\",\n    frame_width=600,\n    frame_height=600,\n    alpha=0.7,\n    geo=True,\n    crs=3857,\n    cmap=\"inferno\"\n) + merged_trim.to_crs(epsg=3857).hvplot(\n    c=\"N\",\n    frame_width=600,\n    frame_height=600,\n    alpha=0.7,\n    geo=True,\n    crs=3857,\n    cmap=\"inferno\"\n)"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#extra-credit",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#extra-credit",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "1.4. Extra Credit",
    "text": "1.4. Extra Credit\nIdentify the 20 most common types of violations within the time period of 2012 to 2016 and create a set of interactive choropleths similar to what was done in section 1.2.7.\nUse this set of maps to identify 3 types of violations that don’t seem to have much spatial overlap with the number of evictions in the City.\n\nviolation_total = violation_0.groupby([\"violationdescription\"], as_index=False).size()\nsort = violation_total.sort_values(by='size', ascending=False)\nselected_column = sort.head(n=20)\nviolation_selected = violation_0['violationdescription'].isin(selected_column['violationdescription'])\nviolation_20 = violation_0.loc[violation_selected]\nviolation_20.to_crs(epsg=3857)\n\n\n\n\n\n\n\n\nlat\nlng\nviolationdescription\ngeometry\n\n\n\n\n0\n40.050526\n-75.126076\nCLIP VIOLATION NOTICE\nPOINT (-8362996.526 4873287.299)\n\n\n2\n40.050593\n-75.126578\nLICENSE-RES SFD/2FD\nPOINT (-8363052.408 4873297.042)\n\n\n3\n39.991994\n-75.128895\nEXT A-CLEAN WEEDS/PLANTS\nPOINT (-8363310.335 4864778.938)\n\n\n4\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-8367312.605 4869322.935)\n\n\n5\n40.023260\n-75.164848\nEXT A-VACANT LOT CLEAN/MAINTAI\nPOINT (-8367312.605 4869322.935)\n\n\n...\n...\n...\n...\n...\n\n\n434044\n39.936179\n-75.192078\nSD-REQD EXIST GROUP R\nPOINT (-8370343.835 4856672.315)\n\n\n434047\n40.012805\n-75.155963\nSD-REQD EXIST GROUP R\nPOINT (-8366323.531 4867803.242)\n\n\n434048\n40.009985\n-75.068968\nRUBBISH/GARBAGE EXTERIOR-OWNER\nPOINT (-8356639.292 4867393.379)\n\n\n434049\n40.009829\n-75.068912\nCLIP VIOLATION NOTICE\nPOINT (-8356633.058 4867370.706)\n\n\n434051\n40.009776\n-75.068895\nLICENSE - RENTAL PROPERTY\nPOINT (-8356631.166 4867363.003)\n\n\n\n\n235542 rows × 4 columns\n\n\n\n\nviolation_join = gpd.sjoin(\n    violation_20,  \n    philly.to_crs(violation_20.crs),  \n    predicate=\"within\",\n    how=\"left\",\n).drop(columns=[\"index_right\", \"lat\", \"evictions\", \"lng\", \"year\"])\n\n\nviolation_size = violation_join.groupby([\"violationdescription\",\"GEOID\"]).size()\nviolation_size = violation_size.unstack(fill_value=0).stack().reset_index(name='N')\nviolation_size\n\n\n\n\n\n\n\n\nviolationdescription\nGEOID\nN\n\n\n\n\n0\nANNUAL CERT FIRE ALARM\n42101000100\n770\n\n\n1\nANNUAL CERT FIRE ALARM\n42101000200\n532\n\n\n2\nANNUAL CERT FIRE ALARM\n42101000300\n378\n\n\n3\nANNUAL CERT FIRE ALARM\n42101000401\n168\n\n\n4\nANNUAL CERT FIRE ALARM\n42101000402\n462\n\n\n...\n...\n...\n...\n\n\n7655\nVIOL C&I MESSAGE\n42101980600\n42\n\n\n7656\nVIOL C&I MESSAGE\n42101980700\n658\n\n\n7657\nVIOL C&I MESSAGE\n42101980800\n14\n\n\n7658\nVIOL C&I MESSAGE\n42101980900\n294\n\n\n7659\nVIOL C&I MESSAGE\n42101989100\n448\n\n\n\n\n7660 rows × 3 columns\n\n\n\n\nvp_merged = philly_projected.merge(violation_size, on=\"GEOID\")\n\n\nchoro_total = vp_merged.to_crs(epsg=3857).hvplot(\n    c=\"N\",\n    frame_width=600,\n    frame_height=600,\n    groupby='violationdescription',\n    alpha=0.7,\n    geo=True,\n    crs=3857,\n    cmap=\"magma\",\n    #hover_cols=[\"GEOID\"]\n)\nchoro_total\n\n\n\n\n\n  \n\n\n\n\n\nprint(\"3 types of violations that don’t seem to have much spatial overlap with the number of evictions in the City:\", \n      \"\\nVIOL C&I MESSAGE\",\n      \"\\nRUBBISH/GARBAGE EXTERIOR-OWNER\",\n      \"\\nLICENSE-RES GENERAL\")\n\n3 types of violations that don’t seem to have much spatial overlap with the number of evictions in the City: \nVIOL C&I MESSAGE \nRUBBISH/GARBAGE EXTERIOR-OWNER \nLICENSE-RES GENERAL"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#part-2-exploring-the-ndvi-in-philadelphia",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#part-2-exploring-the-ndvi-in-philadelphia",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "Part 2: Exploring the NDVI in Philadelphia",
    "text": "Part 2: Exploring the NDVI in Philadelphia\nIn this part, we’ll explore the NDVI in Philadelphia a bit more. This part will include two parts:\n\nWe’ll compare the median NDVI within the city limits and the immediate suburbs\nWe’ll calculate the NDVI around street trees in the city."
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#comparing-the-ndvi-in-the-city-and-the-suburbs",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "2.1 Comparing the NDVI in the city and the suburbs",
    "text": "2.1 Comparing the NDVI in the city and the suburbs\n\n2.1.1 Load Landsat data for Philadelphia\nUse rasterio to load the landsat data for Philadelphia (available in the “data/” folder)\n\nimport rasterio as rio\n\n\n# Open the file and get a file \"handle\"\nlandsat = rio.open(\"./data/landsat8_philly.tif\")\n\nlandsat\n\n&lt;open DatasetReader name='./data/landsat8_philly.tif' mode='r'&gt;\n\n\n\n# The CRS\nlandsat.crs\n\nCRS.from_epsg(32618)\n\n\n\n\n2.1.2 Separating the city from the suburbs\nCreate two polygon objects, one for the city limits and one for the suburbs. To calculate the suburbs polygon, we will take everything outside the city limits but still within the bounding box.\n\nThe city limits are available in the “data/” folder.\nTo calculate the suburbs polygon, the “envelope” attribute of the city limits geometry will be useful.\nYou can use geopandas’ geometric manipulation functionality to calculate the suburbs polygon from the city limits polygon and the envelope polygon.\n\n\nfrom rasterio.mask import mask\nimport matplotlib.colors as mcolors\n\n\ncity_limits = gpd.read_file(\"./data/City_Limits.geojson\")\ncity_limits = city_limits.to_crs(epsg=landsat.crs.to_epsg())\n\n\nenvelope = city_limits.envelope\n\n\nsuburbs = envelope.difference(city_limits)\n\n\n\n2.1.3 Mask and calculate the NDVI for the city and the suburbs\nUsing the two polygons from the last section, use rasterio’s mask functionality to create two masked arrays from the landsat data, one for the city and one for the suburbs.\nFor each masked array, calculate the NDVI.\n\nmasked_city, mask_transform = mask(\n    dataset=landsat,              # The original raster data\n    shapes=city_limits.geometry,  # The vector geometry we want to crop by\n    crop=True,                    # Optional: remove pixels not within boundary\n    all_touched=True,             # Optional: get all pixels that touch the boudnary\n    filled=False,                 # Optional: do not fill cropped pixels with a default value\n)\n\n\nmasked_sub, mask_transform = mask(\n    dataset=landsat,              # The original raster data\n    shapes=suburbs.geometry,  # The vector geometry we want to crop by\n    crop=True,                    # Optional: remove pixels not within boundary\n    all_touched=True,             # Optional: get all pixels that touch the boudnary\n    filled=False,                 # Optional: do not fill cropped pixels with a default value\n)\n\n\n# Note that the indexing here is zero-based, e.g., band 1 is index 0\nred_c = masked_city[3]\nnir_c = masked_city[4]\n\nred_s = masked_sub[3]\nnir_s = masked_sub[4]\n\n\ndef calculate_NDVI(nir, red):\n    \"\"\"\n    Calculate the NDVI from the NIR and red landsat bands\n    \"\"\"\n\n    # Convert to floats\n    nir = nir.astype(float)\n    red = red.astype(float)\n\n    # Get valid entries\n    check = np.logical_and(red.mask == False, nir.mask == False)\n\n    # Where the check is True, return the NDVI, else return NaN\n    ndvi = np.where(check, (nir - red) / (nir + red), np.nan)\n    \n    # Return\n    return ndvi\n\n\nNDVI_c = calculate_NDVI(nir_c, red_c)\nNDVI_s = calculate_NDVI(nir_s, red_s)\n\n\ndata = landsat.read(1)\n\n\n# Initialize\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# The extent of the data\nlandsat_extent = [\n    landsat.bounds.left,\n    landsat.bounds.right,\n    landsat.bounds.bottom,\n    landsat.bounds.top,\n]\n\n# Plot!\nimg = ax.imshow(data, norm=mcolors.LogNorm(), extent=landsat_extent)\n\n# Add the city limits\ncity_limits.plot(ax=ax, facecolor=\"none\", edgecolor=\"white\")\n\n# Add a colorbar and turn off axis lines\nplt.colorbar(img)\nax.set_axis_off()\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Plot NDVI\nimg = ax.imshow(NDVI_c, extent=landsat_extent)\n\n# Format and plot city limits\ncity_limits.plot(ax=ax, edgecolor=\"gray\", facecolor=\"none\", linewidth=4)\nplt.colorbar(img)\nax.set_axis_off()\nax.set_title(\"NDVI in Philadelphia City\", fontsize=18);\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Plot NDVI\nimg = ax.imshow(NDVI_s, extent=landsat_extent)\n\n# Format and plot city limits\ncity_limits.plot(ax=ax, edgecolor=\"gray\", facecolor=\"none\", linewidth=4)\nplt.colorbar(img)\nax.set_axis_off()\nax.set_title(\"NDVI in Philadelphia Suburbs\", fontsize=18);\n\n\n\n\n\n\n2.1.4 Calculate the median NDVI within the city and within the suburbs\n\nCalculate the median value from your NDVI arrays for the city and suburbs\nNumpy’s nanmedian function will be useful for ignoring NaN elements\nPrint out the median values. Which has a higher NDVI: the city or suburbs?\n\n\nnp.nanmedian(NDVI_c)\n\n0.20268593532493442\n\n\n\nnp.nanmedian(NDVI_s)\n\n0.3746654463028859\n\n\n\nSuburbs have a higher median NDVI"
  },
  {
    "objectID": "MUSA550_Python/assignment-3-HangZhao.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "href": "MUSA550_Python/assignment-3-HangZhao.html#calculating-the-ndvi-for-philadelphias-street-treets",
    "title": "MUSA550 Geospatial Data Science in Python - Assignment 3",
    "section": "2.2 Calculating the NDVI for Philadelphia’s street treets",
    "text": "2.2 Calculating the NDVI for Philadelphia’s street treets\n\n2.2.1 Load the street tree data\nThe data is available in the “data/” folder. It has been downloaded from OpenDataPhilly. It contains the locations of abot 2,500 street trees in Philadelphia.\n\nphilly_tree = gpd.read_file(\"./data/ppr_tree_canopy_points_2015.geojson\")\n\n\n\n2.2.2 Calculate the NDVI values at the locations of the street trees\n\nUse the rasterstats package to calculate the NDVI values at the locations of the street trees.\nSince these are point geometries, you can calculate either the median or the mean statistic (only one pixel will contain each point).\n\n\nfrom rasterstats import zonal_stats\n\n\n# Convert to the landsat CRS\nphilly_tree = philly_tree.to_crs(epsg=landsat.crs.to_epsg())\n# Calculate the zonal statistics\nstats_trees = zonal_stats(\n    philly_tree, # Vector data as GeoDataFrame \n    NDVI_c, # Raster data as Numpy array\n    affine=landsat.transform, # Geospatial info via affine transform\n    stats=[\"mean\", \"median\"]\n)\n\n/Users/hangzhao/mambaforge/envs/musa-550-fall-2023/lib/python3.10/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n  warnings.warn(\n\n\n\n# Store the median value in the data frame\ntree_median_ndvi = [stats_dict[\"median\"] for stats_dict in stats_trees]\nphilly_tree[\"median_NDVI\"] = tree_median_ndvi\n\n\n\n2.2.3 Plotting the results\nMake two plots of the results:\n\nA histogram of the NDVI values, using matplotlib’s hist function. Include a vertical line that marks the NDVI = 0 threshold\nA plot of the street tree points, colored by the NDVI value, using geopandas’ plot function. Include the city limits boundary on your plot.\n\nThe figures should be clear and well-styled, with for example, labels for axes, legends, and clear color choices.\n\n# Initialize\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot a quick histogram\nax.hist(philly_tree[\"median_NDVI\"], bins=\"auto\")\nax.axvline(x=0, c=\"k\", lw=2)\n\n# Format\nax.set_xlabel(\"Median NDVI\", fontsize=18)\nax.set_ylabel(\"tree canopy cover\", fontsize=18);\n\n\n\n\n\n# Initialize\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Plot the city limits\ncity_limits.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=4)\n\n# Plot the median NDVI\nphilly_tree.plot(column=\"median_NDVI\", legend=True, ax=ax, cmap=\"viridis\")\n\n# Format\nax.set_axis_off()"
  },
  {
    "objectID": "MUSA550_Python/assignment-4.html",
    "href": "MUSA550_Python/assignment-4.html",
    "title": "MUSA550 - Assignment 4: Street Networks & Web Scraping",
    "section": "",
    "text": "Part 1: Visualizing crash data in Philadelphia\nIn this section, you will use osmnx to analyze the crash incidence in Center City.\nPart 2: Scraping Craigslist\nIn this section, you will use Selenium and BeautifulSoup to scrape data for hundreds of apartments from Philadelphia’s Craigslist portal."
  }
]